{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [텍스트 분류하기]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 베이지안 필터(Baysian filter)\n",
    "학습을 시킬수록 필터의 분류 능력이 오르는 특징을 가지고 있다.  \n",
    "때문에 메일 서비스에서 스팸 메일을 구분하거나/커뮤니티 사이트에서 스팸 글을 구분할 때 많이 사용된다. 이외에도 문장의 카데고리 분류에도 사용된다.  \n",
    "\n",
    "*교사학습, 비교사학습, 강화학습 중 \"교사학습\"에 해당됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) 이론\n",
    "\n",
    "#### (1) 베이즈 정리: 조건부 확률과 관련된 이론으로, 토머스 베이즈에 의해 정리된 이론  \n",
    "\n",
    "P(B|A)=P(A|B)P(B) / P(A)  \n",
    "* P(A): A가 일어날 확률\n",
    "* P(B): B가 일어날 확률(사전확률)\n",
    "* P(A|B): B가 일어난 후 A가 일어날 확률(조건부확률. 사후확률)\n",
    "* P(B|A): A가 일어난 후 B가 일어날 확률(조건부확률. 사전확률)\n",
    "\n",
    "\n",
    "#### (2) 조건부확률\n",
    "<어떤 A라는 사건이 일어났다는 조건>에서 <다른 사건 B가 일어날 확률>을 나타냄  \n",
    "\n",
    "ex)  \n",
    "* 비가 내릴 확률: P(비)\n",
    "* 교통사고가 발생활 확률: P(교통사고)\n",
    "* 비가 내리는 날에 교통사고가 발생할 확률: P(교통사고|비)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) 알고리즘\n",
    "\n",
    "#### 나이브 베이즈 분류(Naive Bayes classifier)\n",
    "베이즈 정리를 사용한 분류 방법.  \n",
    "텍스트 내부에서 단어 출현 비율을 조사→이를 기반으로 해당 텍스트를 어떤 카테고리로 분류하는 것이 적합한지 조사  \n",
    "\n",
    "실제로 판정을 할 때 P(B|A)는 1개의 확률이 아니라 여러 개의 카테고리 중에 어떤 카테고리에 속할 확률이 가장 큰지를 나타내는 정보이다.  \n",
    "따라서 베이즈 정리의 분모에 있는 P(A)는 입력 텍스트가 주어질 확률이다.  \n",
    "다만 어떤 카테고리를 판정하든 같은 입력 텍스트가 주어지는 것이므로 같은 값으로 생각하면 된다.  \n",
    "정리하면,  \n",
    "P(B|A)=P(B)P(A|B)가 된다.  \n",
    "\n",
    "결국 나이브 베이즈 분류는  \n",
    "단순한 출현율=단어의 출현횟수/카테고리 전체 단어 수  \n",
    "P(A|B)=P(B) / P(B|A)  \n",
    "가 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, sys\n",
    "from konlpy.tag import Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianFilter:\n",
    "    \"\"\" 베이지안 필터 \"\"\"\n",
    "    def __init__(self):\n",
    "        self.words = set() # 출현한 단어 기록\n",
    "        self.word_dict = {} # 카테고리마다의 출현 횟수 기록\n",
    "        self.category_dict = {} # 카테고리 출현 횟수 기록\n",
    "        \n",
    "        \n",
    "        \n",
    "    # 1. 형태소 분석하기\n",
    "    def split(self, text):\n",
    "        return text.split()\n",
    "        results = []\n",
    "        twitter = Twitter()\n",
    "        # 단어의 기본형 사용\n",
    "        malist = twitter.pos(text, norm=True, stem=True)\n",
    "        for word in malist:\n",
    "            # 어미/조사/구두점 등은 대상에서 제외 \n",
    "            if not word[1] in [\"Josa\", \"Eomi\", \"Punctuation\"]:\n",
    "                results.append(word[0])\n",
    "        return results\n",
    "    \n",
    "    \n",
    "    \n",
    "    # 2. 단어와 카테고리의 출현 횟수 세기 \n",
    "    def inc_word(self, word, category): #단어 횟수 세기\n",
    "        # 단어를 카테고리에 추가하기\n",
    "        if not category in self.word_dict:\n",
    "            self.word_dict[category] = {}\n",
    "        if not word in self.word_dict[category]:\n",
    "            self.word_dict[category][word] = 0\n",
    "        self.word_dict[category][word] += 1\n",
    "        self.words.add(word)\n",
    "        \n",
    "    def inc_category(self, category): #카테고리 횟수 세기\n",
    "        # 카테고리 계산하기\n",
    "        if not category in self.category_dict:\n",
    "            self.category_dict[category] = 0\n",
    "        self.category_dict[category] += 1\n",
    "    \n",
    "    \n",
    "    \n",
    "    # 3. 텍스트 학습하기 \n",
    "    def fit(self, text, category): #텍스트를 형태소로 분할하고, 카테고리와 단어를 연결함\n",
    "        \"\"\" 텍스트 학습 \"\"\"\n",
    "        word_list = self.split(text)\n",
    "        for word in word_list:\n",
    "            self.inc_word(word, category)\n",
    "        self.inc_category(category)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # 4. 단어 리스트에 점수 매기기\n",
    "    def score(self, words, category):\n",
    "        #확률을 곱할 때 값이 너무 작으면 다운플로가 발생할 수 있어 log를 이용함\n",
    "        score = math.log(self.category_prob(category)) \n",
    "        for word in words:\n",
    "            score += math.log(self.word_prob(word, category))\n",
    "        return score\n",
    "    \n",
    "    \n",
    "    \n",
    "    # 5. 예측하기 \n",
    "    def predict(self, text):\n",
    "        best_category = None\n",
    "        max_score = -sys.maxsize \n",
    "        words = self.split(text)\n",
    "        score_list = []\n",
    "        for category in self.category_dict.keys():\n",
    "            score = self.score(words, category)\n",
    "            score_list.append((category, score))\n",
    "            if score > max_score:\n",
    "                max_score = score\n",
    "                best_category = category\n",
    "        return best_category, score_list\n",
    "    \n",
    "    # 카테고리 내부의 단어 출현 횟수 구하기\n",
    "    def get_word_count(self, word, category):\n",
    "        if word in self.word_dict[category]:\n",
    "            return self.word_dict[category][word]\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    # 카테고리 계산\n",
    "    def category_prob(self, category):\n",
    "        sum_categories = sum(self.category_dict.values())\n",
    "        category_v = self.category_dict[category]\n",
    "        return category_v / sum_categories\n",
    "        \n",
    "        \n",
    "        \n",
    "    # 6. 카테고리 내부의 단어 출현 비율 계산 \n",
    "    def word_prob(self, word, category):\n",
    "        #단어 출현률을 계산할 때 학습사전(word_dict)에 없는 단어가 나오면 카테고리의 확률이 0이 되어버려 1을 더해 활용\n",
    "        n = self.get_word_count(word, category) + 1 # ---(※6a)\n",
    "        d = sum(self.word_dict[category].values()) + len(self.words)\n",
    "        return n / d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "결과 = 광고\n",
      "[('중요', -20.544606748320554), ('광고', -19.942524744665512)]\n"
     ]
    }
   ],
   "source": [
    "### 예시\n",
    "bf= BayesianFilter()\n",
    "\n",
    "# 텍스트 학습\n",
    "bf.fit(\"파격 세일 - 오늘까지만 30% 할인\", \"광고\")\n",
    "bf.fit(\"쿠폰 선물 & 무료 배송\", \"광고\")\n",
    "bf.fit(\"현데계 백화점 세일\", \"광고\")\n",
    "bf.fit(\"봄과 함께 찾아온 따뜻한 신제품 소식\", \"광고\")\n",
    "bf.fit(\"인기 제품 기간 한정 세일\", \"광고\")\n",
    "bf.fit(\"오늘 일정 확인\", \"중요\")\n",
    "bf.fit(\"프로젝트 진행 상황 보고\",\"중요\")\n",
    "bf.fit(\"계약 잘 부탁드립니다\",\"중요\")\n",
    "bf.fit(\"회의 일정이 등록되었습니다.\",\"중요\")\n",
    "bf.fit(\"오늘 일정이 없습니다.\",\"중요\")\n",
    "\n",
    "# 예측\n",
    "pre, scorelist = bf.predict(\"재고 정리 할인, 무료 배송\")\n",
    "print(\"결과 =\", pre)\n",
    "print(scorelist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MLP\n",
    "다층 퍼셉트론(Multi Layer Perceptron, MLP). 입력층과 출력층 사이에 각각 전체 결합하는 은닉층을 넣은 뉴럴 네트워크  \n",
    "텍스트 데이터를 숫자로 표현할 수 있는 벡터로 변환해서 사용한다. \n",
    "\n",
    "### 1) 텍스트 데이터를 고정 길이의 벡터로 변환하는 방법\n",
    "단어 하나하나에 ID를 부여하고, ID의 출현 빈도와 정렬 순서를 기반으로 벡터를 만든다. = Bow 이용\n",
    "* Bow: Bag-of-words. 글에 어떠한 단어가 있는지를 수치로 나타내는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('몇', 'Noun'), ('번', 'Noun'), ('을', 'Josa'), ('쓰러지다', 'Verb'), ('몇', 'Noun'), ('번', 'Noun'), ('을', 'Josa'), ('무너지다', 'Verb'), ('다시', 'Noun'), ('일어나다', 'Verb')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\konlpy\\tag\\_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    }
   ],
   "source": [
    "twitter=Twitter()\n",
    "st=twitter.pos(\"몇 번을 쓰러지더라도 몇 번을 무너지더라도 다시 일어나라\", stem=True, norm= True)\n",
    "print(st)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 예시(몇 번을 쓰러지더라고 몇 번을 무너지더라도 다시 일어나라)에 Bow 적용하기\n",
    "#형태소-ID-출현 횟수\n",
    "#몇-1-2\n",
    "#번-2-2\n",
    "#을-3-2\n",
    "#쓰러지다-4-1\n",
    "#무너지다-5-1\n",
    "#다시-6-1\n",
    "#일어나다-7-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) 텍스트 분류하기\n",
    "[1] 텍스트에서 불필요한 품사를 제거한다.  \n",
    "[2] 사전을 기반으로 단어를 숫자로 변환한다.  \n",
    "[3] 파일 내부의 단어 출현 비율을 계산한다.  \n",
    "[4] 데이터를 학습시킨다.  \n",
    "[5] 테스트 데이터를 넣어 성공률을 확인한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) 단어를 ID로 변환하고 출현 횟수 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "root_dir = \"./newstext\"\n",
    "dic_file = \"/word-dic.json\"\n",
    "data_file = \"/data.json\"\n",
    "data_file_min = \"/data-mini.json\"\n",
    "\n",
    "# 1.어구를 자르고 ID로 변환하기\n",
    "word_dic = { \"_MAX\": 0 }\n",
    "\n",
    "def text_to_ids(text):\n",
    "    text = text.strip()\n",
    "    words = text.split(\" \")\n",
    "    result = []\n",
    "    for n in words:\n",
    "        n = n.strip()\n",
    "        if n == \"\": continue\n",
    "        if not n in word_dic:\n",
    "            wid = word_dic[n] = word_dic[\"_MAX\"]\n",
    "            word_dic[\"_MAX\"] += 1\n",
    "            print(wid, n)\n",
    "        else:\n",
    "            wid = word_dic[n]\n",
    "        result.append(wid)\n",
    "    print(result)\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "# 2. 파일을 읽고 고정 길이의 배열 리턴하기\n",
    "def file_to_ids(fname):\n",
    "    with open(fname, \"r\") as f:\n",
    "        text = f.read()\n",
    "        return text_to_ids(text)\n",
    "    \n",
    "    \n",
    "    \n",
    "# 3. 딕셔너리에 단어 모두 등록하기\n",
    "def register_dic():\n",
    "    files = glob.glob(root_dir+\"/*/*.wakati\", recursive=True)\n",
    "    for i in files:\n",
    "        file_to_ids(i)\n",
    "\n",
    "        \n",
    "        \n",
    "# 4. 파일 내부의 단어 세기\n",
    "def count_file_freq(fname):\n",
    "    cnt = [0 for n in range(word_dic[\"_MAX\"])]\n",
    "    with open(fname,\"r\") as f:\n",
    "        text = f.read().strip()\n",
    "        ids = text_to_ids(text)\n",
    "        for wid in ids:\n",
    "            cnt[wid] += 1\n",
    "    return cnt\n",
    "\n",
    "#카테고리마다 파일 읽어 들이기\n",
    "def count_freq(limit = 0):\n",
    "    X = []\n",
    "    Y = []\n",
    "    max_words = word_dic[\"_MAX\"]\n",
    "    cat_names = []\n",
    "    for cat in os.listdir(root_dir):\n",
    "        cat_dir = root_dir + \"/\" + cat\n",
    "        if not os.path.isdir(cat_dir): continue\n",
    "        cat_idx = len(cat_names)\n",
    "        cat_names.append(cat)\n",
    "        files = glob.glob(cat_dir+\"/*.wakati\")\n",
    "        i = 0\n",
    "        for path in files:\n",
    "            print(path)\n",
    "            cnt = count_file_freq(path)\n",
    "            X.append(cnt)\n",
    "            Y.append(cat_idx)\n",
    "            if limit > 0:\n",
    "                if i > limit: break\n",
    "                i += 1\n",
    "    return X,Y\n",
    "\n",
    "\n",
    "# 5. 단어 딕셔너리 만들기\n",
    "if os.path.exists(dic_file):\n",
    "     word_dic = json.load(open(dic_file))\n",
    "else:\n",
    "    register_dic()\n",
    "    json.dump(word_dic, open(dic_file,\"w\"))\n",
    "    \n",
    "    \n",
    "    \n",
    "# 6. 벡터를 파일로 출력하기\n",
    "# 테스트 목적의 소규모 데이터 만들기\n",
    "X, Y = count_freq(20)\n",
    "json.dump({\"X\": X, \"Y\": Y}, open(data_file_min,\"w\"))\n",
    "\n",
    "# 전체 데이터를 기반으로 데이터 만들기\n",
    "X, Y = count_freq()\n",
    "json.dump({\"X\": X, \"Y\": Y}, open(data_file,\"w\"))\n",
    "\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) MLP로 텍스트 분류하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import model_selection, metrics\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "max_words = 56681 # 입력 단어 수: word-dic.json 파일 참고\n",
    "nb_classes = 9    # 9개의 카테고리\n",
    "batch_size = 64 \n",
    "nb_epoch = 20\n",
    "\n",
    "# 1. MLP 모델 생성하기\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_shape=(max_words,)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(nb_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# 2. 데이터 읽어 들이기\n",
    "data = json.load(open(\"./data-mini.json\"))\n",
    "#data = json.load(open(\"./newstext/data.json\"))\n",
    "X = data[\"X\"] # 텍스트를 나타내는 데이터\n",
    "Y = data[\"Y\"] # 카테고리 데이터\n",
    "\n",
    "\n",
    "\n",
    "# 3. 학습하기\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y)\n",
    "Y_train = np_utils.to_categorical(Y_train, nb_classes)\n",
    "model = KerasClassifier(build_fn=build_model, nb_epoch=nb_epoch, batch_size=batch_size)\n",
    "model.fit(X_train, Y_train)\n",
    "print(len(X_train),len(Y_train))\n",
    "\n",
    "\n",
    "\n",
    "# 4. 예측하기\n",
    "y = model.predict(X_test)\n",
    "ac_score = metrics.accuracy_score(Y_test, y)\n",
    "cl_report = metrics.classification_report(Y_test, y)\n",
    "#print(\"정답률 =\", ac_score)\n",
    "#print(\"리포트 =\\n\", cl_report)\n",
    "print('ok')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
