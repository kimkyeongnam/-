{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [마르코프 체인과 LSTM으로 문장 생성하기]\n",
    "\n",
    "## 1. 마르코프 체인\n",
    "* 마르코프 체인: 확률을 기반으로 하는 방법  \n",
    "\n",
    "러시아의 수학자 마르코프가 연구함. 다른 이름으로 워드 샐러드라도고 한다.  \n",
    "물리학과 통계학의 기본 모델로 응용되고 있다.  \n",
    "마르코프 체인을 이용하면 기존 문장을 기반으로 문장을 자동으로 생성할 수 있다.  \n",
    "\n",
    "\n",
    "### (1) 마르코프 성질(Markov property)\n",
    "과거의 상태를 무시하고, 현재의 상태만을 기반으로 다음 상태를 선택하는 것을 의미한다.  \n",
    "마르코프 성질이 존재하면  \n",
    "현재 상태를 qi, 다음 상태 qj로 이동할 확률은 P(qj|qi)\n",
    "\n",
    "[1] 문장을 단어로 분할(형태소 분석)  \n",
    "[2] 단어의 전후 연결을 딕셔너리에 등록  \n",
    "[3] 사전을 사용해 임의의 문장을 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) 마르코프 체인 구현하기\n",
    "(토지 텍스트 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "from bs4 import BeautifulSoup\n",
    "from konlpy.tag import Twitter\n",
    "import urllib.request\n",
    "import os, re, json, random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 마르코프 체인 딕셔너리 만들기\n",
    "def make_dic(words):\n",
    "    tmp = [\"@\"]\n",
    "    dic = {}\n",
    "    for word in words:\n",
    "        tmp.append(word)\n",
    "        if len(tmp) < 3: continue\n",
    "        if len(tmp) > 3: tmp = tmp[1:]\n",
    "        set_word3(dic, tmp)\n",
    "        if word == \".\":\n",
    "            tmp = [\"@\"]\n",
    "            continue\n",
    "    return dic\n",
    "\n",
    "\n",
    "\n",
    "# 2. 딕셔너리에 데이터 등록하기\n",
    "def set_word3(dic, s3):\n",
    "    w1, w2, w3 = s3\n",
    "    if not w1 in dic: dic[w1] = {}\n",
    "    if not w2 in dic[w1]: dic[w1][w2] = {}\n",
    "    if not w3 in dic[w1][w2]: dic[w1][w2][w3] = 0\n",
    "    dic[w1][w2][w3] += 1\n",
    "\n",
    "    \n",
    "    \n",
    "# 3. 문장 만들기\n",
    "def make_sentence(dic):\n",
    "    ret = []\n",
    "    if not \"@\" in dic: return \"no dic\" \n",
    "    top = dic[\"@\"]\n",
    "    w1 = word_choice(top)\n",
    "    w2 = word_choice(top[w1])\n",
    "    ret.append(w1)\n",
    "    ret.append(w2)\n",
    "    while True:\n",
    "        w3 = word_choice(dic[w1][w2])\n",
    "        ret.append(w3)\n",
    "        if w3 == \".\": break\n",
    "        w1, w2 = w2, w3\n",
    "    ret = \"\".join(ret)\n",
    "    # 띄어쓰기\n",
    "    params = urllib.parse.urlencode({\n",
    "        \"_callback\": \"\",\n",
    "        \"q\": ret\n",
    "    })\n",
    "    # 네이버 맞춤법 검사기 사용\n",
    "    data = urllib.request.urlopen(\"https://m.search.naver.com/p/csearch/dcontent/spellchecker.nhn?\" + params)\n",
    "    data = data.read().decode(\"utf-8\")[1:-2]\n",
    "    data = json.loads(data)\n",
    "    data = data[\"message\"][\"result\"][\"html\"]\n",
    "    #data = soup = BeautifulSoup(data, \"html.parser\").getText()\n",
    "    data = BeautifulSoup(data, \"html.parser\").getText()\n",
    "    \n",
    "    # 리턴\n",
    "    return data\n",
    "\n",
    "def word_choice(sel):\n",
    "    keys = sel.keys()\n",
    "    return random.choice(list(keys))\n",
    "\n",
    "\n",
    "\n",
    "# 4. 문장 읽어 들이기\n",
    "toji_file = \"toji.txt\"\n",
    "dict_file = \"markov-toji.json\"\n",
    "\n",
    "if not os.path.exists(dict_file):\n",
    "    # 토지 텍스트 파일 읽어 들이기\n",
    "    fp = codecs.open(\"toji.txt\", \"r\", encoding=\"utf-16\")\n",
    "    soup = BeautifulSoup(fp, \"html.parser\")\n",
    "    body = soup.select_one(\"body > text\")\n",
    "    text = body.getText()\n",
    "    text = text.replace(\"…\", \"\") # 현재 koNLPy가 …을 구두점으로 잡지 못하는 문제 임시 해결\n",
    "    # 형태소 분석\n",
    "    twitter = Twitter()\n",
    "    malist = twitter.pos(text, norm=True)\n",
    "    words = []\n",
    "    for word in malist:\n",
    "        # 구두점 등은 대상에서 제외(단 마침표는 포함)\n",
    "        if not word[1] in [\"Punctuation\"]:\n",
    "            words.append(word[0])\n",
    "        if word[0] == \".\":\n",
    "            words.append(word[0])\n",
    "    # 딕셔너리 생성\n",
    "    dic = make_dic(words)\n",
    "    json.dump(dic, open(dict_file,\"w\", encoding=\"utf-8\"))\n",
    "else:\n",
    "    dic = json.load(open(dict_file,\"r\"))\n",
    "    \n",
    "    \n",
    "    \n",
    "# 5. 문장 만들기\n",
    "#for i in range(3):\n",
    "    #s = make_sentence(dic)\n",
    "    #print(s)\n",
    "    #print(\"---\")      #결과가 길어서 생략"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 재귀 신경망(Recurrent Neural Network/RNN) & LSTM(Long Short Term-Memory)\n",
    "\n",
    "* RNN: 신경망을 재귀적으로 사용해 시간 순서를 가진 데이터를 다룰 수 있게 한 것\n",
    "* LSTM: RNN 개량. RNN은 바로 전의 데이터밖에 기억하지 못하지만 LSTM은 장기적인 기억 기능을 지니고 있음\n",
    "\n",
    "\"오늘\"을 입력하면 \"아침\", \"날씨\"라는 글자가 이어질 것이라고 예상하고 조합할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "from bs4 import BeautifulSoup\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "import numpy as np\n",
    "import random, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "코퍼스의 길이:  10772\n",
      "사용되고 있는 문자의 수: 1692\n",
      "학습할 구문의 수: 103888\n",
      "텍스트를 ID 벡터로 변환합니다...\n",
      "모델을 구축합니다...\n",
      "\n",
      "--------------------------------------------------\n",
      "반복 = 1\n...........(뒤의 결과는 생략)"
     ]
    }
   ],
   "source": [
    "fp = codecs.open(\"./BEXX0003.txt\", \"r\", encoding=\"utf-16\")\n",
    "soup = BeautifulSoup(fp, \"html.parser\")\n",
    "body = soup.select_one(\"body\")\n",
    "text = body.getText() + \" \"\n",
    "\n",
    "print('코퍼스의 길이: ', len(dic))\n",
    "\n",
    "\n",
    "\n",
    "# 문자를 하나하나 읽어 들이고 ID 붙이기\n",
    "chars = sorted(list(set(text)))\n",
    "print('사용되고 있는 문자의 수:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars)) # 문자 → ID\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars)) # ID → 문자\n",
    "\n",
    "\n",
    "\n",
    "# 텍스트를 maxlen개의 문자로 자르고 다음에 오는 문자 등록하기\n",
    "maxlen = 20\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('학습할 구문의 수:', len(sentences))\n",
    "print('텍스트를 ID 벡터로 변환합니다...')\n",
    "X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1\n",
    "    \n",
    "    \n",
    "    \n",
    "# 모델 구축하기(LSTM)\n",
    "print('모델을 구축합니다...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "\n",
    "\n",
    "# 후보를 배열에서 꺼내기\n",
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "\n",
    "\n",
    "# 학습시키고 텍스트 생성하기 반복\n",
    "for iteration in range(1, 60):\n",
    "    print()\n",
    "    print('-' * 50)\n",
    "    print('반복 =', iteration)\n",
    "    model.fit(X, y, batch_size=128, nb_epoch=1) # \n",
    "    \n",
    "    # 임의의 시작 텍스트 선택하기\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    \n",
    "    # 다양한 다양성의 문장 생성\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print()\n",
    "        print('--- 다양성 = ', diversity)\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('--- 시드 = \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "        \n",
    "        # 시드를 기반으로 텍스트 자동 생성\n",
    "        for i in range(400):\n",
    "            x = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x[0, t, char_indices[char]] = 1.\n",
    "                \n",
    "            # 다음에 올 문자를 예측하기\n",
    "            preds = model.predict(x, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "            \n",
    "            # 출력하기\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
