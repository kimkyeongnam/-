{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "중간고사 정리.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "QuLum8_D2cGd",
        "99bp9Qqc2o2O",
        "izcdVKL_5n2T",
        "7OAsC4RqCKgM",
        "R0hpi7vbq_h-",
        "6aWvb0rKALlV",
        "ddxGZ_2vwYIv",
        "SkM5_iKgxYB4"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "kV4X9xym2eda",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Linear classification"
      ]
    },
    {
      "metadata": {
        "id": "QuLum8_D2cGd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1. unit"
      ]
    },
    {
      "metadata": {
        "id": "L-_llYG0xV3q",
        "colab_type": "code",
        "outputId": "853fac7f-7ba2-4d2e-d5c7-c6c6bc8434ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "tf.set_random_seed(777)\n",
        "\n",
        "x_train =[202.6183, 200.0614, 199.0766, 197.1534, 197.5498, 197.237 ]\n",
        "y_train= [ 94.03061, 108.9159 , 123.1589 , 137.2437 , 151.315  , 164.3793 ]\n",
        "\n",
        "W = tf.Variable(tf.random_normal([1]), name='weight')\n",
        "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
        "\n",
        "hypothesis = x_train * W + b\n",
        "\n",
        "cost = tf.reduce_mean(tf.square(hypothesis - y_train))\n",
        "train = tf.train.GradientDescentOptimizer(learning_rate=0.000001).minimize(cost)\n",
        "\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "for step in range(10001):\n",
        "  _, cost_val, W_val, b_val = sess.run([train, cost, W, b])\n",
        "  \n",
        "  if step%10000 == 0:\n",
        "    print(step, cost_val, W_val, b_val)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 1757.9865 [0.50061417] [-0.8136271]\n",
            "10000 637.017 [0.6555438] [-0.80808234]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "99bp9Qqc2o2O",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2. multi"
      ]
    },
    {
      "metadata": {
        "id": "ydKfB9WYy337",
        "colab_type": "code",
        "outputId": "cc74298e-3ccd-427f-ac31-b8b2c097a75f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "# matrix 없이\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "tf.set_random_seed(777)\n",
        "\n",
        "x1_data = [73., 93., 89., 96., 73.]\n",
        "x2_data = [80., 88., 91., 98., 66.]\n",
        "x3_data = [75., 93., 90., 100., 70.]\n",
        "y_data = [152., 185., 180., 196., 142.]\n",
        "\n",
        "x1 = tf.placeholder(tf.float32)\n",
        "x2 = tf.placeholder(tf.float32)\n",
        "x3 = tf.placeholder(tf.float32)\n",
        "y = tf.placeholder(tf.float32)\n",
        "\n",
        "w1 = tf.Variable(tf.random_normal([1]), name='weight1')\n",
        "w2 = tf.Variable(tf.random_normal([1]), name='weight2')\n",
        "w3 = tf.Variable(tf.random_normal([1]), name='weight3')\n",
        "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
        "\n",
        "hypothesis = x1*w1 + x2*w2 + x3*w3 + b\n",
        "\n",
        "cost = tf.reduce_mean(tf.square(hypothesis-y))\n",
        "train = tf.train.GradientDescentOptimizer(learning_rate = 1e-5).minimize(cost)\n",
        "\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "for step in range(10001):\n",
        "  cost_val, hy_val, _ = sess.run([cost, hypothesis, train], feed_dict={x1:x1_data, x2:x2_data, x3:x3_data, y:y_data})\n",
        "  \n",
        "  if step%10000==0:\n",
        "    print(step, \"Cost: \",cost_val, \"\\nPrediction:\", hy_val,\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 Cost:  39564.0 \n",
            "Prediction: [-23.07097  -28.361422 -27.178469 -34.84842  -17.477777] \n",
            "\n",
            "10000 Cost:  2.8719635 \n",
            "Prediction: [152.99635 183.9482  181.582   193.46118 143.82002] \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GDAaESdbzji7",
        "colab_type": "code",
        "outputId": "5f1ec3f7-93cd-4cdd-909b-fec2894bc910",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "cell_type": "code",
      "source": [
        "# matrix 사용\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "tf.set_random_seed(777)\n",
        "\n",
        "x_data = [[73., 80., 75.],\n",
        "          [93., 88., 93.],\n",
        "          [89., 91., 90.],\n",
        "          [96., 98., 100.],\n",
        "          [73., 66., 70.]]\n",
        "y_data = [[152.],\n",
        "          [185.],\n",
        "          [180.],\n",
        "          [196.],\n",
        "          [142.]]\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=[None,3])\n",
        "Y = tf.placeholder(tf.float32, shape=[None,1])\n",
        "\n",
        "W = tf.Variable(tf.random_normal([3,1]), name='weight')\n",
        "b = tf.Variable(tf.random_normal([1]),name='bias')\n",
        "\n",
        "hypothesis = tf.matmul(X, W) + b\n",
        "\n",
        "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
        "train = tf.train.GradientDescentOptimizer(learning_rate=1e-5).minimize(cost)\n",
        "\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "for step in range(10001):\n",
        "  cost_val, hy_val, _ = sess.run(\n",
        "      [cost, hypothesis, train], feed_dict={X:x_data, Y:y_data})\n",
        "  \n",
        "  if step%10000==0:\n",
        "    print(step, \"Cost: \",cost_val, \"\\nPrediction:\", hy_val,\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 Cost:  21.107025 \n",
            "Prediction: [[154.49529]\n",
            " [179.40346]\n",
            " [179.86642]\n",
            " [197.54999]\n",
            " [133.90265]] \n",
            "\n",
            "10000 Cost:  1.4106073 \n",
            "Prediction: [[150.94534]\n",
            " [184.81314]\n",
            " [180.2107 ]\n",
            " [197.88152]\n",
            " [140.47641]] \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "izcdVKL_5n2T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 과제: 배추가격 예측 AI로봇 만들기"
      ]
    },
    {
      "metadata": {
        "id": "vaBmgr0p2bp1",
        "colab_type": "code",
        "outputId": "5088f901-a7db-481b-8b12-3002f2183a75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import numpy as np\n",
        "from pandas.io.parsers import read_csv\n",
        "\n",
        "if os.path.exists('/content/gdrive')==False:\n",
        "  drive.mount('/content/gdrive')\n",
        "  print('Google Drive is mounted\\n')\n",
        "  \n",
        "else:\n",
        "  print('Google Drive is already mounted\\n')\n",
        "  \n",
        "\n",
        "  \n",
        "if os.path.exists('/content/gdrive/My Drive/Colab Notebooks/price_data_yk.csv')==False:\n",
        "  print('Train data downloading..')\n",
        "  ! curl 'https://raw.githubusercontent.com/unizard/2019.Spring.AI/master/price_data_yk.csv' -o '/content/gdrive/My Drive/Colab Notebooks/price_data_yk.csv'\n",
        "  print('Done..\\n')\n",
        "else:\n",
        "  print('File already exists \\n')\n",
        "    \n",
        "# 학습데이터 로딩\n",
        "data = read_csv('/content/gdrive/My Drive/Colab Notebooks/price_data_yk.csv',sep=',')\n",
        "xy = np.array(data, dtype=np.float32)\n",
        "\n",
        "x_data = xy[:, 1:-1] # year, avgPrice 제외\n",
        "y_data = xy[:, [-1]] #avgPrice만 가져오기\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape = [None, 4])\n",
        "Y = tf.placeholder(tf.float32, shape = [None, 1])\n",
        "\n",
        "W = tf.Variable(tf.random_normal([4,1]),name='weight')\n",
        "b = tf.Variable(tf.random_normal([1]),name='bias')\n",
        "\n",
        "hypothesis = tf.matmul(X,W) + b\n",
        "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
        "train = tf.train.GradientDescentOptimizer(learning_rate = 0.000005).minimize(cost)\n",
        "\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "print('=========================================')\n",
        "print(' 학습 시작')\n",
        "print('=========================================')\n",
        "\n",
        "for step in range(100001):\n",
        "  cost_, hypo_, _ = sess.run([cost, hypothesis, train], feed_dict={X:x_data,Y:y_data})\n",
        "  if step%100000 == 0:\n",
        "    print(\"#\", step, \" 손실비용: \", cost_)\n",
        "    print(\"-배추 가격: \", hypo_[0])\n",
        "\n",
        "print('=========================================')\n",
        "print(' 학습 끝')\n",
        "print('=========================================')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 오늘의 날짜를 파일 이름으로 활용할 예정이었으나, 여러분의 혼란을 줄이고자 주석처리함\n",
        "# 오늘의 날짜를 사용하기 위한 라이브러리 import\n",
        "import datetime\n",
        "# 학습 모델 저장을 위한 그래프 설정\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "# 학습 모델 이름 설정\n",
        "#strTLog = datetime.date.today().strftime(\"%Y%m%d\")  \n",
        "path = '/content/gdrive/My Drive/Colab Notebooks/saved.cpkt'\n",
        "\n",
        "# 학습 모델 저장\n",
        "save_path = saver.save(sess,path)\n",
        "print('학습된 모델을 저장했습니다.')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Google Drive is already mounted\n",
            "\n",
            "File already exists \n",
            "\n",
            "=========================================\n",
            " 학습 시작\n",
            "=========================================\n",
            "# 0  손실비용:  12697070.0\n",
            "-배추 가격:  [9.314626]\n",
            "# 100000  손실비용:  2228358.5\n",
            "-배추 가격:  [2584.6968]\n",
            "=========================================\n",
            " 학습 끝\n",
            "=========================================\n",
            "학습된 모델을 저장했습니다.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5tM2d9ds6GS5",
        "colab_type": "code",
        "outputId": "7c9d027f-962e-4bc2-e2d3-e466132962cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "import os\n",
        "\n",
        "if os.path.exists('/content/gdrive')==False:\n",
        "  drive.mount('/content/gdrive')\n",
        "  print('Google Drive is mounted\\n')\n",
        "else:\n",
        "  print('Google Drive is already mounted\\n')\n",
        "\n",
        "  \n",
        "X = tf.placeholder(tf.float32, shape=[None,4])\n",
        "Y = tf.placeholder(tf.float32, shape=[None,1])\n",
        "\n",
        "hypothesis = tf.matmul(X,W) + b\n",
        "\n",
        "avg_temp=float(input('평균온도: '))\n",
        "min_temp=float(input('최저온도: '))\n",
        "max_temp=float(input('최고온도: '))\n",
        "rain_fall=float(input('강수량: '))\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "model = tf.global_variables_initializer()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    \n",
        "    # 그래프 연결\n",
        "    sess.run(model)\n",
        "    \n",
        "    # 학습 모델 로드\n",
        "    save_path = '/content/gdrive/My Drive/Colab Notebooks/saved.cpkt'\n",
        "    saver.restore(sess, save_path)\n",
        "    \n",
        "    # 모델에 넣을 입력데이터 가공\n",
        "    data = ((avg_temp, min_temp, max_temp, rain_fall), (0, 0, 0, 0))\n",
        "    arr = np.array(data, dtype=np.float32)\n",
        "    x_data = arr[0:4]\n",
        "    print(x_data[0])\n",
        "    \n",
        "    # 평균온도, 최대온도, 최저온도, 강수량으로 배추가격 예측을 위해 세션 실행\n",
        "    predict = sess.run(hypothesis, feed_dict={X: x_data})\n",
        "    print(predict[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Google Drive is already mounted\n",
            "\n",
            "평균온도: -2.7\n",
            "최저온도: -6.6\n",
            "최고온도: 2.\n",
            "강수량: .1\n",
            "INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/Colab Notebooks/saved.cpkt\n",
            "[-2.7 -6.6  2.   0.1]\n",
            "[2000.4777]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "l2WVIHdfB7gl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Linear Regression"
      ]
    },
    {
      "metadata": {
        "id": "dVEdAgeSB9gk",
        "colab_type": "code",
        "outputId": "5f61c151-0eb7-4a84-8297-2540b124d1c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "if os.path.exists('/content/gdrive')==False:\n",
        "    drive.mount('/content/gdrive')\n",
        "    print('Google Drive is mounted\\n')\n",
        "else:\n",
        "    print('Google Drive is already mounted\\n')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "Google Drive is mounted\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7OAsC4RqCKgM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1. Binary classification\n",
        "* 학습/테스트의 데이터는 같음\n",
        "* hypothesis: _Sigmoid_ 사용\n",
        "\n",
        "<br>\n",
        "x_data = [[1, 2],[2, 3],[3, 1],[4, 3],[5, 3],[6, 2]]  \n",
        "y_data = [[0],[0],[0],[1],[1],[1]]"
      ]
    },
    {
      "metadata": {
        "id": "PPSW4db4CJYb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "e0daccc1-9d17-48ad-e385-628a42fe0d26"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "tf.set_random_seed(777)\n",
        "\n",
        "x_data = [[1, 2],[2, 3],[3, 1],[4, 3],[5, 3],[6, 2]]\n",
        "y_data = [[0],[0],[0],[1],[1],[1]]\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=[None,2])\n",
        "Y = tf.placeholder(tf.float32, shape=[None,1])\n",
        "\n",
        "W = tf.Variable(tf.random_normal([2,1]), name='weight')\n",
        "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
        "\n",
        "hypothesis = tf.sigmoid(tf.matmul(X,W) + b)\n",
        "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1-Y)*tf.log(1-hypothesis))\n",
        "train = tf.train.GradientDescentOptimizer(learning_rate = 0.01).minimize(cost)\n",
        "\n",
        "\n",
        "predicted = tf.cast(hypothesis>0.5, dtype=tf.float32)\n",
        "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted,Y),dtype=tf.float32))\n",
        "\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "for step in range(10001):\n",
        "  cost_val, _ = sess.run([cost, train], feed_dict={X:x_data, Y:y_data})\n",
        "  if step%1000==0:\n",
        "    print(step,cost_val)\n",
        "\n",
        "fin_hyp, fin_prd, fin_acr = sess.run([hypothesis, predicted, accuracy], feed_dict={X:x_data,Y:y_data})\n",
        "print(\"\\nHypothesis: \\n\", fin_hyp, \"\\nCorrect (Y): \\n\", fin_prd, \"\\nAccuracy: \\n\", fin_acr)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 3.0963078\n",
            "1000 0.49478617\n",
            "2000 0.41475156\n",
            "3000 0.35225573\n",
            "4000 0.30352256\n",
            "5000 0.2652935\n",
            "6000 0.23493777\n",
            "7000 0.21047556\n",
            "8000 0.19046216\n",
            "9000 0.17384946\n",
            "10000 0.15987395\n",
            "\n",
            "Hypothesis: \n",
            " [[0.03522347]\n",
            " [0.16451049]\n",
            " [0.32564858]\n",
            " [0.7720432 ]\n",
            " [0.9335378 ]\n",
            " [0.97816443]] \n",
            "Correct (Y): \n",
            " [[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]] \n",
            "Accuracy: \n",
            " 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "R0hpi7vbq_h-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2. 당뇨병 환자를 판별하는 AI머신 만들기"
      ]
    },
    {
      "metadata": {
        "id": "maN7gmZhq_QD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52091
        },
        "outputId": "353686a0-dbcf-48e3-8da7-8b24e23ef1b6"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "tf.set_random_seed(777)\n",
        "\n",
        "if os.path.exists('/contents/gdrive/My Drive/Colab Notebooks/diabetes.csv')==False:\n",
        "  print('Train data downloading..')\n",
        "  ! curl 'https://raw.githubusercontent.com/unizard/2019.Spring.AI/master/diabetes.csv' -o '/contents/gdrive/My Drive/Colab Notebooks/diabetes.csv'\n",
        "  print('Done..\\n')\n",
        "else:\n",
        "  print('File already exists\\n')\n",
        "  \n",
        "xy = np.loadtxt('/content/gdrive/My Drive/Colab Notebooks/diabetes.csv', delimiter=',', dtype=np.float32)\n",
        "\n",
        "x_data = xy[:,0:-1]\n",
        "y_data = xy[:, [-1]]\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=[None,8])\n",
        "Y = tf.placeholder(tf.float32, shape=[None,1])\n",
        "\n",
        "W = tf.Variable(tf.random_normal([8,1]),name='weight')\n",
        "b = tf.Variable(tf.random_normal([1]),name='bias')\n",
        "\n",
        "hypothesis = tf.sigmoid(tf.matmul(X,W)+b)\n",
        "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1-Y) * tf.log(1-hypothesis))\n",
        "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
        "\n",
        "predicted = tf.cast(hypothesis>0.5, dtype=tf.float32)\n",
        "accuracy = tf.reduce_mean(tf.cast(tf.equal(hypothesis, Y), dtype=tf.float32))\n",
        "\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "print(\"-------------------------------------------\")\n",
        "print(\" Loss values tracing \")\n",
        "print(\"-------------------------------------------\")\n",
        "for step in range(10001):\n",
        "  cost_val, _ = sess.run([cost,train], feed_dict={X:x_data, Y:y_data})\n",
        "  if step%1000 == 0:\n",
        "    print(step, cost_val)\n",
        "\n",
        "print(\"-------------------------------------------\")\n",
        "\n",
        "print(tf.concat([hypothesis, predicted], 0))\n",
        "Rst_hypothesis, Rst_predict, Rst_accuracy = sess.run([hypothesis, predicted, accuracy],feed_dict={X: x_data, Y: y_data})\n",
        "print(\"\\nHypothesis: \", Rst_hypothesis, \"\\nCorrect (Y): \", Rst_predict, \"\\nAccuracy: \", Rst_accuracy)\n",
        "print([Rst_hypothesis, Rst_predict, Rst_accuracy])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train data downloading..\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0Warning: Failed to create the file /contents/gdrive/My Drive/Colab \n",
            "Warning: Notebooks/diabetes.csv: No such file or directory\n",
            "  3 54197    3  1920    0     0  29090      0  0:00:01 --:--:--  0:00:01 29090\n",
            "curl: (23) Failed writing body (0 != 1920)\n",
            "Done..\n",
            "\n",
            "-------------------------------------------\n",
            " Loss values tracing \n",
            "-------------------------------------------\n",
            "0 0.73323053\n",
            "1000 0.62040526\n",
            "2000 0.5610019\n",
            "3000 0.5287101\n",
            "4000 0.510097\n",
            "5000 0.49869546\n",
            "6000 0.49133366\n",
            "7000 0.48637122\n",
            "8000 0.48290807\n",
            "9000 0.48042235\n",
            "10000 0.47859657\n",
            "-------------------------------------------\n",
            "Tensor(\"concat_1:0\", shape=(?, 1), dtype=float32)\n",
            "\n",
            "Hypothesis:  [[0.41140026]\n",
            " [0.93318105]\n",
            " [0.27762294]\n",
            " [0.9284421 ]\n",
            " [0.18833327]\n",
            " [0.78313506]\n",
            " [0.9322443 ]\n",
            " [0.6102789 ]\n",
            " [0.2190769 ]\n",
            " [0.52008545]\n",
            " [0.68870854]\n",
            " [0.18249992]\n",
            " [0.3151294 ]\n",
            " [0.29544652]\n",
            " [0.7614491 ]\n",
            " [0.4195692 ]\n",
            " [0.75461745]\n",
            " [0.7644009 ]\n",
            " [0.7848271 ]\n",
            " [0.56539094]\n",
            " [0.6591786 ]\n",
            " [0.09726134]\n",
            " [0.72249115]\n",
            " [0.6862008 ]\n",
            " [0.32693142]\n",
            " [0.9462006 ]\n",
            " [0.6202206 ]\n",
            " [0.64019823]\n",
            " [0.6833549 ]\n",
            " [0.45489684]\n",
            " [0.95786774]\n",
            " [0.9054591 ]\n",
            " [0.6586106 ]\n",
            " [0.84918165]\n",
            " [0.40791592]\n",
            " [0.68258953]\n",
            " [0.81013143]\n",
            " [0.49427983]\n",
            " [0.41818282]\n",
            " [0.3613699 ]\n",
            " [0.87626004]\n",
            " [0.14570206]\n",
            " [0.41996717]\n",
            " [0.07745296]\n",
            " [0.56582344]\n",
            " [0.94639885]\n",
            " [0.7161254 ]\n",
            " [0.72884476]\n",
            " [0.9530518 ]\n",
            " [0.9355931 ]\n",
            " [0.9382882 ]\n",
            " [0.227445  ]\n",
            " [0.35187393]\n",
            " [0.96750087]\n",
            " [0.18319792]\n",
            " [0.4254709 ]\n",
            " [0.14840907]\n",
            " [0.6807692 ]\n",
            " [0.8378807 ]\n",
            " [0.5084572 ]\n",
            " [0.9559711 ]\n",
            " [0.7218727 ]\n",
            " [0.6507822 ]\n",
            " [0.8739946 ]\n",
            " [0.6717376 ]\n",
            " [0.50746894]\n",
            " [0.9634229 ]\n",
            " [0.7012085 ]\n",
            " [0.85589516]\n",
            " [0.6765248 ]\n",
            " [0.27841014]\n",
            " [0.73832333]\n",
            " [0.92580223]\n",
            " [0.93824935]\n",
            " [0.8841729 ]\n",
            " [0.79805213]\n",
            " [0.39457804]\n",
            " [0.89318764]\n",
            " [0.92207587]\n",
            " [0.91167426]\n",
            " [0.88269955]\n",
            " [0.87873715]\n",
            " [0.30799267]\n",
            " [0.8116041 ]\n",
            " [0.5382074 ]\n",
            " [0.8446133 ]\n",
            " [0.4211353 ]\n",
            " [0.9148897 ]\n",
            " [0.93836486]\n",
            " [0.783566  ]\n",
            " [0.7339774 ]\n",
            " [0.64667505]\n",
            " [0.7222667 ]\n",
            " [0.56570816]\n",
            " [0.9078479 ]\n",
            " [0.9790193 ]\n",
            " [0.8956215 ]\n",
            " [0.4715651 ]\n",
            " [0.23120111]\n",
            " [0.65264016]\n",
            " [0.6741203 ]\n",
            " [0.9605236 ]\n",
            " [0.75603575]\n",
            " [0.7299362 ]\n",
            " [0.91007304]\n",
            " [0.6671716 ]\n",
            " [0.907271  ]\n",
            " [0.82314336]\n",
            " [0.46476203]\n",
            " [0.38059318]\n",
            " [0.9217423 ]\n",
            " [0.8722378 ]\n",
            " [0.4276154 ]\n",
            " [0.41333306]\n",
            " [0.61287075]\n",
            " [0.84172285]\n",
            " [0.89552784]\n",
            " [0.92396015]\n",
            " [0.1014187 ]\n",
            " [0.74215513]\n",
            " [0.8420259 ]\n",
            " [0.5917376 ]\n",
            " [0.6675307 ]\n",
            " [0.6702721 ]\n",
            " [0.63853693]\n",
            " [0.80721736]\n",
            " [0.7884437 ]\n",
            " [0.6423134 ]\n",
            " [0.52362764]\n",
            " [0.45942992]\n",
            " [0.39260197]\n",
            " [0.787924  ]\n",
            " [0.9474087 ]\n",
            " [0.8056207 ]\n",
            " [0.7865973 ]\n",
            " [0.87721145]\n",
            " [0.51633406]\n",
            " [0.7953547 ]\n",
            " [0.742035  ]\n",
            " [0.7192073 ]\n",
            " [0.86586905]\n",
            " [0.6475876 ]\n",
            " [0.5686786 ]\n",
            " [0.7332467 ]\n",
            " [0.91823614]\n",
            " [0.7530165 ]\n",
            " [0.39030218]\n",
            " [0.9448581 ]\n",
            " [0.57234824]\n",
            " [0.80704474]\n",
            " [0.29906258]\n",
            " [0.37719277]\n",
            " [0.08377996]\n",
            " [0.20009077]\n",
            " [0.90914583]\n",
            " [0.8907696 ]\n",
            " [0.94323933]\n",
            " [0.1164254 ]\n",
            " [0.5491092 ]\n",
            " [0.7266902 ]\n",
            " [0.5885207 ]\n",
            " [0.86464274]\n",
            " [0.48199552]\n",
            " [0.82389057]\n",
            " [0.6419191 ]\n",
            " [0.66081685]\n",
            " [0.7308087 ]\n",
            " [0.8823404 ]\n",
            " [0.7768266 ]\n",
            " [0.6249455 ]\n",
            " [0.9090307 ]\n",
            " [0.8304838 ]\n",
            " [0.9457543 ]\n",
            " [0.22422162]\n",
            " [0.8218155 ]\n",
            " [0.1552796 ]\n",
            " [0.30647027]\n",
            " [0.3932973 ]\n",
            " [0.902564  ]\n",
            " [0.7024578 ]\n",
            " [0.90807366]\n",
            " [0.91869175]\n",
            " [0.59813285]\n",
            " [0.15261355]\n",
            " [0.20600396]\n",
            " [0.6048604 ]\n",
            " [0.7648729 ]\n",
            " [0.6413884 ]\n",
            " [0.82723385]\n",
            " [0.5954616 ]\n",
            " [0.37393236]\n",
            " [0.17153063]\n",
            " [0.9227182 ]\n",
            " [0.35151136]\n",
            " [0.8509625 ]\n",
            " [0.9183928 ]\n",
            " [0.7005925 ]\n",
            " [0.64776564]\n",
            " [0.690574  ]\n",
            " [0.51261294]\n",
            " [0.7930912 ]\n",
            " [0.9535885 ]\n",
            " [0.7166042 ]\n",
            " [0.86080575]\n",
            " [0.1208657 ]\n",
            " [0.27835488]\n",
            " [0.8721424 ]\n",
            " [0.21766001]\n",
            " [0.9447514 ]\n",
            " [0.27164066]\n",
            " [0.24759167]\n",
            " [0.45103377]\n",
            " [0.7139844 ]\n",
            " [0.21183899]\n",
            " [0.7312075 ]\n",
            " [0.7282101 ]\n",
            " [0.862381  ]\n",
            " [0.6268791 ]\n",
            " [0.17103007]\n",
            " [0.31375614]\n",
            " [0.7350787 ]\n",
            " [0.5036377 ]\n",
            " [0.93375397]\n",
            " [0.92449236]\n",
            " [0.71236604]\n",
            " [0.3618738 ]\n",
            " [0.05905345]\n",
            " [0.60486853]\n",
            " [0.33776718]\n",
            " [0.38256803]\n",
            " [0.95445025]\n",
            " [0.6261928 ]\n",
            " [0.9476713 ]\n",
            " [0.19659144]\n",
            " [0.15964177]\n",
            " [0.36978406]\n",
            " [0.83691114]\n",
            " [0.90793645]\n",
            " [0.87860024]\n",
            " [0.67731744]\n",
            " [0.72295046]\n",
            " [0.538004  ]\n",
            " [0.20830333]\n",
            " [0.5750561 ]\n",
            " [0.14043719]\n",
            " [0.602396  ]\n",
            " [0.886909  ]\n",
            " [0.67875165]\n",
            " [0.7211014 ]\n",
            " [0.9562729 ]\n",
            " [0.83556294]\n",
            " [0.83712363]\n",
            " [0.73208004]\n",
            " [0.781877  ]\n",
            " [0.876138  ]\n",
            " [0.44479403]\n",
            " [0.40193397]\n",
            " [0.539898  ]\n",
            " [0.84404784]\n",
            " [0.6336752 ]\n",
            " [0.66973275]\n",
            " [0.8125788 ]\n",
            " [0.35895312]\n",
            " [0.48936275]\n",
            " [0.66212714]\n",
            " [0.6282766 ]\n",
            " [0.44214022]\n",
            " [0.8821515 ]\n",
            " [0.78094745]\n",
            " [0.8958324 ]\n",
            " [0.56608415]\n",
            " [0.7120854 ]\n",
            " [0.8572811 ]\n",
            " [0.8543743 ]\n",
            " [0.68959445]\n",
            " [0.8981639 ]\n",
            " [0.3559286 ]\n",
            " [0.5784408 ]\n",
            " [0.70222014]\n",
            " [0.36198997]\n",
            " [0.79689014]\n",
            " [0.30152145]\n",
            " [0.54384285]\n",
            " [0.9471804 ]\n",
            " [0.7415364 ]\n",
            " [0.8312303 ]\n",
            " [0.70092726]\n",
            " [0.4471031 ]\n",
            " [0.57719606]\n",
            " [0.40825078]\n",
            " [0.45352173]\n",
            " [0.65742964]\n",
            " [0.6842769 ]\n",
            " [0.67051804]\n",
            " [0.66131794]\n",
            " [0.23963225]\n",
            " [0.65901935]\n",
            " [0.8841573 ]\n",
            " [0.41835573]\n",
            " [0.6589915 ]\n",
            " [0.7024515 ]\n",
            " [0.50518036]\n",
            " [0.7425411 ]\n",
            " [0.60257614]\n",
            " [0.7096949 ]\n",
            " [0.91468287]\n",
            " [0.66239715]\n",
            " [0.7138427 ]\n",
            " [0.87073326]\n",
            " [0.5962595 ]\n",
            " [0.8414769 ]\n",
            " [0.95439327]\n",
            " [0.3066117 ]\n",
            " [0.7376456 ]\n",
            " [0.24719584]\n",
            " [0.8041469 ]\n",
            " [0.8273128 ]\n",
            " [0.75950813]\n",
            " [0.41480082]\n",
            " [0.753103  ]\n",
            " [0.7495746 ]\n",
            " [0.718692  ]\n",
            " [0.18684703]\n",
            " [0.757089  ]\n",
            " [0.8362788 ]\n",
            " [0.69569236]\n",
            " [0.93594503]\n",
            " [0.18570676]\n",
            " [0.7528572 ]\n",
            " [0.95217085]\n",
            " [0.18301845]\n",
            " [0.49319777]\n",
            " [0.67929524]\n",
            " [0.3587951 ]\n",
            " [0.16057324]\n",
            " [0.8613914 ]\n",
            " [0.9188776 ]\n",
            " [0.8656539 ]\n",
            " [0.6174793 ]\n",
            " [0.638067  ]\n",
            " [0.5236825 ]\n",
            " [0.7705354 ]\n",
            " [0.8359367 ]\n",
            " [0.9424521 ]\n",
            " [0.7260787 ]\n",
            " [0.73383045]\n",
            " [0.5952511 ]\n",
            " [0.93290395]\n",
            " [0.9448234 ]\n",
            " [0.6683486 ]\n",
            " [0.28647205]\n",
            " [0.6653828 ]\n",
            " [0.3822711 ]\n",
            " [0.7395848 ]\n",
            " [0.1911504 ]\n",
            " [0.27030414]\n",
            " [0.37952062]\n",
            " [0.65115505]\n",
            " [0.3027358 ]\n",
            " [0.5719311 ]\n",
            " [0.8321162 ]\n",
            " [0.6869527 ]\n",
            " [0.890054  ]\n",
            " [0.95396656]\n",
            " [0.7185439 ]\n",
            " [0.11730349]\n",
            " [0.5052242 ]\n",
            " [0.82527584]\n",
            " [0.8271877 ]\n",
            " [0.6372939 ]\n",
            " [0.26820475]\n",
            " [0.91217285]\n",
            " [0.8731291 ]\n",
            " [0.23305872]\n",
            " [0.6008943 ]\n",
            " [0.8411689 ]\n",
            " [0.90470576]\n",
            " [0.87539804]\n",
            " [0.9205693 ]\n",
            " [0.8788494 ]\n",
            " [0.9251822 ]\n",
            " [0.71693134]\n",
            " [0.5943241 ]\n",
            " [0.5357061 ]\n",
            " [0.8449689 ]\n",
            " [0.8587959 ]\n",
            " [0.19912079]\n",
            " [0.8259727 ]\n",
            " [0.8936914 ]\n",
            " [0.3466268 ]\n",
            " [0.69149435]\n",
            " [0.8715161 ]\n",
            " [0.588546  ]\n",
            " [0.93046397]\n",
            " [0.27637407]\n",
            " [0.8285054 ]\n",
            " [0.57961166]\n",
            " [0.9007414 ]\n",
            " [0.33296248]\n",
            " [0.6372951 ]\n",
            " [0.71472305]\n",
            " [0.8424381 ]\n",
            " [0.13219994]\n",
            " [0.2044856 ]\n",
            " [0.7174325 ]\n",
            " [0.8072305 ]\n",
            " [0.4648835 ]\n",
            " [0.7552744 ]\n",
            " [0.4600049 ]\n",
            " [0.38102224]\n",
            " [0.88759446]\n",
            " [0.45376176]\n",
            " [0.938166  ]\n",
            " [0.81633294]\n",
            " [0.5936079 ]\n",
            " [0.9129447 ]\n",
            " [0.58724475]\n",
            " [0.79437184]\n",
            " [0.3092786 ]\n",
            " [0.25592703]\n",
            " [0.77167094]\n",
            " [0.37419122]\n",
            " [0.4395424 ]\n",
            " [0.8750503 ]\n",
            " [0.9087942 ]\n",
            " [0.9110201 ]\n",
            " [0.95307064]\n",
            " [0.7032779 ]\n",
            " [0.92352986]\n",
            " [0.34175915]\n",
            " [0.396475  ]\n",
            " [0.5086786 ]\n",
            " [0.9509922 ]\n",
            " [0.6173023 ]\n",
            " [0.20409968]\n",
            " [0.9268049 ]\n",
            " [0.80561197]\n",
            " [0.634971  ]\n",
            " [0.8378349 ]\n",
            " [0.01613098]\n",
            " [0.92911005]\n",
            " [0.7597734 ]\n",
            " [0.76282597]\n",
            " [0.7899442 ]\n",
            " [0.9705647 ]\n",
            " [0.65765095]\n",
            " [0.7742949 ]\n",
            " [0.72707784]\n",
            " [0.8314676 ]\n",
            " [0.23126078]\n",
            " [0.5874126 ]\n",
            " [0.91163284]\n",
            " [0.56264067]\n",
            " [0.7907665 ]\n",
            " [0.95135957]\n",
            " [0.83377403]\n",
            " [0.89980173]\n",
            " [0.63508964]\n",
            " [0.8128947 ]\n",
            " [0.9524929 ]\n",
            " [0.7494316 ]\n",
            " [0.65496755]\n",
            " [0.2644993 ]\n",
            " [0.44128627]\n",
            " [0.57527506]\n",
            " [0.6501857 ]\n",
            " [0.5279675 ]\n",
            " [0.78852457]\n",
            " [0.60805243]\n",
            " [0.7659012 ]\n",
            " [0.86335826]\n",
            " [0.7400171 ]\n",
            " [0.64967847]\n",
            " [0.47066882]\n",
            " [0.6229272 ]\n",
            " [0.9361308 ]\n",
            " [0.8430283 ]\n",
            " [0.2421085 ]\n",
            " [0.41872597]\n",
            " [0.4304159 ]\n",
            " [0.09219679]\n",
            " [0.91488516]\n",
            " [0.1509105 ]\n",
            " [0.888062  ]\n",
            " [0.8738256 ]\n",
            " [0.83269393]\n",
            " [0.70569396]\n",
            " [0.8866366 ]\n",
            " [0.33651286]\n",
            " [0.7989652 ]\n",
            " [0.9422695 ]\n",
            " [0.2918342 ]\n",
            " [0.4644709 ]\n",
            " [0.865913  ]\n",
            " [0.8715792 ]\n",
            " [0.64920837]\n",
            " [0.81046724]\n",
            " [0.7922399 ]\n",
            " [0.8213217 ]\n",
            " [0.26911074]\n",
            " [0.7403673 ]\n",
            " [0.886244  ]\n",
            " [0.6324289 ]\n",
            " [0.8301866 ]\n",
            " [0.74185777]\n",
            " [0.8218251 ]\n",
            " [0.879511  ]\n",
            " [0.9344803 ]\n",
            " [0.5951085 ]\n",
            " [0.42999092]\n",
            " [0.79484826]\n",
            " [0.81336164]\n",
            " [0.96698666]\n",
            " [0.75218254]\n",
            " [0.7000078 ]\n",
            " [0.42424953]\n",
            " [0.71036303]\n",
            " [0.93575007]\n",
            " [0.9495321 ]\n",
            " [0.90178883]\n",
            " [0.73879886]\n",
            " [0.72773206]\n",
            " [0.8214048 ]\n",
            " [0.4746273 ]\n",
            " [0.75769997]\n",
            " [0.81666964]\n",
            " [0.89412713]\n",
            " [0.6169275 ]\n",
            " [0.72194904]\n",
            " [0.90713656]\n",
            " [0.52904665]\n",
            " [0.5092299 ]\n",
            " [0.63482034]\n",
            " [0.7347332 ]\n",
            " [0.69197744]\n",
            " [0.88971174]\n",
            " [0.91371274]\n",
            " [0.2130945 ]\n",
            " [0.11488006]\n",
            " [0.7539311 ]\n",
            " [0.4301547 ]\n",
            " [0.28485355]\n",
            " [0.8334733 ]\n",
            " [0.90310574]\n",
            " [0.6780379 ]\n",
            " [0.9322494 ]\n",
            " [0.9024482 ]\n",
            " [0.7778653 ]\n",
            " [0.8165902 ]\n",
            " [0.6934984 ]\n",
            " [0.5254563 ]\n",
            " [0.7992848 ]\n",
            " [0.5672872 ]\n",
            " [0.1194528 ]\n",
            " [0.8803232 ]\n",
            " [0.8919617 ]\n",
            " [0.75868976]\n",
            " [0.92919487]\n",
            " [0.80336636]\n",
            " [0.86274266]\n",
            " [0.56372505]\n",
            " [0.6892966 ]\n",
            " [0.87498957]\n",
            " [0.73277485]\n",
            " [0.8514507 ]\n",
            " [0.90192926]\n",
            " [0.6375083 ]\n",
            " [0.7431388 ]\n",
            " [0.85586834]\n",
            " [0.4715912 ]\n",
            " [0.5751952 ]\n",
            " [0.05907607]\n",
            " [0.23358163]\n",
            " [0.8680527 ]\n",
            " [0.69847256]\n",
            " [0.6525472 ]\n",
            " [0.63562685]\n",
            " [0.9541064 ]\n",
            " [0.42405924]\n",
            " [0.8387966 ]\n",
            " [0.26484883]\n",
            " [0.9250456 ]\n",
            " [0.3416279 ]\n",
            " [0.7175978 ]\n",
            " [0.54354465]\n",
            " [0.8879714 ]\n",
            " [0.5847995 ]\n",
            " [0.29902813]\n",
            " [0.74811137]\n",
            " [0.92205167]\n",
            " [0.3203414 ]\n",
            " [0.9245526 ]\n",
            " [0.897236  ]\n",
            " [0.8731874 ]\n",
            " [0.8158859 ]\n",
            " [0.39096808]\n",
            " [0.31583953]\n",
            " [0.6532121 ]\n",
            " [0.18211266]\n",
            " [0.9554213 ]\n",
            " [0.33250028]\n",
            " [0.93309796]\n",
            " [0.88070714]\n",
            " [0.40860996]\n",
            " [0.20556694]\n",
            " [0.75814795]\n",
            " [0.43574622]\n",
            " [0.84753835]\n",
            " [0.73455894]\n",
            " [0.98222196]\n",
            " [0.6456699 ]\n",
            " [0.6585392 ]\n",
            " [0.73938036]\n",
            " [0.87322086]\n",
            " [0.07755023]\n",
            " [0.6920759 ]\n",
            " [0.77078015]\n",
            " [0.80939925]\n",
            " [0.6544016 ]\n",
            " [0.46616685]\n",
            " [0.56964   ]\n",
            " [0.9118146 ]\n",
            " [0.68093544]\n",
            " [0.7751335 ]\n",
            " [0.82979685]\n",
            " [0.86842334]\n",
            " [0.8356122 ]\n",
            " [0.6274548 ]\n",
            " [0.7816997 ]\n",
            " [0.90422666]\n",
            " [0.68799937]\n",
            " [0.96168876]\n",
            " [0.83989584]\n",
            " [0.62022483]\n",
            " [0.47864828]\n",
            " [0.84571487]\n",
            " [0.8625008 ]\n",
            " [0.44566995]\n",
            " [0.6788304 ]\n",
            " [0.23435938]\n",
            " [0.5893271 ]\n",
            " [0.8590338 ]\n",
            " [0.9475337 ]\n",
            " [0.8171631 ]\n",
            " [0.7004526 ]\n",
            " [0.76702154]\n",
            " [0.8717843 ]\n",
            " [0.49628168]\n",
            " [0.93275344]\n",
            " [0.5194202 ]\n",
            " [0.85488343]\n",
            " [0.32927918]\n",
            " [0.08882269]\n",
            " [0.25478292]\n",
            " [0.32552105]\n",
            " [0.6948499 ]\n",
            " [0.7930228 ]\n",
            " [0.56112885]\n",
            " [0.7878096 ]\n",
            " [0.79266584]\n",
            " [0.48893636]\n",
            " [0.35978523]\n",
            " [0.91187   ]\n",
            " [0.8892162 ]\n",
            " [0.33420902]\n",
            " [0.640681  ]\n",
            " [0.19530478]\n",
            " [0.42099816]\n",
            " [0.75710404]\n",
            " [0.67939425]\n",
            " [0.9217698 ]\n",
            " [0.97804224]\n",
            " [0.18073565]\n",
            " [0.6880687 ]\n",
            " [0.62891835]\n",
            " [0.3988032 ]\n",
            " [0.7272786 ]\n",
            " [0.76723486]\n",
            " [0.90152997]\n",
            " [0.7647625 ]\n",
            " [0.36145866]\n",
            " [0.72713923]\n",
            " [0.16330189]\n",
            " [0.6541931 ]\n",
            " [0.464475  ]\n",
            " [0.9173962 ]\n",
            " [0.6133173 ]\n",
            " [0.5648905 ]\n",
            " [0.83539575]\n",
            " [0.74451196]\n",
            " [0.44835234]\n",
            " [0.740939  ]\n",
            " [0.68302697]\n",
            " [0.31966206]\n",
            " [0.54324496]\n",
            " [0.8878771 ]\n",
            " [0.8387221 ]\n",
            " [0.61564094]\n",
            " [0.7835878 ]\n",
            " [0.2898789 ]\n",
            " [0.8314017 ]\n",
            " [0.63730514]\n",
            " [0.7438524 ]\n",
            " [0.41791132]\n",
            " [0.68423074]\n",
            " [0.8324574 ]\n",
            " [0.19139767]\n",
            " [0.295623  ]\n",
            " [0.8574188 ]\n",
            " [0.7815284 ]\n",
            " [0.78441477]\n",
            " [0.9233179 ]\n",
            " [0.713834  ]\n",
            " [0.6598053 ]\n",
            " [0.6787272 ]\n",
            " [0.7427086 ]\n",
            " [0.66746515]\n",
            " [0.7770467 ]\n",
            " [0.5730668 ]\n",
            " [0.49561793]\n",
            " [0.8823924 ]\n",
            " [0.79180956]\n",
            " [0.699475  ]\n",
            " [0.23795271]\n",
            " [0.8822476 ]\n",
            " [0.8167275 ]\n",
            " [0.8377595 ]\n",
            " [0.6838975 ]\n",
            " [0.9054873 ]\n",
            " [0.83504415]\n",
            " [0.71771586]\n",
            " [0.39311934]\n",
            " [0.8890898 ]\n",
            " [0.91061556]\n",
            " [0.364727  ]\n",
            " [0.178098  ]\n",
            " [0.7470008 ]\n",
            " [0.33775198]\n",
            " [0.73907185]\n",
            " [0.29954034]\n",
            " [0.45318952]\n",
            " [0.4918438 ]\n",
            " [0.71059203]\n",
            " [0.8922789 ]\n",
            " [0.14609921]\n",
            " [0.40570396]\n",
            " [0.5710371 ]\n",
            " [0.5287396 ]\n",
            " [0.48836246]\n",
            " [0.76074755]\n",
            " [0.13159755]\n",
            " [0.9194552 ]\n",
            " [0.17687884]\n",
            " [0.878309  ]\n",
            " [0.6754063 ]\n",
            " [0.72901034]\n",
            " [0.8409812 ]\n",
            " [0.67819035]\n",
            " [0.9042747 ]] \n",
            "Correct (Y):  [[0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]] \n",
            "Accuracy:  0.0\n",
            "[array([[0.41140026],\n",
            "       [0.93318105],\n",
            "       [0.27762294],\n",
            "       [0.9284421 ],\n",
            "       [0.18833327],\n",
            "       [0.78313506],\n",
            "       [0.9322443 ],\n",
            "       [0.6102789 ],\n",
            "       [0.2190769 ],\n",
            "       [0.52008545],\n",
            "       [0.68870854],\n",
            "       [0.18249992],\n",
            "       [0.3151294 ],\n",
            "       [0.29544652],\n",
            "       [0.7614491 ],\n",
            "       [0.4195692 ],\n",
            "       [0.75461745],\n",
            "       [0.7644009 ],\n",
            "       [0.7848271 ],\n",
            "       [0.56539094],\n",
            "       [0.6591786 ],\n",
            "       [0.09726134],\n",
            "       [0.72249115],\n",
            "       [0.6862008 ],\n",
            "       [0.32693142],\n",
            "       [0.9462006 ],\n",
            "       [0.6202206 ],\n",
            "       [0.64019823],\n",
            "       [0.6833549 ],\n",
            "       [0.45489684],\n",
            "       [0.95786774],\n",
            "       [0.9054591 ],\n",
            "       [0.6586106 ],\n",
            "       [0.84918165],\n",
            "       [0.40791592],\n",
            "       [0.68258953],\n",
            "       [0.81013143],\n",
            "       [0.49427983],\n",
            "       [0.41818282],\n",
            "       [0.3613699 ],\n",
            "       [0.87626004],\n",
            "       [0.14570206],\n",
            "       [0.41996717],\n",
            "       [0.07745296],\n",
            "       [0.56582344],\n",
            "       [0.94639885],\n",
            "       [0.7161254 ],\n",
            "       [0.72884476],\n",
            "       [0.9530518 ],\n",
            "       [0.9355931 ],\n",
            "       [0.9382882 ],\n",
            "       [0.227445  ],\n",
            "       [0.35187393],\n",
            "       [0.96750087],\n",
            "       [0.18319792],\n",
            "       [0.4254709 ],\n",
            "       [0.14840907],\n",
            "       [0.6807692 ],\n",
            "       [0.8378807 ],\n",
            "       [0.5084572 ],\n",
            "       [0.9559711 ],\n",
            "       [0.7218727 ],\n",
            "       [0.6507822 ],\n",
            "       [0.8739946 ],\n",
            "       [0.6717376 ],\n",
            "       [0.50746894],\n",
            "       [0.9634229 ],\n",
            "       [0.7012085 ],\n",
            "       [0.85589516],\n",
            "       [0.6765248 ],\n",
            "       [0.27841014],\n",
            "       [0.73832333],\n",
            "       [0.92580223],\n",
            "       [0.93824935],\n",
            "       [0.8841729 ],\n",
            "       [0.79805213],\n",
            "       [0.39457804],\n",
            "       [0.89318764],\n",
            "       [0.92207587],\n",
            "       [0.91167426],\n",
            "       [0.88269955],\n",
            "       [0.87873715],\n",
            "       [0.30799267],\n",
            "       [0.8116041 ],\n",
            "       [0.5382074 ],\n",
            "       [0.8446133 ],\n",
            "       [0.4211353 ],\n",
            "       [0.9148897 ],\n",
            "       [0.93836486],\n",
            "       [0.783566  ],\n",
            "       [0.7339774 ],\n",
            "       [0.64667505],\n",
            "       [0.7222667 ],\n",
            "       [0.56570816],\n",
            "       [0.9078479 ],\n",
            "       [0.9790193 ],\n",
            "       [0.8956215 ],\n",
            "       [0.4715651 ],\n",
            "       [0.23120111],\n",
            "       [0.65264016],\n",
            "       [0.6741203 ],\n",
            "       [0.9605236 ],\n",
            "       [0.75603575],\n",
            "       [0.7299362 ],\n",
            "       [0.91007304],\n",
            "       [0.6671716 ],\n",
            "       [0.907271  ],\n",
            "       [0.82314336],\n",
            "       [0.46476203],\n",
            "       [0.38059318],\n",
            "       [0.9217423 ],\n",
            "       [0.8722378 ],\n",
            "       [0.4276154 ],\n",
            "       [0.41333306],\n",
            "       [0.61287075],\n",
            "       [0.84172285],\n",
            "       [0.89552784],\n",
            "       [0.92396015],\n",
            "       [0.1014187 ],\n",
            "       [0.74215513],\n",
            "       [0.8420259 ],\n",
            "       [0.5917376 ],\n",
            "       [0.6675307 ],\n",
            "       [0.6702721 ],\n",
            "       [0.63853693],\n",
            "       [0.80721736],\n",
            "       [0.7884437 ],\n",
            "       [0.6423134 ],\n",
            "       [0.52362764],\n",
            "       [0.45942992],\n",
            "       [0.39260197],\n",
            "       [0.787924  ],\n",
            "       [0.9474087 ],\n",
            "       [0.8056207 ],\n",
            "       [0.7865973 ],\n",
            "       [0.87721145],\n",
            "       [0.51633406],\n",
            "       [0.7953547 ],\n",
            "       [0.742035  ],\n",
            "       [0.7192073 ],\n",
            "       [0.86586905],\n",
            "       [0.6475876 ],\n",
            "       [0.5686786 ],\n",
            "       [0.7332467 ],\n",
            "       [0.91823614],\n",
            "       [0.7530165 ],\n",
            "       [0.39030218],\n",
            "       [0.9448581 ],\n",
            "       [0.57234824],\n",
            "       [0.80704474],\n",
            "       [0.29906258],\n",
            "       [0.37719277],\n",
            "       [0.08377996],\n",
            "       [0.20009077],\n",
            "       [0.90914583],\n",
            "       [0.8907696 ],\n",
            "       [0.94323933],\n",
            "       [0.1164254 ],\n",
            "       [0.5491092 ],\n",
            "       [0.7266902 ],\n",
            "       [0.5885207 ],\n",
            "       [0.86464274],\n",
            "       [0.48199552],\n",
            "       [0.82389057],\n",
            "       [0.6419191 ],\n",
            "       [0.66081685],\n",
            "       [0.7308087 ],\n",
            "       [0.8823404 ],\n",
            "       [0.7768266 ],\n",
            "       [0.6249455 ],\n",
            "       [0.9090307 ],\n",
            "       [0.8304838 ],\n",
            "       [0.9457543 ],\n",
            "       [0.22422162],\n",
            "       [0.8218155 ],\n",
            "       [0.1552796 ],\n",
            "       [0.30647027],\n",
            "       [0.3932973 ],\n",
            "       [0.902564  ],\n",
            "       [0.7024578 ],\n",
            "       [0.90807366],\n",
            "       [0.91869175],\n",
            "       [0.59813285],\n",
            "       [0.15261355],\n",
            "       [0.20600396],\n",
            "       [0.6048604 ],\n",
            "       [0.7648729 ],\n",
            "       [0.6413884 ],\n",
            "       [0.82723385],\n",
            "       [0.5954616 ],\n",
            "       [0.37393236],\n",
            "       [0.17153063],\n",
            "       [0.9227182 ],\n",
            "       [0.35151136],\n",
            "       [0.8509625 ],\n",
            "       [0.9183928 ],\n",
            "       [0.7005925 ],\n",
            "       [0.64776564],\n",
            "       [0.690574  ],\n",
            "       [0.51261294],\n",
            "       [0.7930912 ],\n",
            "       [0.9535885 ],\n",
            "       [0.7166042 ],\n",
            "       [0.86080575],\n",
            "       [0.1208657 ],\n",
            "       [0.27835488],\n",
            "       [0.8721424 ],\n",
            "       [0.21766001],\n",
            "       [0.9447514 ],\n",
            "       [0.27164066],\n",
            "       [0.24759167],\n",
            "       [0.45103377],\n",
            "       [0.7139844 ],\n",
            "       [0.21183899],\n",
            "       [0.7312075 ],\n",
            "       [0.7282101 ],\n",
            "       [0.862381  ],\n",
            "       [0.6268791 ],\n",
            "       [0.17103007],\n",
            "       [0.31375614],\n",
            "       [0.7350787 ],\n",
            "       [0.5036377 ],\n",
            "       [0.93375397],\n",
            "       [0.92449236],\n",
            "       [0.71236604],\n",
            "       [0.3618738 ],\n",
            "       [0.05905345],\n",
            "       [0.60486853],\n",
            "       [0.33776718],\n",
            "       [0.38256803],\n",
            "       [0.95445025],\n",
            "       [0.6261928 ],\n",
            "       [0.9476713 ],\n",
            "       [0.19659144],\n",
            "       [0.15964177],\n",
            "       [0.36978406],\n",
            "       [0.83691114],\n",
            "       [0.90793645],\n",
            "       [0.87860024],\n",
            "       [0.67731744],\n",
            "       [0.72295046],\n",
            "       [0.538004  ],\n",
            "       [0.20830333],\n",
            "       [0.5750561 ],\n",
            "       [0.14043719],\n",
            "       [0.602396  ],\n",
            "       [0.886909  ],\n",
            "       [0.67875165],\n",
            "       [0.7211014 ],\n",
            "       [0.9562729 ],\n",
            "       [0.83556294],\n",
            "       [0.83712363],\n",
            "       [0.73208004],\n",
            "       [0.781877  ],\n",
            "       [0.876138  ],\n",
            "       [0.44479403],\n",
            "       [0.40193397],\n",
            "       [0.539898  ],\n",
            "       [0.84404784],\n",
            "       [0.6336752 ],\n",
            "       [0.66973275],\n",
            "       [0.8125788 ],\n",
            "       [0.35895312],\n",
            "       [0.48936275],\n",
            "       [0.66212714],\n",
            "       [0.6282766 ],\n",
            "       [0.44214022],\n",
            "       [0.8821515 ],\n",
            "       [0.78094745],\n",
            "       [0.8958324 ],\n",
            "       [0.56608415],\n",
            "       [0.7120854 ],\n",
            "       [0.8572811 ],\n",
            "       [0.8543743 ],\n",
            "       [0.68959445],\n",
            "       [0.8981639 ],\n",
            "       [0.3559286 ],\n",
            "       [0.5784408 ],\n",
            "       [0.70222014],\n",
            "       [0.36198997],\n",
            "       [0.79689014],\n",
            "       [0.30152145],\n",
            "       [0.54384285],\n",
            "       [0.9471804 ],\n",
            "       [0.7415364 ],\n",
            "       [0.8312303 ],\n",
            "       [0.70092726],\n",
            "       [0.4471031 ],\n",
            "       [0.57719606],\n",
            "       [0.40825078],\n",
            "       [0.45352173],\n",
            "       [0.65742964],\n",
            "       [0.6842769 ],\n",
            "       [0.67051804],\n",
            "       [0.66131794],\n",
            "       [0.23963225],\n",
            "       [0.65901935],\n",
            "       [0.8841573 ],\n",
            "       [0.41835573],\n",
            "       [0.6589915 ],\n",
            "       [0.7024515 ],\n",
            "       [0.50518036],\n",
            "       [0.7425411 ],\n",
            "       [0.60257614],\n",
            "       [0.7096949 ],\n",
            "       [0.91468287],\n",
            "       [0.66239715],\n",
            "       [0.7138427 ],\n",
            "       [0.87073326],\n",
            "       [0.5962595 ],\n",
            "       [0.8414769 ],\n",
            "       [0.95439327],\n",
            "       [0.3066117 ],\n",
            "       [0.7376456 ],\n",
            "       [0.24719584],\n",
            "       [0.8041469 ],\n",
            "       [0.8273128 ],\n",
            "       [0.75950813],\n",
            "       [0.41480082],\n",
            "       [0.753103  ],\n",
            "       [0.7495746 ],\n",
            "       [0.718692  ],\n",
            "       [0.18684703],\n",
            "       [0.757089  ],\n",
            "       [0.8362788 ],\n",
            "       [0.69569236],\n",
            "       [0.93594503],\n",
            "       [0.18570676],\n",
            "       [0.7528572 ],\n",
            "       [0.95217085],\n",
            "       [0.18301845],\n",
            "       [0.49319777],\n",
            "       [0.67929524],\n",
            "       [0.3587951 ],\n",
            "       [0.16057324],\n",
            "       [0.8613914 ],\n",
            "       [0.9188776 ],\n",
            "       [0.8656539 ],\n",
            "       [0.6174793 ],\n",
            "       [0.638067  ],\n",
            "       [0.5236825 ],\n",
            "       [0.7705354 ],\n",
            "       [0.8359367 ],\n",
            "       [0.9424521 ],\n",
            "       [0.7260787 ],\n",
            "       [0.73383045],\n",
            "       [0.5952511 ],\n",
            "       [0.93290395],\n",
            "       [0.9448234 ],\n",
            "       [0.6683486 ],\n",
            "       [0.28647205],\n",
            "       [0.6653828 ],\n",
            "       [0.3822711 ],\n",
            "       [0.7395848 ],\n",
            "       [0.1911504 ],\n",
            "       [0.27030414],\n",
            "       [0.37952062],\n",
            "       [0.65115505],\n",
            "       [0.3027358 ],\n",
            "       [0.5719311 ],\n",
            "       [0.8321162 ],\n",
            "       [0.6869527 ],\n",
            "       [0.890054  ],\n",
            "       [0.95396656],\n",
            "       [0.7185439 ],\n",
            "       [0.11730349],\n",
            "       [0.5052242 ],\n",
            "       [0.82527584],\n",
            "       [0.8271877 ],\n",
            "       [0.6372939 ],\n",
            "       [0.26820475],\n",
            "       [0.91217285],\n",
            "       [0.8731291 ],\n",
            "       [0.23305872],\n",
            "       [0.6008943 ],\n",
            "       [0.8411689 ],\n",
            "       [0.90470576],\n",
            "       [0.87539804],\n",
            "       [0.9205693 ],\n",
            "       [0.8788494 ],\n",
            "       [0.9251822 ],\n",
            "       [0.71693134],\n",
            "       [0.5943241 ],\n",
            "       [0.5357061 ],\n",
            "       [0.8449689 ],\n",
            "       [0.8587959 ],\n",
            "       [0.19912079],\n",
            "       [0.8259727 ],\n",
            "       [0.8936914 ],\n",
            "       [0.3466268 ],\n",
            "       [0.69149435],\n",
            "       [0.8715161 ],\n",
            "       [0.588546  ],\n",
            "       [0.93046397],\n",
            "       [0.27637407],\n",
            "       [0.8285054 ],\n",
            "       [0.57961166],\n",
            "       [0.9007414 ],\n",
            "       [0.33296248],\n",
            "       [0.6372951 ],\n",
            "       [0.71472305],\n",
            "       [0.8424381 ],\n",
            "       [0.13219994],\n",
            "       [0.2044856 ],\n",
            "       [0.7174325 ],\n",
            "       [0.8072305 ],\n",
            "       [0.4648835 ],\n",
            "       [0.7552744 ],\n",
            "       [0.4600049 ],\n",
            "       [0.38102224],\n",
            "       [0.88759446],\n",
            "       [0.45376176],\n",
            "       [0.938166  ],\n",
            "       [0.81633294],\n",
            "       [0.5936079 ],\n",
            "       [0.9129447 ],\n",
            "       [0.58724475],\n",
            "       [0.79437184],\n",
            "       [0.3092786 ],\n",
            "       [0.25592703],\n",
            "       [0.77167094],\n",
            "       [0.37419122],\n",
            "       [0.4395424 ],\n",
            "       [0.8750503 ],\n",
            "       [0.9087942 ],\n",
            "       [0.9110201 ],\n",
            "       [0.95307064],\n",
            "       [0.7032779 ],\n",
            "       [0.92352986],\n",
            "       [0.34175915],\n",
            "       [0.396475  ],\n",
            "       [0.5086786 ],\n",
            "       [0.9509922 ],\n",
            "       [0.6173023 ],\n",
            "       [0.20409968],\n",
            "       [0.9268049 ],\n",
            "       [0.80561197],\n",
            "       [0.634971  ],\n",
            "       [0.8378349 ],\n",
            "       [0.01613098],\n",
            "       [0.92911005],\n",
            "       [0.7597734 ],\n",
            "       [0.76282597],\n",
            "       [0.7899442 ],\n",
            "       [0.9705647 ],\n",
            "       [0.65765095],\n",
            "       [0.7742949 ],\n",
            "       [0.72707784],\n",
            "       [0.8314676 ],\n",
            "       [0.23126078],\n",
            "       [0.5874126 ],\n",
            "       [0.91163284],\n",
            "       [0.56264067],\n",
            "       [0.7907665 ],\n",
            "       [0.95135957],\n",
            "       [0.83377403],\n",
            "       [0.89980173],\n",
            "       [0.63508964],\n",
            "       [0.8128947 ],\n",
            "       [0.9524929 ],\n",
            "       [0.7494316 ],\n",
            "       [0.65496755],\n",
            "       [0.2644993 ],\n",
            "       [0.44128627],\n",
            "       [0.57527506],\n",
            "       [0.6501857 ],\n",
            "       [0.5279675 ],\n",
            "       [0.78852457],\n",
            "       [0.60805243],\n",
            "       [0.7659012 ],\n",
            "       [0.86335826],\n",
            "       [0.7400171 ],\n",
            "       [0.64967847],\n",
            "       [0.47066882],\n",
            "       [0.6229272 ],\n",
            "       [0.9361308 ],\n",
            "       [0.8430283 ],\n",
            "       [0.2421085 ],\n",
            "       [0.41872597],\n",
            "       [0.4304159 ],\n",
            "       [0.09219679],\n",
            "       [0.91488516],\n",
            "       [0.1509105 ],\n",
            "       [0.888062  ],\n",
            "       [0.8738256 ],\n",
            "       [0.83269393],\n",
            "       [0.70569396],\n",
            "       [0.8866366 ],\n",
            "       [0.33651286],\n",
            "       [0.7989652 ],\n",
            "       [0.9422695 ],\n",
            "       [0.2918342 ],\n",
            "       [0.4644709 ],\n",
            "       [0.865913  ],\n",
            "       [0.8715792 ],\n",
            "       [0.64920837],\n",
            "       [0.81046724],\n",
            "       [0.7922399 ],\n",
            "       [0.8213217 ],\n",
            "       [0.26911074],\n",
            "       [0.7403673 ],\n",
            "       [0.886244  ],\n",
            "       [0.6324289 ],\n",
            "       [0.8301866 ],\n",
            "       [0.74185777],\n",
            "       [0.8218251 ],\n",
            "       [0.879511  ],\n",
            "       [0.9344803 ],\n",
            "       [0.5951085 ],\n",
            "       [0.42999092],\n",
            "       [0.79484826],\n",
            "       [0.81336164],\n",
            "       [0.96698666],\n",
            "       [0.75218254],\n",
            "       [0.7000078 ],\n",
            "       [0.42424953],\n",
            "       [0.71036303],\n",
            "       [0.93575007],\n",
            "       [0.9495321 ],\n",
            "       [0.90178883],\n",
            "       [0.73879886],\n",
            "       [0.72773206],\n",
            "       [0.8214048 ],\n",
            "       [0.4746273 ],\n",
            "       [0.75769997],\n",
            "       [0.81666964],\n",
            "       [0.89412713],\n",
            "       [0.6169275 ],\n",
            "       [0.72194904],\n",
            "       [0.90713656],\n",
            "       [0.52904665],\n",
            "       [0.5092299 ],\n",
            "       [0.63482034],\n",
            "       [0.7347332 ],\n",
            "       [0.69197744],\n",
            "       [0.88971174],\n",
            "       [0.91371274],\n",
            "       [0.2130945 ],\n",
            "       [0.11488006],\n",
            "       [0.7539311 ],\n",
            "       [0.4301547 ],\n",
            "       [0.28485355],\n",
            "       [0.8334733 ],\n",
            "       [0.90310574],\n",
            "       [0.6780379 ],\n",
            "       [0.9322494 ],\n",
            "       [0.9024482 ],\n",
            "       [0.7778653 ],\n",
            "       [0.8165902 ],\n",
            "       [0.6934984 ],\n",
            "       [0.5254563 ],\n",
            "       [0.7992848 ],\n",
            "       [0.5672872 ],\n",
            "       [0.1194528 ],\n",
            "       [0.8803232 ],\n",
            "       [0.8919617 ],\n",
            "       [0.75868976],\n",
            "       [0.92919487],\n",
            "       [0.80336636],\n",
            "       [0.86274266],\n",
            "       [0.56372505],\n",
            "       [0.6892966 ],\n",
            "       [0.87498957],\n",
            "       [0.73277485],\n",
            "       [0.8514507 ],\n",
            "       [0.90192926],\n",
            "       [0.6375083 ],\n",
            "       [0.7431388 ],\n",
            "       [0.85586834],\n",
            "       [0.4715912 ],\n",
            "       [0.5751952 ],\n",
            "       [0.05907607],\n",
            "       [0.23358163],\n",
            "       [0.8680527 ],\n",
            "       [0.69847256],\n",
            "       [0.6525472 ],\n",
            "       [0.63562685],\n",
            "       [0.9541064 ],\n",
            "       [0.42405924],\n",
            "       [0.8387966 ],\n",
            "       [0.26484883],\n",
            "       [0.9250456 ],\n",
            "       [0.3416279 ],\n",
            "       [0.7175978 ],\n",
            "       [0.54354465],\n",
            "       [0.8879714 ],\n",
            "       [0.5847995 ],\n",
            "       [0.29902813],\n",
            "       [0.74811137],\n",
            "       [0.92205167],\n",
            "       [0.3203414 ],\n",
            "       [0.9245526 ],\n",
            "       [0.897236  ],\n",
            "       [0.8731874 ],\n",
            "       [0.8158859 ],\n",
            "       [0.39096808],\n",
            "       [0.31583953],\n",
            "       [0.6532121 ],\n",
            "       [0.18211266],\n",
            "       [0.9554213 ],\n",
            "       [0.33250028],\n",
            "       [0.93309796],\n",
            "       [0.88070714],\n",
            "       [0.40860996],\n",
            "       [0.20556694],\n",
            "       [0.75814795],\n",
            "       [0.43574622],\n",
            "       [0.84753835],\n",
            "       [0.73455894],\n",
            "       [0.98222196],\n",
            "       [0.6456699 ],\n",
            "       [0.6585392 ],\n",
            "       [0.73938036],\n",
            "       [0.87322086],\n",
            "       [0.07755023],\n",
            "       [0.6920759 ],\n",
            "       [0.77078015],\n",
            "       [0.80939925],\n",
            "       [0.6544016 ],\n",
            "       [0.46616685],\n",
            "       [0.56964   ],\n",
            "       [0.9118146 ],\n",
            "       [0.68093544],\n",
            "       [0.7751335 ],\n",
            "       [0.82979685],\n",
            "       [0.86842334],\n",
            "       [0.8356122 ],\n",
            "       [0.6274548 ],\n",
            "       [0.7816997 ],\n",
            "       [0.90422666],\n",
            "       [0.68799937],\n",
            "       [0.96168876],\n",
            "       [0.83989584],\n",
            "       [0.62022483],\n",
            "       [0.47864828],\n",
            "       [0.84571487],\n",
            "       [0.8625008 ],\n",
            "       [0.44566995],\n",
            "       [0.6788304 ],\n",
            "       [0.23435938],\n",
            "       [0.5893271 ],\n",
            "       [0.8590338 ],\n",
            "       [0.9475337 ],\n",
            "       [0.8171631 ],\n",
            "       [0.7004526 ],\n",
            "       [0.76702154],\n",
            "       [0.8717843 ],\n",
            "       [0.49628168],\n",
            "       [0.93275344],\n",
            "       [0.5194202 ],\n",
            "       [0.85488343],\n",
            "       [0.32927918],\n",
            "       [0.08882269],\n",
            "       [0.25478292],\n",
            "       [0.32552105],\n",
            "       [0.6948499 ],\n",
            "       [0.7930228 ],\n",
            "       [0.56112885],\n",
            "       [0.7878096 ],\n",
            "       [0.79266584],\n",
            "       [0.48893636],\n",
            "       [0.35978523],\n",
            "       [0.91187   ],\n",
            "       [0.8892162 ],\n",
            "       [0.33420902],\n",
            "       [0.640681  ],\n",
            "       [0.19530478],\n",
            "       [0.42099816],\n",
            "       [0.75710404],\n",
            "       [0.67939425],\n",
            "       [0.9217698 ],\n",
            "       [0.97804224],\n",
            "       [0.18073565],\n",
            "       [0.6880687 ],\n",
            "       [0.62891835],\n",
            "       [0.3988032 ],\n",
            "       [0.7272786 ],\n",
            "       [0.76723486],\n",
            "       [0.90152997],\n",
            "       [0.7647625 ],\n",
            "       [0.36145866],\n",
            "       [0.72713923],\n",
            "       [0.16330189],\n",
            "       [0.6541931 ],\n",
            "       [0.464475  ],\n",
            "       [0.9173962 ],\n",
            "       [0.6133173 ],\n",
            "       [0.5648905 ],\n",
            "       [0.83539575],\n",
            "       [0.74451196],\n",
            "       [0.44835234],\n",
            "       [0.740939  ],\n",
            "       [0.68302697],\n",
            "       [0.31966206],\n",
            "       [0.54324496],\n",
            "       [0.8878771 ],\n",
            "       [0.8387221 ],\n",
            "       [0.61564094],\n",
            "       [0.7835878 ],\n",
            "       [0.2898789 ],\n",
            "       [0.8314017 ],\n",
            "       [0.63730514],\n",
            "       [0.7438524 ],\n",
            "       [0.41791132],\n",
            "       [0.68423074],\n",
            "       [0.8324574 ],\n",
            "       [0.19139767],\n",
            "       [0.295623  ],\n",
            "       [0.8574188 ],\n",
            "       [0.7815284 ],\n",
            "       [0.78441477],\n",
            "       [0.9233179 ],\n",
            "       [0.713834  ],\n",
            "       [0.6598053 ],\n",
            "       [0.6787272 ],\n",
            "       [0.7427086 ],\n",
            "       [0.66746515],\n",
            "       [0.7770467 ],\n",
            "       [0.5730668 ],\n",
            "       [0.49561793],\n",
            "       [0.8823924 ],\n",
            "       [0.79180956],\n",
            "       [0.699475  ],\n",
            "       [0.23795271],\n",
            "       [0.8822476 ],\n",
            "       [0.8167275 ],\n",
            "       [0.8377595 ],\n",
            "       [0.6838975 ],\n",
            "       [0.9054873 ],\n",
            "       [0.83504415],\n",
            "       [0.71771586],\n",
            "       [0.39311934],\n",
            "       [0.8890898 ],\n",
            "       [0.91061556],\n",
            "       [0.364727  ],\n",
            "       [0.178098  ],\n",
            "       [0.7470008 ],\n",
            "       [0.33775198],\n",
            "       [0.73907185],\n",
            "       [0.29954034],\n",
            "       [0.45318952],\n",
            "       [0.4918438 ],\n",
            "       [0.71059203],\n",
            "       [0.8922789 ],\n",
            "       [0.14609921],\n",
            "       [0.40570396],\n",
            "       [0.5710371 ],\n",
            "       [0.5287396 ],\n",
            "       [0.48836246],\n",
            "       [0.76074755],\n",
            "       [0.13159755],\n",
            "       [0.9194552 ],\n",
            "       [0.17687884],\n",
            "       [0.878309  ],\n",
            "       [0.6754063 ],\n",
            "       [0.72901034],\n",
            "       [0.8409812 ],\n",
            "       [0.67819035],\n",
            "       [0.9042747 ]], dtype=float32), array([[0.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [0.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.],\n",
            "       [1.]], dtype=float32), 0.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6aWvb0rKALlV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3. Multinomial Classification\n",
        "\n",
        "* 클래스 3개인 데이터 학습\n",
        "* 1번 클래스: [1, 0 0], 2번 클래스: [0,1,0], 3번 클래스: [0,0,1] 로 표기"
      ]
    },
    {
      "metadata": {
        "id": "Ge2jQUflFJPO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "outputId": "b902fc2e-7e35-4044-936b-40bf1cfdc9b3"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "tf.set_random_seed(777)\n",
        "\n",
        "x_data = [[1, 2, 1, 1],\n",
        "          [2, 1, 3, 2],\n",
        "          [3, 1, 3, 4],\n",
        "          [4, 1, 5, 5],\n",
        "          [1, 7, 5, 5],\n",
        "          [1, 2, 5, 6],\n",
        "          [1, 6, 6, 6],\n",
        "          [1, 7, 7, 7]]\n",
        "y_data = [[0, 0, 1],\n",
        "          [0, 0, 1],\n",
        "          [0, 0, 1],\n",
        "          [0, 1, 0],\n",
        "          [0, 1, 0],\n",
        "          [0, 1, 0],\n",
        "          [1, 0, 0],\n",
        "          [1, 0, 0]]\n",
        "nb_classes = 3\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=[None,4])\n",
        "Y = tf.placeholder(tf.float32, shape=[None,3])\n",
        "\n",
        "W = tf.Variable(tf.random_normal([4,nb_classes]),name='weight')\n",
        "b = tf.Variable(tf.random_normal([nb_classes]),name='bias')\n",
        "\n",
        "hypothesis = tf.nn.softmax(tf.matmul(X,W)+b)\n",
        "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
        "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
        "\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "for step in range(2001):\n",
        "  _, cost_val = sess.run([train, cost], feed_dict={X:x_data, Y:y_data})\n",
        "  if step%1000==0:\n",
        "    print(step,cost_val)\n",
        "    \n",
        "print('--------------')\n",
        "# Testing & One-hot encoding\n",
        "a = sess.run(hypothesis, feed_dict={X: [[1, 11, 7, 9]]})\n",
        "print(a, sess.run(tf.argmax(a, 1)))\n",
        "\n",
        "print('--------------')\n",
        "b = sess.run(hypothesis, feed_dict={X: [[1, 3, 4, 3]]})\n",
        "print(b, sess.run(tf.argmax(b, 1)))\n",
        "\n",
        "print('--------------')\n",
        "c = sess.run(hypothesis, feed_dict={X: [[1, 1, 0, 1]]})\n",
        "print(c, sess.run(tf.argmax(c, 1)))\n",
        "\n",
        "print('--------------')\n",
        "all = sess.run(hypothesis, feed_dict={X: [[1, 11, 7, 9], [1, 3, 4, 3], [1, 1, 0, 1]]})\n",
        "print(all, sess.run(tf.argmax(all, 1)))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "0 5.238928\n",
            "1000 0.2668044\n",
            "2000 0.16057643\n",
            "--------------\n",
            "[[1.6168805e-02 9.8382288e-01 8.3161503e-06]] [1]\n",
            "--------------\n",
            "[[0.76160705 0.21634768 0.02204525]] [0]\n",
            "--------------\n",
            "[[1.5344991e-08 3.2354318e-04 9.9967647e-01]] [2]\n",
            "--------------\n",
            "[[1.6168805e-02 9.8382288e-01 8.3161513e-06]\n",
            " [7.6160705e-01 2.1634768e-01 2.2045251e-02]\n",
            " [1.5344991e-08 3.2354321e-04 9.9967647e-01]] [1 0 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7Bd4bNOY1bv8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 4. 동물 판별하는 분류기 만들기\n",
        "\n",
        "- Zoo 데이터 <br>\n",
        "  - Row: 동물 하나의 데이터\n",
        "  - 1~16: 특성데이터, 17: 동물 클래스 번호\n",
        "\n",
        "![캡처](https://user-images.githubusercontent.com/11758940/55483288-402cb900-5661-11e9-82e8-040724eccfb9.PNG)\n"
      ]
    },
    {
      "metadata": {
        "id": "uUNsYCJ4IDVA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2193
        },
        "outputId": "8381a595-c7ad-4f83-9464-79290d232a64"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "tf.set_random_seed(777)\n",
        "\n",
        "if os.path.exists('/content/gdrive/My Drive/Colab Notebooks/data-zoo-v2.csv')==False:\n",
        "    print('Train data downloading..')\n",
        "    ! curl 'https://raw.githubusercontent.com/unizard/2019.Spring.AI/master/data-zoo-v2.csv' -o '/content/gdrive/My Drive/Colab Notebooks/data-zoo-v2.csv'\n",
        "    print('Done..\\n')\n",
        "else:\n",
        "    print('File already exists \\n')\n",
        "    \n",
        "    \n",
        "xy = np.loadtxt('/content/gdrive/My Drive/Colab Notebooks/data-zoo-v2.csv', delimiter=',', dtype=np.float32)\n",
        "x_data = xy[:, 0:-1]\n",
        "y_data = xy[:, [-1]]\n",
        "\n",
        "# 학습 데이터 사이즈 확인\n",
        "print(x_data.shape, y_data.shape)\n",
        "\n",
        "# 학습 데이터 클래스 설정\n",
        "nb_classes = 7 # 0~6\n",
        "\n",
        "X = tf.placeholder(tf.float32, [None,16])\n",
        "Y = tf.placeholder(tf.int32, [None,1]) # 0~6\n",
        "\n",
        "# 변환에 주의할 것\n",
        "\n",
        "# one-hot(원핫)인코딩이란? 단 하나의 값만 True이고 나머지는 모두 False인 인코딩을 말한다.\n",
        "# 즉, 1개만 Hot(True)이고 나머지는 Cold(False)이다.\n",
        "# 예를들면 [0, 0, 0, 0, 1]이다. 5번째(Zero-based 인덱스이므로 4)만 1이고 나머지는 0이다.\n",
        "# 행렬을 자주 사용하는 연산에서는 4와 같은 스칼라값보다 [0, 0, 0, 0, 1]와 같은 행렬이 자주 사용된다.\n",
        "'''\n",
        "one_hot: Tensor(\"one_hot:0\", shape=(?, 1, 7), dtype=float32)\n",
        "reshape one_hot: Tensor(\"Reshape:0\", shape=(?, 7), dtype=float32)\n",
        "'''\n",
        "Y_one_hot = tf.one_hot(Y, nb_classes)  # one hot\n",
        "print(\"one_hot:\", Y_one_hot)\n",
        "Y_one_hot = tf.reshape(Y_one_hot, [-1, nb_classes])\n",
        "print(\"reshape one_hot:\", Y_one_hot)\n",
        "\n",
        "\n",
        "# 학습 파라미터 초기화\n",
        "W = tf.Variable(tf.random_normal([16, nb_classes]), name='weight')\n",
        "b = tf.Variable(tf.random_normal([nb_classes]), name='bias')\n",
        "\n",
        "\n",
        "\n",
        "# 가설 함수 그래프 설정\n",
        "logits = tf.matmul(X, W) + b\n",
        "hypothesis = tf.nn.softmax(logits)\n",
        "\n",
        "\n",
        "\n",
        "# 비용 함수 그래프 설정, Cross Entropy 로 설정\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,labels=tf.stop_gradient([Y_one_hot])))\n",
        "\n",
        "\n",
        "# 최적화 함수 설정(경사하강법)\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
        "\n",
        "\n",
        "\n",
        "# 예측 그래프, \n",
        "prediction = tf.argmax(hypothesis, 1)\n",
        "correct_prediction = tf.equal(prediction, tf.argmax(Y_one_hot, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 세션 생성 및 변수 초기화\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "\n",
        "# 세션 실행\n",
        "for step in range(2001):\n",
        "    # 실행하면서 비용함수와 정확도 출력\n",
        "    _, cost_val, acc_val = sess.run([optimizer, cost, accuracy], feed_dict={X: x_data, Y: y_data})\n",
        "    if step % 100 == 0:\n",
        "        print(\"Step: {:5}\\tCost: {:.3f}\\tAcc: {:.2%}\".format(step, cost_val, acc_val))\n",
        "\n",
        "        \n",
        "        \n",
        "# 예측값과 정답을 함께 출력하기\n",
        "cnt = 0\n",
        "pred = sess.run(prediction, feed_dict={X: x_data})\n",
        "for p, y in zip(pred, y_data.flatten()):\n",
        "    print(\"[{}] Prediction: {} True Y: {}\".format(p == int(y), p, int(y)))\n",
        "    cnt = cnt + 1\n",
        "        \n",
        "# 최종 정확도 출력 \n",
        "print(\"Accuracy: \", cnt/pred.size*100)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File already exists \n",
            "\n",
            "(101, 16) (101, 1)\n",
            "one_hot: Tensor(\"one_hot:0\", shape=(?, 1, 7), dtype=float32)\n",
            "reshape one_hot: Tensor(\"Reshape:0\", shape=(?, 7), dtype=float32)\n",
            "Step:     0\tCost: 6.993\tAcc: 2.97%\n",
            "Step:   100\tCost: 0.771\tAcc: 83.17%\n",
            "Step:   200\tCost: 0.479\tAcc: 86.14%\n",
            "Step:   300\tCost: 0.354\tAcc: 88.12%\n",
            "Step:   400\tCost: 0.282\tAcc: 89.11%\n",
            "Step:   500\tCost: 0.234\tAcc: 93.07%\n",
            "Step:   600\tCost: 0.199\tAcc: 95.05%\n",
            "Step:   700\tCost: 0.173\tAcc: 96.04%\n",
            "Step:   800\tCost: 0.153\tAcc: 99.01%\n",
            "Step:   900\tCost: 0.137\tAcc: 99.01%\n",
            "Step:  1000\tCost: 0.123\tAcc: 99.01%\n",
            "Step:  1100\tCost: 0.112\tAcc: 99.01%\n",
            "Step:  1200\tCost: 0.103\tAcc: 99.01%\n",
            "Step:  1300\tCost: 0.095\tAcc: 99.01%\n",
            "Step:  1400\tCost: 0.088\tAcc: 99.01%\n",
            "Step:  1500\tCost: 0.081\tAcc: 100.00%\n",
            "Step:  1600\tCost: 0.076\tAcc: 100.00%\n",
            "Step:  1700\tCost: 0.071\tAcc: 100.00%\n",
            "Step:  1800\tCost: 0.067\tAcc: 100.00%\n",
            "Step:  1900\tCost: 0.063\tAcc: 100.00%\n",
            "Step:  2000\tCost: 0.060\tAcc: 100.00%\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 3 True Y: 3\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 3 True Y: 3\n",
            "[True] Prediction: 3 True Y: 3\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 1 True Y: 1\n",
            "[True] Prediction: 3 True Y: 3\n",
            "[True] Prediction: 6 True Y: 6\n",
            "[True] Prediction: 6 True Y: 6\n",
            "[True] Prediction: 6 True Y: 6\n",
            "[True] Prediction: 1 True Y: 1\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 3 True Y: 3\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 1 True Y: 1\n",
            "[True] Prediction: 1 True Y: 1\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 1 True Y: 1\n",
            "[True] Prediction: 5 True Y: 5\n",
            "[True] Prediction: 4 True Y: 4\n",
            "[True] Prediction: 4 True Y: 4\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 5 True Y: 5\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 1 True Y: 1\n",
            "[True] Prediction: 3 True Y: 3\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 1 True Y: 1\n",
            "[True] Prediction: 3 True Y: 3\n",
            "[True] Prediction: 5 True Y: 5\n",
            "[True] Prediction: 5 True Y: 5\n",
            "[True] Prediction: 1 True Y: 1\n",
            "[True] Prediction: 5 True Y: 5\n",
            "[True] Prediction: 1 True Y: 1\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 6 True Y: 6\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 5 True Y: 5\n",
            "[True] Prediction: 4 True Y: 4\n",
            "[True] Prediction: 6 True Y: 6\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 1 True Y: 1\n",
            "[True] Prediction: 1 True Y: 1\n",
            "[True] Prediction: 1 True Y: 1\n",
            "[True] Prediction: 1 True Y: 1\n",
            "[True] Prediction: 3 True Y: 3\n",
            "[True] Prediction: 3 True Y: 3\n",
            "[True] Prediction: 2 True Y: 2\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 1 True Y: 1\n",
            "[True] Prediction: 6 True Y: 6\n",
            "[True] Prediction: 3 True Y: 3\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 2 True Y: 2\n",
            "[True] Prediction: 6 True Y: 6\n",
            "[True] Prediction: 1 True Y: 1\n",
            "[True] Prediction: 1 True Y: 1\n",
            "[True] Prediction: 2 True Y: 2\n",
            "[True] Prediction: 6 True Y: 6\n",
            "[True] Prediction: 3 True Y: 3\n",
            "[True] Prediction: 1 True Y: 1\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 6 True Y: 6\n",
            "[True] Prediction: 3 True Y: 3\n",
            "[True] Prediction: 1 True Y: 1\n",
            "[True] Prediction: 5 True Y: 5\n",
            "[True] Prediction: 4 True Y: 4\n",
            "[True] Prediction: 2 True Y: 2\n",
            "[True] Prediction: 2 True Y: 2\n",
            "[True] Prediction: 3 True Y: 3\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 1 True Y: 1\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 5 True Y: 5\n",
            "[True] Prediction: 0 True Y: 0\n",
            "[True] Prediction: 6 True Y: 6\n",
            "[True] Prediction: 1 True Y: 1\n",
            "Accuracy:  100.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_V26pT9lLgBR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 파이썬으로 Gradient Descent Algorithm 작성하기\n",
        "\n",
        "\n",
        "### 과제 방법\n",
        "\n",
        "gradient_descent 함수를 완성하시오.\n",
        "\n",
        "반드시 스스로 직접! 해보세요.\n"
      ]
    },
    {
      "metadata": {
        "id": "zVRyDFF1OTRX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from numpy import *\n",
        "import os\n",
        "\n",
        "def gradient_descent(points, starting_b, starting_m, learning_rate, num_iterations):\n",
        "\n",
        "    #\n",
        "    # 본 함수를 완성하시오.\n",
        "    #   \n",
        "    b = starting_b\n",
        "    m = starting_m\n",
        "    n=len(points[:,1])\n",
        "    \n",
        "    #맨 왼쪽에 행 순서 나온거 빼고 활용하기\n",
        "    for i in range(num_iterations):\n",
        "        pred = dot(points[:,0],m)+b\n",
        "        m = m-(1/n)*learning_rate*(points[:,0].T.dot(pred-points[:,1])) \n",
        "        b = b-learning_rate*(mean(pred-points[:,1]))\n",
        "        \n",
        "        # 확인용\n",
        "        #if i%200 == 0:\n",
        "          #print('b={}, m={}'.format(m,b))\n",
        "          \n",
        "    return [b, m]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "np16bU9VOb3x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "bd96cad9-e3c4-4b95-8401-74e9b3ee6c2a"
      },
      "cell_type": "code",
      "source": [
        "# 학습 데이터 다운로드\n",
        "if os.path.exists('20190407_data.csv')==False:\n",
        "    print('Train data downloading..')\n",
        "    ! curl 'https://raw.githubusercontent.com/unizard/2019.Spring.AI/master/20190407_data.csv' -o './20190407_data.csv'        \n",
        "    print('Done..\\n')\n",
        "else:\n",
        "    print('File already exists \\n')\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "def run():\n",
        "    points = genfromtxt(\"20190407_data.csv\", delimiter=\",\")\n",
        "    learning_rate = 0.0001\n",
        "    initial_b = 0 # initial y-intercept guess\n",
        "    initial_m = 0 # initial slope guess\n",
        "    num_iterations = 1000\n",
        "    \n",
        "    print(\"========================\")\n",
        "    print(\" Start\")    \n",
        "    print(\"========================\")\n",
        "    [b, w] = gradient_descent(points, initial_b, initial_m, learning_rate, num_iterations)\n",
        "    print(\"b=\",b)\n",
        "    print(\"w=\",w)\n",
        "\n",
        "    \n",
        "    \n",
        "    \n",
        "if __name__ == '__main__':\n",
        "    run()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train data downloading..\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  3778  100  3778    0     0  25019      0 --:--:-- --:--:-- --:--:-- 25186\n",
            "Done..\n",
            "\n",
            "========================\n",
            " Start\n",
            "========================\n",
            "b= 0.0590585566421608\n",
            "w= 1.4783313274545458\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qM8ni6AcNM69",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Neural Network"
      ]
    },
    {
      "metadata": {
        "id": "h2uVVDK4u9EH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Multinomial Classification 문제를 해결하기 위해 적절한  Learning Rate 를 찾아봅시다.\n",
        "\n",
        "\n",
        "### 1) 직접 코드를 이해하여 주석을 달아 제출\n",
        "- 주석을 달면서 이해가 안가는 부분 조교에게 질문\n",
        "\n",
        "### 2) Learning rate 를 찾아보세요\n",
        "-  1.5 => 1e-10 => 0.1 순으로 변경해보자. \n",
        "- 아래의 ?? 부분에 learning rate 값을 넣으면 된다.\n",
        "  - optimizer = tf.train.GradientDescentOptimizer(learning_rate=???).minimize(cost)\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "uWUhZoB6NOtF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 10302
        },
        "outputId": "590e9e4d-8e7b-44e8-e468-783139c5f50a"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.set_random_seed(777)  # for reproducibility\n",
        "\n",
        "# 학습용 데이터 셋\n",
        "x_data = [[1, 2, 1],\n",
        "          [1, 3, 2],\n",
        "          [1, 3, 4],\n",
        "          [1, 5, 5],\n",
        "          [1, 7, 5],\n",
        "          [1, 2, 5],\n",
        "          [1, 6, 6],\n",
        "          [1, 7, 7]]\n",
        "y_data = [[0, 0, 1],\n",
        "          [0, 0, 1],\n",
        "          [0, 0, 1],\n",
        "          [0, 1, 0],\n",
        "          [0, 1, 0],\n",
        "          [0, 1, 0],\n",
        "          [1, 0, 0],\n",
        "          [1, 0, 0]]\n",
        "\n",
        "# 평가용 데이터 셋\n",
        "x_test = [[2, 1, 1],\n",
        "          [3, 1, 2],\n",
        "          [3, 3, 4]]\n",
        "y_test = [[0, 0, 1],\n",
        "          [0, 0, 1],\n",
        "          [0, 0, 1]]\n",
        "\n",
        "\n",
        "\n",
        "X = tf.placeholder(\"float\", [None, 3])\n",
        "Y = tf.placeholder(\"float\", [None, 3])\n",
        "\n",
        "W = tf.Variable(tf.random_normal([3, 3]))\n",
        "b = tf.Variable(tf.random_normal([3]))\n",
        "\n",
        "\n",
        "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
        "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
        "\n",
        "prediction = tf.argmax(hypothesis, 1)\n",
        "is_correct = tf.equal(prediction, tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
        "\n",
        "\n",
        "\n",
        "with tf.Session() as sess:\n",
        "\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    for step in range(201):\n",
        "        cost_val, W_val, _ = sess.run([cost, W, optimizer], feed_dict={X: x_data, Y: y_data})\n",
        "        print(step, cost_val, W_val)\n",
        "\n",
        "    print(\"Prediction:\", sess.run(prediction, feed_dict={X: x_test}))\n",
        "    print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: x_test, Y: y_test}))\n",
        "\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 1.8452613 [[ 1.2201109  -0.08175069  2.4965332 ]\n",
            " [-0.538348    0.5975348  -0.0965065 ]\n",
            " [ 0.25963336 -0.3803388  -1.0953832 ]]\n",
            "1 1.444674 [[ 1.2146817  -0.10198049  2.522192  ]\n",
            " [-0.47274482  0.46078247 -0.02535737]\n",
            " [ 0.29014668 -0.47929475 -1.0269406 ]]\n",
            "2 1.1974428 [[ 1.1877325  -0.09614011  2.5433009 ]\n",
            " [-0.5269472   0.4593837   0.0302438 ]\n",
            " [ 0.20187968 -0.4471972  -0.97077113]]\n",
            "3 1.0831964 [[ 1.1775916  -0.10163492  2.5589366 ]\n",
            " [-0.49481368  0.3917533   0.06574073]\n",
            " [ 0.19925658 -0.48155499 -0.93379027]]\n",
            "4 1.028961 [[ 1.1607196  -0.09504925  2.569223  ]\n",
            " [-0.5042182   0.39104912  0.07584946]\n",
            " [ 0.15627731 -0.45296267 -0.9194033 ]]\n",
            "5 1.0074649 [[ 1.1513671  -0.09360686  2.5771332 ]\n",
            " [-0.47481784  0.36099392  0.07650428]\n",
            " [ 0.15298285 -0.45393422 -0.9151373 ]]\n",
            "6 0.9924283 [[ 1.1388111  -0.0879785   2.584061  ]\n",
            " [-0.46417865  0.3551377   0.07172132]\n",
            " [ 0.13153465 -0.43217418 -0.91544914]]\n",
            "7 0.9789294 [[ 1.1286358  -0.08482663  2.5910842 ]\n",
            " [-0.4413602   0.33565837  0.06838222]\n",
            " [ 0.12298489 -0.42406076 -0.9150128 ]]\n",
            "8 0.96613765 [[ 1.1171356  -0.08015914  2.5979168 ]\n",
            " [-0.42667726  0.32530192  0.06405573]\n",
            " [ 0.10692923 -0.407627   -0.9153909 ]]\n",
            "9 0.9538886 [[ 1.1066517  -0.07654206  2.6047838 ]\n",
            " [-0.40732253  0.30952257  0.06048037]\n",
            " [ 0.09626555 -0.3969556  -0.9153986 ]]\n",
            "10 0.9421047 [[ 1.095656   -0.07231747  2.6115549 ]\n",
            " [-0.3916726   0.29774103  0.056612  ]\n",
            " [ 0.08258099 -0.3829384  -0.91573125]]\n",
            "11 0.930762 [[ 1.0851713  -0.06858845  2.6183105 ]\n",
            " [-0.37415752  0.28373364  0.05310431]\n",
            " [ 0.07149629 -0.3716265  -0.91595846]]\n",
            "12 0.91984826 [[ 1.0745041  -0.06461176  2.625001  ]\n",
            " [-0.35857463  0.27171728  0.04953779]\n",
            " [ 0.05919815 -0.35893658 -0.91635025]]\n",
            "13 0.90935767 [[ 1.0641382  -0.06090383  2.631659  ]\n",
            " [-0.34229946  0.25879538  0.04618454]\n",
            " [ 0.04834063 -0.34769863 -0.9167307 ]]\n",
            "14 0.89928436 [[ 1.0537326  -0.05710378  2.6382644 ]\n",
            " [-0.32719582  0.2470141   0.04286219]\n",
            " [ 0.03705214 -0.33592927 -0.91721153]]\n",
            "15 0.88962245 [[ 1.0435348  -0.05347314  2.6448317 ]\n",
            " [-0.3119278   0.23491609  0.03969217]\n",
            " [ 0.02668538 -0.32505977 -0.9177143 ]]\n",
            "16 0.880365 [[ 1.033364   -0.04982295  2.6513522 ]\n",
            " [-0.29748     0.22357218  0.03658827]\n",
            " [ 0.01625017 -0.31405097 -0.9182879 ]]\n",
            "17 0.871504 [[ 1.023356   -0.04629563  2.657833  ]\n",
            " [-0.28312072  0.2121934   0.0336078 ]\n",
            " [ 0.00648611 -0.30368102 -0.9188938 ]]\n",
            "18 0.8630306 [[ 1.0134078  -0.04278496  2.6642704 ]\n",
            " [-0.26940608  0.20138039  0.03070618]\n",
            " [-0.00316914 -0.29336527 -0.9195543 ]]\n",
            "19 0.8549346 [[ 1.0035988  -0.03937371  2.6706681 ]\n",
            " [-0.25590664  0.19067568  0.02791147]\n",
            " [-0.0122849  -0.28355533 -0.92024845]]\n",
            "20 0.847205 [[ 0.99386656 -0.03599804  2.6770246 ]\n",
            " [-0.24295557  0.18043777  0.0251983 ]\n",
            " [-0.02120261 -0.27389956 -0.92098653]]\n",
            "21 0.83983004 [[ 0.9842598  -0.03270889  2.6833422 ]\n",
            " [-0.23028369  0.17038377  0.02258046]\n",
            " [-0.02965717 -0.2646756  -0.92175597]]\n",
            "22 0.8327971 [[ 0.9747382  -0.02946541  2.6896203 ]\n",
            " [-0.218101    0.16073953  0.020042  ]\n",
            " [-0.03786924 -0.2556583  -0.92256117]]\n",
            "23 0.8260932 [[ 0.96533316 -0.0263005   2.6958604 ]\n",
            " [-0.20622814  0.15131935  0.01758933]\n",
            " [-0.04566782 -0.2470269  -0.92339396]]\n",
            "24 0.8197051 [[ 0.95601714 -0.02318643  2.7020624 ]\n",
            " [-0.194803    0.14227213  0.01521142]\n",
            " [-0.05320386 -0.2386288  -0.924256  ]]\n",
            "25 0.813619 [[ 0.9468111  -0.02014531  2.7082274 ]\n",
            " [-0.18369931  0.13346909  0.01291076]\n",
            " [-0.06036272 -0.23058449 -0.92514145]]\n",
            "26 0.80782115 [[ 0.9376951  -0.01715728  2.7143552 ]\n",
            " [-0.17301063  0.12501216  0.01067903]\n",
            " [-0.06725308 -0.2227849  -0.92605066]]\n",
            "27 0.80229765 [[ 0.9286837  -0.01423769  2.720447  ]\n",
            " [-0.16264339  0.11680735  0.0085166 ]\n",
            " [-0.07379553 -0.21531391 -0.9269792 ]]\n",
            "28 0.7970348 [[ 0.9197618  -0.01137161  2.726503  ]\n",
            " [-0.15266293  0.10892667  0.00641684]\n",
            " [-0.08007211 -0.20808972 -0.9279268 ]]\n",
            "29 0.7920191 [[ 0.9109395  -0.00856997  2.7325234 ]\n",
            " [-0.14299674  0.10129841  0.00437891]\n",
            " [-0.08602621 -0.20117237 -0.92889005]]\n",
            "30 0.78723735 [[ 9.0220499e-01 -5.8210841e-03  2.7385089e+00]\n",
            " [-1.3369146e-01  9.3974672e-02  2.3973614e-03]\n",
            " [-9.1722198e-02 -1.9449806e-01 -9.2986840e-01]]\n",
            "31 0.7826766 [[ 8.93565595e-01 -3.13287671e-03  2.74446011e+00]\n",
            " [-1.24688834e-01  8.68986771e-02  4.70735889e-04]\n",
            " [-9.71189216e-02 -1.88110635e-01 -9.30859089e-01]]\n",
            "32 0.77832425 [[ 8.8501161e-01 -4.9587555e-04  2.7503772e+00]\n",
            " [-1.1602265e-01  8.0108799e-02 -1.4055861e-03]\n",
            " [-1.0226841e-01 -1.8195869e-01 -9.3186158e-01]]\n",
            "33 0.77416825 [[ 8.7654835e-01  2.0840713e-03  2.7562604e+00]\n",
            " [-1.0764480e-01  7.3558681e-02 -3.2333222e-03]\n",
            " [-1.0714025e-01 -1.7607479e-01 -9.3287367e-01]]\n",
            "34 0.7701969 [[ 0.86816764  0.00461485  2.7621102 ]\n",
            " [-0.09958008  0.06727702 -0.00501638]\n",
            " [-0.11177761 -0.17041638 -0.9338947 ]]\n",
            "35 0.7663994 [[ 0.8598734   0.0070921   2.7679272 ]\n",
            " [-0.09178784  0.06122497 -0.00675658]\n",
            " [-0.11615765 -0.16500804 -0.934923  ]]\n",
            "36 0.7627648 [[ 0.8516587   0.00952253  2.7737114 ]\n",
            " [-0.08428647  0.05542429 -0.00845726]\n",
            " [-0.12031692 -0.15981373 -0.935958  ]]\n",
            "37 0.7592833 [[ 0.8435263   0.01190286  2.7794635 ]\n",
            " [-0.07704104  0.04984175 -0.01012013]\n",
            " [-0.12423809 -0.15485226 -0.9369983 ]]\n",
            "38 0.7559453 [[ 0.83547026  0.01423889  2.7851837 ]\n",
            " [-0.0700653   0.044494   -0.01174812]\n",
            " [-0.12795253 -0.15009262 -0.93804353]]\n",
            "39 0.75274175 [[ 0.8274926   0.01652812  2.790872  ]\n",
            " [-0.06332886  0.03935223 -0.01334279]\n",
            " [-0.13144694 -0.14554928 -0.93909246]]\n",
            "40 0.7496643 [[ 0.81958795  0.01877563  2.7965293 ]\n",
            " [-0.05684207  0.03442926 -0.01490661]\n",
            " [-0.13474862 -0.14119528 -0.9401448 ]]\n",
            "41 0.7467049 [[ 0.8117579   0.02097952  2.8021555 ]\n",
            " [-0.05057823  0.02969981 -0.016441  ]\n",
            " [-0.13784724 -0.13704188 -0.94119954]]\n",
            "42 0.74385595 [[ 0.80399776  0.02314427  2.807751  ]\n",
            " [-0.04454517  0.02517384 -0.01794809]\n",
            " [-0.14076677 -0.13306533 -0.94225657]]\n",
            "43 0.7411106 [[ 0.7963085   0.02526841  2.813316  ]\n",
            " [-0.03871928  0.02082896 -0.0194291 ]\n",
            " [-0.14349912 -0.12927452 -0.943315  ]]\n",
            "44 0.7384622 [[ 0.788686    0.02735595  2.8188512 ]\n",
            " [-0.03310653  0.01667295 -0.02088584]\n",
            " [-0.14606549 -0.12564838 -0.9443748 ]]\n",
            "45 0.73590475 [[ 0.7811309   0.02940574  2.8243566 ]\n",
            " [-0.02768589  0.01268587 -0.0223194 ]\n",
            " [-0.1484595  -0.12219393 -0.9454352 ]]\n",
            "46 0.7334323 [[ 0.7736395   0.03142136  2.8298323 ]\n",
            " [-0.02246193  0.00887382 -0.02373131]\n",
            " [-0.15069993 -0.11889245 -0.94649625]]\n",
            "47 0.73103964 [[ 0.7662123   0.03340193  2.835279  ]\n",
            " [-0.01741589  0.00521893 -0.02512247]\n",
            " [-0.15278174 -0.11574951 -0.9475574 ]]\n",
            "48 0.72872174 [[ 7.5884581e-01  3.5350651e-02  2.8406968e+00]\n",
            " [-1.2551293e-02  1.7260408e-03 -2.6494164e-02]\n",
            " [-1.5472175e-01 -1.1274833e-01 -9.4861859e-01]]\n",
            "49 0.72647405 [[ 7.5154042e-01  3.7266843e-02  2.8460860e+00]\n",
            " [-7.8512980e-03 -1.6209299e-03 -2.7847182e-02]\n",
            " [-1.5651585e-01 -1.0989340e-01 -9.4967943e-01]]\n",
            "50 0.7242921 [[ 0.74429303  0.03915339  2.8514469 ]\n",
            " [-0.00331862 -0.00481817 -0.02918261]\n",
            " [-0.15817909 -0.10716966 -0.9507399 ]]\n",
            "51 0.722172 [[ 7.3710376e-01  4.1009754e-02  2.8567798e+00]\n",
            " [ 1.0618466e-03 -7.8801494e-03 -3.0501096e-02]\n",
            " [-1.5970820e-01 -1.0458072e-01 -9.5179975e-01]]\n",
            "52 0.72011 [[ 0.7299698   0.04283854  2.8620849 ]\n",
            " [ 0.005288   -0.01080385 -0.03180355]\n",
            " [-0.1611167  -0.10211305 -0.9528589 ]]\n",
            "53 0.7181026 [[ 0.7228913   0.04463935  2.8673625 ]\n",
            " [ 0.00937346 -0.01360235 -0.03309053]\n",
            " [-0.16240188 -0.09976954 -0.95391726]]\n",
            "54 0.7161465 [[ 0.71586555  0.04641452  2.872613  ]\n",
            " [ 0.01331659 -0.01627321 -0.03436279]\n",
            " [-0.16357593 -0.09753798 -0.9549748 ]]\n",
            "55 0.71423894 [[ 0.70889264  0.04816373  2.8778367 ]\n",
            " [ 0.01712966 -0.0188283  -0.03562078]\n",
            " [-0.16463664 -0.09542075 -0.95603126]]\n",
            "56 0.71237683 [[ 0.7019702   0.04988912  2.8830338 ]\n",
            " [ 0.02081143 -0.02126569 -0.03686514]\n",
            " [-0.16559497 -0.0934068  -0.95708686]]\n",
            "57 0.71055794 [[ 0.69509816  0.05159044  2.8882046 ]\n",
            " [ 0.02437292 -0.02359609 -0.03809624]\n",
            " [-0.16644917 -0.09149807 -0.9581414 ]]\n",
            "58 0.70877945 [[ 0.6882743   0.0532696   2.8933492 ]\n",
            " [ 0.02781326 -0.02581805 -0.03931462]\n",
            " [-0.16720907 -0.08968462 -0.95919496]]\n",
            "59 0.70703936 [[ 0.6814986   0.05492644  2.898468  ]\n",
            " [ 0.03114235 -0.02794119 -0.04052058]\n",
            " [-0.1678733  -0.08796794 -0.9602474 ]]\n",
            "60 0.7053356 [[ 0.674769    0.05656268  2.9035614 ]\n",
            " [ 0.03435956 -0.02996441 -0.04171456]\n",
            " [-0.16845065 -0.08633909 -0.9612989 ]]\n",
            "61 0.70366627 [[ 0.66808534  0.0581782   2.9086294 ]\n",
            " [ 0.03747385 -0.03189644 -0.04289681]\n",
            " [-0.16894007 -0.08479927 -0.9623493 ]]\n",
            "62 0.7020294 [[ 0.66144586  0.05977457  2.9136724 ]\n",
            " [ 0.04048476 -0.03373643 -0.04406772]\n",
            " [-0.16934949 -0.08334036 -0.96339875]]\n",
            "63 0.7004234 [[ 0.6548503   0.06135171  2.918691  ]\n",
            " [ 0.04340038 -0.03549231 -0.04522748]\n",
            " [-0.16967808 -0.08196334 -0.9644472 ]]\n",
            "64 0.6988468 [[ 0.6482971   0.06291103  2.9236848 ]\n",
            " [ 0.04622044 -0.03716343 -0.04637642]\n",
            " [-0.16993299 -0.08066086 -0.96549475]]\n",
            "65 0.69729805 [[ 0.6417859   0.06445247  2.9286544 ]\n",
            " [ 0.04895226 -0.03875697 -0.04751468]\n",
            " [-0.17011356 -0.07943368 -0.96654135]]\n",
            "66 0.69577575 [[ 0.63531536  0.06597733  2.9336002 ]\n",
            " [ 0.05159562 -0.04027247 -0.04864254]\n",
            " [-0.17022632 -0.07827515 -0.9675871 ]]\n",
            "67 0.69427884 [[ 0.6288851   0.06748559  2.938522  ]\n",
            " [ 0.05415724 -0.04171649 -0.04976015]\n",
            " [-0.17027065 -0.0771859  -0.96863204]]\n",
            "68 0.692806 [[ 0.62249374  0.0689784   2.9434206 ]\n",
            " [ 0.05663695 -0.04308864 -0.0508677 ]\n",
            " [-0.17025253 -0.07615984 -0.9696762 ]]\n",
            "69 0.69135606 [[ 0.6161411   0.07045576  2.948296  ]\n",
            " [ 0.05904083 -0.04439489 -0.05196533]\n",
            " [-0.17017147 -0.07519747 -0.97071964]]\n",
            "70 0.6899283 [[ 0.6098258   0.07191873  2.9531484 ]\n",
            " [ 0.06136884 -0.045635   -0.05305322]\n",
            " [-0.17003284 -0.07429329 -0.9717624 ]]\n",
            "71 0.6885213 [[ 0.60354763  0.07336732  2.957978  ]\n",
            " [ 0.06362646 -0.04681439 -0.05413143]\n",
            " [-0.16983631 -0.0734477  -0.97280455]]\n",
            "72 0.68713456 [[ 0.59730536  0.07480249  2.962785  ]\n",
            " [ 0.06581373 -0.04793291 -0.05520016]\n",
            " [-0.16958675 -0.07265568 -0.97384614]]\n",
            "73 0.68576705 [[ 0.5910988   0.07622425  2.9675698 ]\n",
            " [ 0.06793561 -0.04899548 -0.05625947]\n",
            " [-0.16928388 -0.07191751 -0.97488713]]\n",
            "74 0.68441814 [[ 0.5849267   0.0776335   2.9723327 ]\n",
            " [ 0.06999221 -0.05000205 -0.05730951]\n",
            " [-0.16893214 -0.07122868 -0.9759277 ]]\n",
            "75 0.683087 [[ 0.578789    0.07903025  2.9770737 ]\n",
            " [ 0.07198805 -0.05095707 -0.05835034]\n",
            " [-0.1685313  -0.07058935 -0.9769679 ]]\n",
            "76 0.6817728 [[ 0.5726845   0.08041532  2.9817932 ]\n",
            " [ 0.07392325 -0.05186053 -0.05938207]\n",
            " [-0.16808541 -0.06999543 -0.9780077 ]]\n",
            "77 0.6804751 [[ 0.5666131   0.08178873  2.9864912 ]\n",
            " [ 0.07580193 -0.05271652 -0.06040476]\n",
            " [-0.16759431 -0.06944704 -0.9790471 ]]\n",
            "78 0.6791933 [[ 0.56057364  0.08315121  2.991168  ]\n",
            " [ 0.07762425 -0.05352509 -0.0614185 ]\n",
            " [-0.16706169 -0.06894044 -0.9800863 ]]\n",
            "79 0.6779268 [[ 0.554566    0.08450281  2.995824  ]\n",
            " [ 0.07939398 -0.05428994 -0.06242337]\n",
            " [-0.16648741 -0.0684757  -0.98112535]]\n",
            "80 0.67667496 [[ 0.5485893   0.08584418  3.0004594 ]\n",
            " [ 0.08111124 -0.05501114 -0.06341944]\n",
            " [-0.16587485 -0.06804941 -0.9821642 ]]\n",
            "81 0.67543733 [[ 0.54264325  0.08717538  3.0050743 ]\n",
            " [ 0.08277953 -0.0556921  -0.06440675]\n",
            " [-0.16522393 -0.06766162 -0.98320293]]\n",
            "82 0.67421347 [[ 0.536727    0.088497    3.0096688 ]\n",
            " [ 0.08439899 -0.05633291 -0.06538539]\n",
            " [-0.16453768 -0.06730922 -0.9842416 ]]\n",
            "83 0.67300284 [[ 0.53084034  0.0898091   3.0142434 ]\n",
            " [ 0.08597276 -0.05693666 -0.06635541]\n",
            " [-0.16381611 -0.06699219 -0.9852802 ]]\n",
            "84 0.6718052 [[ 0.5249825   0.09111223  3.018798  ]\n",
            " [ 0.08750106 -0.0575035  -0.06731688]\n",
            " [-0.16306193 -0.06670773 -0.9863188 ]]\n",
            "85 0.67062 [[ 0.51915324  0.09240644  3.0233333 ]\n",
            " [ 0.08898672 -0.05803619 -0.06826986]\n",
            " [-0.16227521 -0.06645577 -0.9873575 ]]\n",
            "86 0.66944695 [[ 0.5133518   0.09369224  3.027849  ]\n",
            " [ 0.09043002 -0.05853494 -0.06921439]\n",
            " [-0.1614584  -0.06623378 -0.9883963 ]]\n",
            "87 0.6682856 [[ 0.50757796  0.09496969  3.0323453 ]\n",
            " [ 0.09183348 -0.05900228 -0.07015051]\n",
            " [-0.16061163 -0.06604166 -0.9894352 ]]\n",
            "88 0.6671357 [[ 0.50183105  0.09623923  3.0368228 ]\n",
            " [ 0.09319741 -0.05943843 -0.0710783 ]\n",
            " [-0.1597371  -0.06587712 -0.9904743 ]]\n",
            "89 0.6659969 [[ 0.4961108   0.09750094  3.0412815 ]\n",
            " [ 0.09452411 -0.05984566 -0.07199779]\n",
            " [-0.15883495 -0.06574002 -0.99151355]]\n",
            "90 0.6648689 [[ 0.49041662  0.09875521  3.0457213 ]\n",
            " [ 0.09581391 -0.06022419 -0.07290906]\n",
            " [-0.15790719 -0.06562827 -0.99255306]]\n",
            "91 0.66375136 [[ 0.48474827  0.10000212  3.0501428 ]\n",
            " [ 0.09706891 -0.06057613 -0.07381212]\n",
            " [-0.15695395 -0.06554173 -0.9935928 ]]\n",
            "92 0.6626441 [[ 0.47910517  0.10124204  3.0545459 ]\n",
            " [ 0.09828941 -0.0609017  -0.07470705]\n",
            " [-0.1559771  -0.0654785  -0.9946329 ]]\n",
            "93 0.6615468 [[ 0.47348708  0.10247502  3.0589309 ]\n",
            " [ 0.09947734 -0.06120282 -0.07559385]\n",
            " [-0.1549768  -0.06543845 -0.99567324]]\n",
            "94 0.6604592 [[ 0.46789345  0.10370144  3.063298  ]\n",
            " [ 0.10063299 -0.0614797  -0.07647262]\n",
            " [-0.15395471 -0.0654198  -0.99671394]]\n",
            "95 0.65938115 [[ 0.46232405  0.10492133  3.0676475 ]\n",
            " [ 0.10175817 -0.0617341  -0.07734338]\n",
            " [-0.15291099 -0.06542244 -0.997755  ]]\n",
            "96 0.6583123 [[ 0.4567784   0.10613501  3.0719795 ]\n",
            " [ 0.10285313 -0.06196623 -0.0782062 ]\n",
            " [-0.1518472  -0.06544474 -0.99879646]]\n",
            "97 0.65725255 [[ 0.45125625  0.10734255  3.0762942 ]\n",
            " [ 0.10391952 -0.0621777  -0.07906111]\n",
            " [-0.15076348 -0.06548657 -0.99983835]]\n",
            "98 0.6562016 [[ 0.44575712  0.10854423  3.0805917 ]\n",
            " [ 0.10495763 -0.06236874 -0.07990817]\n",
            " [-0.14966124 -0.06554649 -1.0008807 ]]\n",
            "99 0.65515935 [[ 0.4402808   0.10974012  3.084872  ]\n",
            " [ 0.10596893 -0.06254079 -0.08074742]\n",
            " [-0.14854063 -0.06562433 -1.0019234 ]]\n",
            "100 0.6541256 [[ 0.43482682  0.11093047  3.0891356 ]\n",
            " [ 0.1069537  -0.06269407 -0.08157892]\n",
            " [-0.14740299 -0.06571875 -1.0029666 ]]\n",
            "101 0.6531 [[ 0.42939502  0.11211535  3.0933826 ]\n",
            " [ 0.10791337 -0.06282992 -0.08240271]\n",
            " [-0.14624839 -0.06582963 -1.0040103 ]]\n",
            "102 0.65208256 [[ 0.42398494  0.113295    3.097613  ]\n",
            " [ 0.10884812 -0.06294852 -0.08321887]\n",
            " [-0.1450781  -0.06595571 -1.0050545 ]]\n",
            "103 0.65107316 [[ 0.41859636  0.11446947  3.1018271 ]\n",
            " [ 0.10975929 -0.06305114 -0.08402741]\n",
            " [-0.1438922  -0.06609688 -1.0060992 ]]\n",
            "104 0.6500714 [[ 0.4132289   0.11563899  3.106025  ]\n",
            " [ 0.11064709 -0.06313796 -0.08482838]\n",
            " [-0.14269185 -0.06625202 -1.0071445 ]]\n",
            "105 0.6490773 [[ 0.40788236  0.11680359  3.1102068 ]\n",
            " [ 0.11151269 -0.06321011 -0.08562183]\n",
            " [-0.14147718 -0.06642099 -1.0081902 ]]\n",
            "106 0.6480908 [[ 0.40255636  0.11796351  3.114373  ]\n",
            " [ 0.11235637 -0.06326778 -0.08640785]\n",
            " [-0.14024915 -0.06660277 -1.0092365 ]]\n",
            "107 0.64711154 [[ 0.3972507   0.11911877  3.1185234 ]\n",
            " [ 0.11317918 -0.06331199 -0.08718646]\n",
            " [-0.13900794 -0.0667972  -1.0102832 ]]\n",
            "108 0.6461395 [[ 0.39196506  0.12026958  3.1226583 ]\n",
            " [ 0.11398137 -0.06334291 -0.08795772]\n",
            " [-0.13775447 -0.06700334 -1.0113306 ]]\n",
            "109 0.6451746 [[ 0.3866992   0.12141598  3.1267776 ]\n",
            " [ 0.11476394 -0.0633615  -0.08872169]\n",
            " [-0.13648887 -0.06722108 -1.0123785 ]]\n",
            "110 0.64421666 [[ 0.3814528   0.12255815  3.1308818 ]\n",
            " [ 0.11552712 -0.06336795 -0.08947841]\n",
            " [-0.13521196 -0.06744955 -1.0134269 ]]\n",
            "111 0.6432655 [[ 0.37622568  0.12369612  3.134971  ]\n",
            " [ 0.11627178 -0.06336309 -0.09022793]\n",
            " [-0.13392393 -0.06768863 -1.0144758 ]]\n",
            "112 0.642321 [[ 0.37101755  0.12483006  3.139045  ]\n",
            " [ 0.11699821 -0.06334713 -0.09097034]\n",
            " [-0.13262552 -0.06793753 -1.0155253 ]]\n",
            "113 0.6413832 [[ 0.3658282   0.12596002  3.1431043 ]\n",
            " [ 0.11770721 -0.06332083 -0.09170564]\n",
            " [-0.13131689 -0.06819611 -1.0165755 ]]\n",
            "114 0.6404518 [[ 0.3606573   0.12708615  3.147149  ]\n",
            " [ 0.11839901 -0.06328436 -0.09243391]\n",
            " [-0.12999871 -0.06846368 -1.017626  ]]\n",
            "115 0.63952684 [[ 0.35550472  0.12820847  3.1511793 ]\n",
            " [ 0.11907439 -0.06323845 -0.0931552 ]\n",
            " [-0.12867115 -0.06874011 -1.0186772 ]]\n",
            "116 0.6386081 [[ 0.35037017  0.12932713  3.1551952 ]\n",
            " [ 0.11973355 -0.06318324 -0.09386957]\n",
            " [-0.12733486 -0.06902471 -1.0197289 ]]\n",
            "117 0.63769567 [[ 0.34525347  0.13044217  3.1591969 ]\n",
            " [ 0.12037724 -0.0631194  -0.09457709]\n",
            " [-0.12598994 -0.06931739 -1.0207812 ]]\n",
            "118 0.6367892 [[ 0.34015435  0.13155372  3.1631844 ]\n",
            " [ 0.12100562 -0.06304708 -0.09527779]\n",
            " [-0.12463702 -0.06961755 -1.0218339 ]]\n",
            "119 0.63588876 [[ 0.33507267  0.13266182  3.1671581 ]\n",
            " [ 0.1216194  -0.06296689 -0.09597173]\n",
            " [-0.1232762  -0.06992508 -1.0228872 ]]\n",
            "120 0.6349942 [[ 0.33000815  0.13376658  3.171118  ]\n",
            " [ 0.12221872 -0.06287895 -0.09665897]\n",
            " [-0.12190806 -0.07023939 -1.023941  ]]\n",
            "121 0.6341053 [[ 0.32496062  0.13486803  3.175064  ]\n",
            " [ 0.12280422 -0.06278385 -0.09733958]\n",
            " [-0.12053271 -0.07056043 -1.0249953 ]]\n",
            "122 0.63322234 [[ 0.31992987  0.13596627  3.1789966 ]\n",
            " [ 0.12337612 -0.06268172 -0.0980136 ]\n",
            " [-0.11915062 -0.07088766 -1.0260502 ]]\n",
            "123 0.6323449 [[ 0.3149157   0.13706136  3.1829157 ]\n",
            " [ 0.12393492 -0.06257303 -0.09868108]\n",
            " [-0.11776199 -0.07122097 -1.0271056 ]]\n",
            "124 0.63147306 [[ 0.3099179   0.13815337  3.1868215 ]\n",
            " [ 0.12448087 -0.06245797 -0.09934209]\n",
            " [-0.1163672  -0.07155994 -1.0281614 ]]\n",
            "125 0.63060665 [[ 0.30493632  0.13924237  3.1907141 ]\n",
            " [ 0.12501441 -0.06233694 -0.09999667]\n",
            " [-0.11496644 -0.07190442 -1.0292177 ]]\n",
            "126 0.6297457 [[ 0.29997075  0.1403284   3.1945937 ]\n",
            " [ 0.12553583 -0.06221013 -0.10064489]\n",
            " [-0.11356005 -0.07225404 -1.0302745 ]]\n",
            "127 0.62889 [[ 0.29502103  0.14141154  3.1984603 ]\n",
            " [ 0.1260455  -0.06207789 -0.10128681]\n",
            " [-0.11214824 -0.07260863 -1.0313318 ]]\n",
            "128 0.6280396 [[ 0.29008695  0.14249185  3.2023141 ]\n",
            " [ 0.12654367 -0.06194041 -0.10192248]\n",
            " [-0.1107313  -0.07296789 -1.0323895 ]]\n",
            "129 0.6271944 [[ 0.28516838  0.14356937  3.2061553 ]\n",
            " [ 0.12703073 -0.06179801 -0.10255195]\n",
            " [-0.10930943 -0.07333167 -1.0334476 ]]\n",
            "130 0.62635416 [[ 0.2802651   0.14464417  3.2099838 ]\n",
            " [ 0.12750693 -0.06165085 -0.1031753 ]\n",
            " [-0.10788291 -0.07369963 -1.0345062 ]]\n",
            "131 0.62551904 [[ 0.27537695  0.1457163   3.2138    ]\n",
            " [ 0.12797259 -0.06149925 -0.10379257]\n",
            " [-0.10645191 -0.07407168 -1.0355651 ]]\n",
            "132 0.6246889 [[ 0.2705038   0.1467858   3.2176037 ]\n",
            " [ 0.12842797 -0.06134335 -0.10440382]\n",
            " [-0.10501669 -0.07444751 -1.0366246 ]]\n",
            "133 0.62386364 [[ 0.26564544  0.1478527   3.221395  ]\n",
            " [ 0.12887333 -0.06118344 -0.10500908]\n",
            " [-0.10357744 -0.07482701 -1.0376843 ]]\n",
            "134 0.6230432 [[ 0.26080176  0.14891708  3.2251744 ]\n",
            " [ 0.12930892 -0.06101969 -0.10560843]\n",
            " [-0.10213439 -0.07520995 -1.0387444 ]]\n",
            "135 0.62222755 [[ 0.2559726   0.14997897  3.2289417 ]\n",
            " [ 0.12973502 -0.06085228 -0.10620193]\n",
            " [-0.1006877  -0.07559614 -1.0398049 ]]\n",
            "136 0.6214166 [[ 0.2511578   0.15103841  3.232697  ]\n",
            " [ 0.13015188 -0.06068142 -0.10678965]\n",
            " [-0.09923755 -0.07598542 -1.0408658 ]]\n",
            "137 0.62061036 [[ 0.24635717  0.15209545  3.2364407 ]\n",
            " [ 0.13055971 -0.0605073  -0.10737161]\n",
            " [-0.09778418 -0.07637765 -1.041927  ]]\n",
            "138 0.61980873 [[ 0.24157062  0.15315014  3.2401726 ]\n",
            " [ 0.13095878 -0.06033007 -0.10794788]\n",
            " [-0.09632768 -0.07677263 -1.0429885 ]]\n",
            "139 0.6190115 [[ 0.23679799  0.1542025   3.243893  ]\n",
            " [ 0.13134928 -0.06014992 -0.10851852]\n",
            " [-0.09486827 -0.07717023 -1.0440503 ]]\n",
            "140 0.61821884 [[ 0.23203911  0.15525258  3.2476017 ]\n",
            " [ 0.13173142 -0.059967   -0.10908359]\n",
            " [-0.0934061  -0.07757028 -1.0451125 ]]\n",
            "141 0.61743057 [[ 0.22729386  0.15630041  3.2512991 ]\n",
            " [ 0.13210543 -0.05978147 -0.10964313]\n",
            " [-0.09194135 -0.07797267 -1.0461749 ]]\n",
            "142 0.6166467 [[ 0.22256212  0.15734603  3.2549853 ]\n",
            " [ 0.13247149 -0.05959347 -0.1101972 ]\n",
            " [-0.09047415 -0.07837725 -1.0472375 ]]\n",
            "143 0.61586726 [[ 0.21784374  0.15838946  3.2586603 ]\n",
            " [ 0.13282986 -0.05940315 -0.11074588]\n",
            " [-0.08900461 -0.07878388 -1.0483004 ]]\n",
            "144 0.6150919 [[ 0.2131386   0.15943076  3.262324  ]\n",
            " [ 0.13318063 -0.0592106  -0.11128921]\n",
            " [-0.08753295 -0.07919239 -1.0493635 ]]\n",
            "145 0.6143208 [[ 0.20844655  0.16046993  3.265977  ]\n",
            " [ 0.1335241  -0.05901605 -0.11182723]\n",
            " [-0.08605921 -0.07960278 -1.0504268 ]]\n",
            "146 0.613554 [[ 0.20376746  0.16150704  3.269619  ]\n",
            " [ 0.13386035 -0.05881952 -0.11236   ]\n",
            " [-0.08458361 -0.08001481 -1.0514904 ]]\n",
            "147 0.6127912 [[ 0.19910124  0.16254207  3.27325   ]\n",
            " [ 0.13418964 -0.05862123 -0.11288758]\n",
            " [-0.08310619 -0.08042847 -1.0525541 ]]\n",
            "148 0.6120324 [[ 0.19444774  0.1635751   3.2768705 ]\n",
            " [ 0.13451204 -0.05842119 -0.11341003]\n",
            " [-0.08162717 -0.08084356 -1.0536181 ]]\n",
            "149 0.6112778 [[ 0.18980685  0.16460612  3.2804804 ]\n",
            " [ 0.1348278  -0.05821959 -0.11392736]\n",
            " [-0.08014658 -0.08126006 -1.0546821 ]]\n",
            "150 0.61052704 [[ 0.18517844  0.16563518  3.2840798 ]\n",
            " [ 0.13513702 -0.05801649 -0.11443967]\n",
            " [-0.07866459 -0.08167782 -1.0557463 ]]\n",
            "151 0.60978025 [[ 0.1805624   0.1666623   3.2876687 ]\n",
            " [ 0.13543992 -0.05781202 -0.11494701]\n",
            " [-0.07718125 -0.08209677 -1.0568107 ]]\n",
            "152 0.6090374 [[ 0.17595862  0.1676875   3.2912474 ]\n",
            " [ 0.13573657 -0.05760628 -0.1154494 ]\n",
            " [-0.07569674 -0.08251683 -1.0578752 ]]\n",
            "153 0.6082983 [[ 0.17136697  0.16871081  3.2948158 ]\n",
            " [ 0.13602716 -0.05739936 -0.11594692]\n",
            " [-0.07421111 -0.08293789 -1.0589397 ]]\n",
            "154 0.607563 [[ 0.16678736  0.16973226  3.298374  ]\n",
            " [ 0.13631183 -0.05719133 -0.11643962]\n",
            " [-0.07272448 -0.08335987 -1.0600044 ]]\n",
            "155 0.60683155 [[ 0.16221966  0.17075185  3.301922  ]\n",
            " [ 0.13659072 -0.0569823  -0.11692753]\n",
            " [-0.07123692 -0.0837827  -1.061069  ]]\n",
            "156 0.60610384 [[ 0.15766378  0.17176963  3.3054602 ]\n",
            " [ 0.13686396 -0.05677232 -0.11741074]\n",
            " [-0.06974854 -0.08420628 -1.0621338 ]]\n",
            "157 0.6053797 [[ 0.15311961  0.17278561  3.3089883 ]\n",
            " [ 0.13713168 -0.0565615  -0.11788926]\n",
            " [-0.06825943 -0.08463058 -1.0631986 ]]\n",
            "158 0.60465926 [[ 0.14858705  0.1737998   3.3125067 ]\n",
            " [ 0.13739398 -0.05634992 -0.11836316]\n",
            " [-0.06676967 -0.08505551 -1.0642635 ]]\n",
            "159 0.6039424 [[ 0.14406598  0.17481223  3.3160152 ]\n",
            " [ 0.13765101 -0.05613764 -0.11883248]\n",
            " [-0.06527935 -0.085481   -1.0653282 ]]\n",
            "160 0.6032291 [[ 0.13955632  0.17582291  3.3195143 ]\n",
            " [ 0.13790289 -0.05592472 -0.11929727]\n",
            " [-0.06378853 -0.08590697 -1.066393  ]]\n",
            "161 0.6025193 [[ 0.13505796  0.17683189  3.3230038 ]\n",
            " [ 0.13814972 -0.05571122 -0.11975761]\n",
            " [-0.0622973  -0.08633336 -1.0674578 ]]\n",
            "162 0.601813 [[ 0.1305708   0.17783915  3.3264837 ]\n",
            " [ 0.13839166 -0.05549726 -0.12021351]\n",
            " [-0.06080571 -0.08676017 -1.0685226 ]]\n",
            "163 0.6011102 [[ 0.12609474  0.17884474  3.3299541 ]\n",
            " [ 0.13862872 -0.05528279 -0.12066503]\n",
            " [-0.0593139  -0.08718723 -1.0695873 ]]\n",
            "164 0.6004107 [[ 0.12162971  0.17984864  3.3334153 ]\n",
            " [ 0.13886112 -0.05506804 -0.12111218]\n",
            " [-0.05782183 -0.08761466 -1.070652  ]]\n",
            "165 0.59971464 [[ 0.11717558  0.18085091  3.336867  ]\n",
            " [ 0.13908882 -0.05485287 -0.12155507]\n",
            " [-0.05632972 -0.08804221 -1.0717165 ]]\n",
            "166 0.599022 [[ 0.11273229  0.18185152  3.3403099 ]\n",
            " [ 0.13931212 -0.05463751 -0.12199373]\n",
            " [-0.05483744 -0.08847    -1.0727811 ]]\n",
            "167 0.5983325 [[ 0.10829972  0.18285054  3.3437433 ]\n",
            " [ 0.13953091 -0.05442185 -0.12242819]\n",
            " [-0.05334525 -0.08889783 -1.0738455 ]]\n",
            "168 0.59764636 [[ 0.10387782  0.18384793  3.3471677 ]\n",
            " [ 0.13974546 -0.05420609 -0.12285849]\n",
            " [-0.05185304 -0.08932582 -1.0749097 ]]\n",
            "169 0.5969635 [[ 0.09946647  0.18484376  3.3505833 ]\n",
            " [ 0.13995573 -0.05399013 -0.1232847 ]\n",
            " [-0.05036097 -0.08975375 -1.0759737 ]]\n",
            "170 0.5962838 [[ 0.09506559  0.185838    3.3539898 ]\n",
            " [ 0.1401619  -0.05377414 -0.12370688]\n",
            " [-0.04886905 -0.09018172 -1.0770377 ]]\n",
            "171 0.5956073 [[ 0.09067509  0.18683068  3.3573875 ]\n",
            " [ 0.14036399 -0.05355807 -0.12412505]\n",
            " [-0.04737737 -0.0906096  -1.0781015 ]]\n",
            "172 0.59493387 [[ 0.08629491  0.18782182  3.3607767 ]\n",
            " [ 0.14056216 -0.05334205 -0.12453924]\n",
            " [-0.04588592 -0.09103743 -1.0791651 ]]\n",
            "173 0.5942636 [[ 0.08192494  0.18881144  3.364157  ]\n",
            " [ 0.1407564  -0.05312601 -0.12494951]\n",
            " [-0.04439483 -0.09146507 -1.0802286 ]]\n",
            "174 0.59359646 [[ 0.07756511  0.18979952  3.3675287 ]\n",
            " [ 0.14094688 -0.05291011 -0.1253559 ]\n",
            " [-0.04290406 -0.09189262 -1.0812918 ]]\n",
            "175 0.59293234 [[ 0.07321534  0.19078611  3.3708918 ]\n",
            " [ 0.14113359 -0.05269426 -0.12575845]\n",
            " [-0.04141377 -0.09231991 -1.0823548 ]]\n",
            "176 0.5922712 [[ 0.06887555  0.19177118  3.3742466 ]\n",
            " [ 0.14131673 -0.05247861 -0.12615722]\n",
            " [-0.03992387 -0.09274702 -1.0834175 ]]\n",
            "177 0.591613 [[ 0.06454565  0.19275479  3.3775928 ]\n",
            " [ 0.14149623 -0.05226309 -0.12655225]\n",
            " [-0.03843453 -0.09317382 -1.08448   ]]\n",
            "178 0.5909579 [[ 0.06022559  0.19373691  3.3809307 ]\n",
            " [ 0.14167228 -0.05204783 -0.12694357]\n",
            " [-0.03694569 -0.09360038 -1.0855423 ]]\n",
            "179 0.5903057 [[ 0.05591526  0.19471759  3.3842604 ]\n",
            " [ 0.14184487 -0.05183276 -0.12733123]\n",
            " [-0.03545746 -0.09402658 -1.0866044 ]]\n",
            "180 0.58965635 [[ 0.05161462  0.1956968   3.3875818 ]\n",
            " [ 0.14201413 -0.05161802 -0.12771524]\n",
            " [-0.03396982 -0.09445251 -1.087666  ]]\n",
            "181 0.5890099 [[ 0.04732355  0.19667459  3.3908951 ]\n",
            " [ 0.14218006 -0.0514035  -0.12809569]\n",
            " [-0.0324829  -0.094878   -1.0887275 ]]\n",
            "182 0.58836627 [[ 0.04304203  0.19765092  3.3942003 ]\n",
            " [ 0.14234284 -0.05118936 -0.12847258]\n",
            " [-0.03099659 -0.09530316 -1.0897886 ]]\n",
            "183 0.5877255 [[ 0.03876995  0.19862586  3.3974974 ]\n",
            " [ 0.1425024  -0.0509755  -0.12884599]\n",
            " [-0.02951107 -0.09572783 -1.0908494 ]]\n",
            "184 0.5870875 [[ 0.03450726  0.19959939  3.4007866 ]\n",
            " [ 0.14265889 -0.05076206 -0.12921591]\n",
            " [-0.02802628 -0.09615213 -1.0919099 ]]\n",
            "185 0.5864523 [[ 0.03025388  0.2005715   3.404068  ]\n",
            " [ 0.14281233 -0.05054897 -0.12958242]\n",
            " [-0.02654231 -0.09657594 -1.09297   ]]\n",
            "186 0.58581984 [[ 0.02600974  0.20154224  3.4073415 ]\n",
            " [ 0.1429628  -0.05033631 -0.12994555]\n",
            " [-0.02505913 -0.09699929 -1.0940298 ]]\n",
            "187 0.58519006 [[ 0.02177477  0.2025116   3.410607  ]\n",
            " [ 0.14311033 -0.05012406 -0.13030532]\n",
            " [-0.02357684 -0.09742212 -1.0950892 ]]\n",
            "188 0.584563 [[ 0.01754891  0.20347957  3.413865  ]\n",
            " [ 0.14325503 -0.04991228 -0.13066179]\n",
            " [-0.02209542 -0.09784447 -1.0961483 ]]\n",
            "189 0.58393866 [[ 0.01333209  0.2044462   3.4171152 ]\n",
            " [ 0.14339688 -0.04970095 -0.13101497]\n",
            " [-0.02061493 -0.09826627 -1.097207  ]]\n",
            "190 0.5833169 [[ 0.00912424  0.20541145  3.4203577 ]\n",
            " [ 0.143536   -0.0494901  -0.13136493]\n",
            " [-0.01913537 -0.09868754 -1.0982653 ]]\n",
            "191 0.58269775 [[ 0.00492531  0.20637536  3.4235928 ]\n",
            " [ 0.1436724  -0.04927976 -0.13171166]\n",
            " [-0.01765677 -0.09910826 -1.0993232 ]]\n",
            "192 0.58208126 [[ 7.3521252e-04  2.0733793e-01  3.4268203e+00]\n",
            " [ 1.4380616e-01 -4.9069937e-02 -1.3205524e-01]\n",
            " [-1.6179178e-02 -9.9528417e-02 -1.1003807e+00]]\n",
            "193 0.5814673 [[-0.0034461   0.20829917  3.4300404 ]\n",
            " [ 0.14393729 -0.04886064 -0.13239568]\n",
            " [-0.01470262 -0.09994797 -1.1014377 ]]\n",
            "194 0.58085597 [[-0.00761869  0.2092591   3.433253  ]\n",
            " [ 0.14406589 -0.04865187 -0.13273305]\n",
            " [-0.01322706 -0.10036693 -1.1024942 ]]\n",
            "195 0.58024704 [[-0.01178263  0.2102177   3.4364583 ]\n",
            " [ 0.14419197 -0.04844365 -0.13306735]\n",
            " [-0.01175259 -0.10078525 -1.1035504 ]]\n",
            "196 0.5796407 [[-0.01593796  0.21117498  3.4396565 ]\n",
            " [ 0.14431562 -0.04823603 -0.13339862]\n",
            " [-0.01027917 -0.10120299 -1.1046062 ]]\n",
            "197 0.57903683 [[-0.02008476  0.21213096  3.4428473 ]\n",
            " [ 0.1444368  -0.04802893 -0.13372691]\n",
            " [-0.0088069  -0.10162003 -1.1056614 ]]\n",
            "198 0.5784354 [[-0.02422307  0.21308565  3.4460309 ]\n",
            " [ 0.14455569 -0.04782246 -0.13405223]\n",
            " [-0.00733568 -0.10203648 -1.1067162 ]]\n",
            "199 0.57783645 [[-0.02835297  0.21403906  3.4492073 ]\n",
            " [ 0.14467217 -0.04761654 -0.13437463]\n",
            " [-0.00586568 -0.10245223 -1.1077704 ]]\n",
            "200 0.5772399 [[-0.0324745   0.21499117  3.4523768 ]\n",
            " [ 0.14478643 -0.04741128 -0.13469416]\n",
            " [-0.00439678 -0.10286736 -1.1088243 ]]\n",
            "Prediction: [2 2 2]\n",
            "Accuracy:  1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ddxGZ_2vwYIv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 입력 데이터의 정규화의 유무에 따른 모델 학습 성공 유무 확인하기\n",
        "\n",
        "### 1) 코드에 주석을 달아 제출하세요\n",
        "\n",
        "### 2) 데이터 정규화 유무에 따른 모델 학습의 결과를 서술하여 제출하세요.\n",
        "\n",
        "- 입력데이터의 feature 별 값의 스케일 차가 크면, 학습이 이루어지지 않고 발산하기 쉽다. <br> [??] 부분에 xy = min_max_scaler(xy)  코드 넣어, 안정적으로 모델이 학습되는 것을 확인한다."
      ]
    },
    {
      "metadata": {
        "id": "jMLMUkQaNfF1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17187
        },
        "outputId": "8672d13c-4613-48b7-c6ae-9116a972a0ae"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "tf.set_random_seed(777)  \n",
        "\n",
        "xy = np.array([[828.659973, 833.450012, 908100, 828.349976, 831.659973],\n",
        "               [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n",
        "               [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n",
        "               [816, 820.958984, 1008100, 815.48999, 819.23999],\n",
        "               [819.359985, 823, 1188100, 818.469971, 818.97998],\n",
        "               [819, 823, 1198100, 816, 820.450012],\n",
        "               [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n",
        "               [809.51001, 816.659973, 1398100, 804.539978, 809.559998]])\n",
        "\n",
        "\n",
        "# very important. It does not work without it.\n",
        "# very important. It does not work without it.\n",
        "# 이 코드를 그대로 돌리면 발산하여 nan값이 나온다.\n",
        "# 따라서 우리는 데이터를 정규화할 필요가 있다.(모든 특성들이 0 과 1 사이에 위치하도록 데이터를 비례적으로 조정)\n",
        "def min_max_scaler(data):\n",
        "    numerator = data - np.min(data, 0) # 분자\n",
        "    denominator = np.max(data, 0) - np.min(data, 0) # 분모\n",
        "    return numerator / (denominator + 1e-7)\n",
        "  \n",
        "xy = min_max_scaler(xy) # xy값을 정규화\n",
        "\n",
        "\n",
        "x_data = xy[:, 0:-1]\n",
        "y_data = xy[:, [-1]]\n",
        "\n",
        "\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=[None, 4])\n",
        "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
        "\n",
        "W = tf.Variable(tf.random_normal([4, 1]), name='weight')\n",
        "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
        "\n",
        "hypothesis = tf.matmul(X, W) + b\n",
        "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
        "train = optimizer.minimize(cost)\n",
        "\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "for step in range(101):\n",
        "    cost_val, hy_val, _ = sess.run(\n",
        "        [cost, hypothesis, train], feed_dict={X: x_data, Y: y_data})\n",
        "    print(step, \"Cost: \", cost_val, \"\\nPrediction:\\n\", hy_val)\n",
        "\n",
        "\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 Cost:  0.25993007 \n",
            "Prediction:\n",
            " [[-0.07906365]\n",
            " [ 1.2109591 ]\n",
            " [ 0.6404447 ]\n",
            " [ 0.07747714]\n",
            " [ 0.24732162]\n",
            " [ 0.28449512]\n",
            " [ 0.12401056]\n",
            " [ 0.7523401 ]]\n",
            "1 Cost:  0.25992706 \n",
            "Prediction:\n",
            " [[-0.0790532 ]\n",
            " [ 1.2109658 ]\n",
            " [ 0.6404505 ]\n",
            " [ 0.07748213]\n",
            " [ 0.24732745]\n",
            " [ 0.28450063]\n",
            " [ 0.12401322]\n",
            " [ 0.75234145]]\n",
            "2 Cost:  0.25992408 \n",
            "Prediction:\n",
            " [[-0.07904274]\n",
            " [ 1.2109722 ]\n",
            " [ 0.6404564 ]\n",
            " [ 0.0774871 ]\n",
            " [ 0.24733335]\n",
            " [ 0.28450614]\n",
            " [ 0.12401588]\n",
            " [ 0.7523429 ]]\n",
            "3 Cost:  0.25992107 \n",
            "Prediction:\n",
            " [[-0.07903228]\n",
            " [ 1.2109787 ]\n",
            " [ 0.6404622 ]\n",
            " [ 0.0774921 ]\n",
            " [ 0.24733923]\n",
            " [ 0.28451163]\n",
            " [ 0.12401852]\n",
            " [ 0.7523443 ]]\n",
            "4 Cost:  0.25991803 \n",
            "Prediction:\n",
            " [[-0.07902182]\n",
            " [ 1.2109853 ]\n",
            " [ 0.6404681 ]\n",
            " [ 0.07749707]\n",
            " [ 0.24734506]\n",
            " [ 0.28451708]\n",
            " [ 0.12402119]\n",
            " [ 0.7523457 ]]\n",
            "5 Cost:  0.259915 \n",
            "Prediction:\n",
            " [[-0.07901137]\n",
            " [ 1.2109919 ]\n",
            " [ 0.64047384]\n",
            " [ 0.07750206]\n",
            " [ 0.24735099]\n",
            " [ 0.2845226 ]\n",
            " [ 0.12402383]\n",
            " [ 0.75234705]]\n",
            "6 Cost:  0.259912 \n",
            "Prediction:\n",
            " [[-0.07900091]\n",
            " [ 1.2109985 ]\n",
            " [ 0.64047974]\n",
            " [ 0.07750705]\n",
            " [ 0.24735685]\n",
            " [ 0.28452814]\n",
            " [ 0.12402648]\n",
            " [ 0.7523485 ]]\n",
            "7 Cost:  0.259909 \n",
            "Prediction:\n",
            " [[-0.07899044]\n",
            " [ 1.2110051 ]\n",
            " [ 0.6404856 ]\n",
            " [ 0.07751202]\n",
            " [ 0.24736273]\n",
            " [ 0.2845336 ]\n",
            " [ 0.12402912]\n",
            " [ 0.75234985]]\n",
            "8 Cost:  0.25990596 \n",
            "Prediction:\n",
            " [[-0.07897998]\n",
            " [ 1.2110115 ]\n",
            " [ 0.64049137]\n",
            " [ 0.07751702]\n",
            " [ 0.24736857]\n",
            " [ 0.28453904]\n",
            " [ 0.12403178]\n",
            " [ 0.7523513 ]]\n",
            "9 Cost:  0.25990292 \n",
            "Prediction:\n",
            " [[-0.07896953]\n",
            " [ 1.2110181 ]\n",
            " [ 0.6404972 ]\n",
            " [ 0.07752198]\n",
            " [ 0.24737449]\n",
            " [ 0.2845446 ]\n",
            " [ 0.12403443]\n",
            " [ 0.75235265]]\n",
            "10 Cost:  0.25989994 \n",
            "Prediction:\n",
            " [[-0.07895907]\n",
            " [ 1.2110246 ]\n",
            " [ 0.6405031 ]\n",
            " [ 0.07752698]\n",
            " [ 0.24738038]\n",
            " [ 0.28455007]\n",
            " [ 0.12403709]\n",
            " [ 0.7523541 ]]\n",
            "11 Cost:  0.25989693 \n",
            "Prediction:\n",
            " [[-0.07894862]\n",
            " [ 1.2110313 ]\n",
            " [ 0.64050883]\n",
            " [ 0.07753193]\n",
            " [ 0.24738622]\n",
            " [ 0.28455555]\n",
            " [ 0.12403975]\n",
            " [ 0.75235546]]\n",
            "12 Cost:  0.2598939 \n",
            "Prediction:\n",
            " [[-0.07893816]\n",
            " [ 1.2110379 ]\n",
            " [ 0.64051473]\n",
            " [ 0.07753693]\n",
            " [ 0.2473921 ]\n",
            " [ 0.2845611 ]\n",
            " [ 0.12404239]\n",
            " [ 0.7523568 ]]\n",
            "13 Cost:  0.2598909 \n",
            "Prediction:\n",
            " [[-0.0789277 ]\n",
            " [ 1.2110444 ]\n",
            " [ 0.64052063]\n",
            " [ 0.0775419 ]\n",
            " [ 0.24739802]\n",
            " [ 0.28456652]\n",
            " [ 0.12404505]\n",
            " [ 0.75235826]]\n",
            "14 Cost:  0.25988793 \n",
            "Prediction:\n",
            " [[-0.07891724]\n",
            " [ 1.211051  ]\n",
            " [ 0.6405265 ]\n",
            " [ 0.07754689]\n",
            " [ 0.24740386]\n",
            " [ 0.28457206]\n",
            " [ 0.12404769]\n",
            " [ 0.7523597 ]]\n",
            "15 Cost:  0.2598849 \n",
            "Prediction:\n",
            " [[-0.07890678]\n",
            " [ 1.2110574 ]\n",
            " [ 0.64053226]\n",
            " [ 0.07755186]\n",
            " [ 0.24740972]\n",
            " [ 0.28457752]\n",
            " [ 0.12405036]\n",
            " [ 0.75236106]]\n",
            "16 Cost:  0.25988185 \n",
            "Prediction:\n",
            " [[-0.07889633]\n",
            " [ 1.211064  ]\n",
            " [ 0.64053816]\n",
            " [ 0.07755686]\n",
            " [ 0.2474156 ]\n",
            " [ 0.28458303]\n",
            " [ 0.12405302]\n",
            " [ 0.7523625 ]]\n",
            "17 Cost:  0.25987887 \n",
            "Prediction:\n",
            " [[-0.07888587]\n",
            " [ 1.2110707 ]\n",
            " [ 0.64054394]\n",
            " [ 0.07756182]\n",
            " [ 0.24742147]\n",
            " [ 0.28458855]\n",
            " [ 0.12405566]\n",
            " [ 0.7523638 ]]\n",
            "18 Cost:  0.25987583 \n",
            "Prediction:\n",
            " [[-0.07887541]\n",
            " [ 1.2110772 ]\n",
            " [ 0.6405497 ]\n",
            " [ 0.07756682]\n",
            " [ 0.24742736]\n",
            " [ 0.28459397]\n",
            " [ 0.12405832]\n",
            " [ 0.75236523]]\n",
            "19 Cost:  0.2598728 \n",
            "Prediction:\n",
            " [[-0.07886495]\n",
            " [ 1.2110838 ]\n",
            " [ 0.64055556]\n",
            " [ 0.07757177]\n",
            " [ 0.24743325]\n",
            " [ 0.28459948]\n",
            " [ 0.12406096]\n",
            " [ 0.75236666]]\n",
            "20 Cost:  0.25986978 \n",
            "Prediction:\n",
            " [[-0.07885449]\n",
            " [ 1.2110902 ]\n",
            " [ 0.64056146]\n",
            " [ 0.07757678]\n",
            " [ 0.24743907]\n",
            " [ 0.28460503]\n",
            " [ 0.12406362]\n",
            " [ 0.75236803]]\n",
            "21 Cost:  0.25986683 \n",
            "Prediction:\n",
            " [[-0.07884404]\n",
            " [ 1.2110969 ]\n",
            " [ 0.6405673 ]\n",
            " [ 0.07758177]\n",
            " [ 0.247445  ]\n",
            " [ 0.2846105 ]\n",
            " [ 0.12406627]\n",
            " [ 0.75236946]]\n",
            "22 Cost:  0.2598638 \n",
            "Prediction:\n",
            " [[-0.07883358]\n",
            " [ 1.2111034 ]\n",
            " [ 0.6405731 ]\n",
            " [ 0.07758673]\n",
            " [ 0.24745086]\n",
            " [ 0.284616  ]\n",
            " [ 0.12406893]\n",
            " [ 0.7523708 ]]\n",
            "23 Cost:  0.25986075 \n",
            "Prediction:\n",
            " [[-0.07882313]\n",
            " [ 1.21111   ]\n",
            " [ 0.640579  ]\n",
            " [ 0.07759172]\n",
            " [ 0.24745674]\n",
            " [ 0.28462148]\n",
            " [ 0.12407158]\n",
            " [ 0.7523722 ]]\n",
            "24 Cost:  0.25985777 \n",
            "Prediction:\n",
            " [[-0.07881267]\n",
            " [ 1.2111166 ]\n",
            " [ 0.6405848 ]\n",
            " [ 0.07759669]\n",
            " [ 0.24746257]\n",
            " [ 0.28462696]\n",
            " [ 0.12407423]\n",
            " [ 0.75237364]]\n",
            "25 Cost:  0.25985473 \n",
            "Prediction:\n",
            " [[-0.07880221]\n",
            " [ 1.211123  ]\n",
            " [ 0.6405906 ]\n",
            " [ 0.07760169]\n",
            " [ 0.24746847]\n",
            " [ 0.28463247]\n",
            " [ 0.12407688]\n",
            " [ 0.752375  ]]\n",
            "26 Cost:  0.25985175 \n",
            "Prediction:\n",
            " [[-0.07879175]\n",
            " [ 1.2111297 ]\n",
            " [ 0.6405965 ]\n",
            " [ 0.07760665]\n",
            " [ 0.24747439]\n",
            " [ 0.28463793]\n",
            " [ 0.12407953]\n",
            " [ 0.75237644]]\n",
            "27 Cost:  0.25984874 \n",
            "Prediction:\n",
            " [[-0.07878131]\n",
            " [ 1.2111362 ]\n",
            " [ 0.6406023 ]\n",
            " [ 0.07761164]\n",
            " [ 0.24748021]\n",
            " [ 0.2846434 ]\n",
            " [ 0.12408219]\n",
            " [ 0.7523778 ]]\n",
            "28 Cost:  0.25984573 \n",
            "Prediction:\n",
            " [[-0.07877085]\n",
            " [ 1.2111429 ]\n",
            " [ 0.6406081 ]\n",
            " [ 0.0776166 ]\n",
            " [ 0.2474861 ]\n",
            " [ 0.28464893]\n",
            " [ 0.12408483]\n",
            " [ 0.7523792 ]]\n",
            "29 Cost:  0.2598427 \n",
            "Prediction:\n",
            " [[-0.07876039]\n",
            " [ 1.2111493 ]\n",
            " [ 0.640614  ]\n",
            " [ 0.07762159]\n",
            " [ 0.24749196]\n",
            " [ 0.2846544 ]\n",
            " [ 0.12408748]\n",
            " [ 0.7523806 ]]\n",
            "30 Cost:  0.25983968 \n",
            "Prediction:\n",
            " [[-0.07874994]\n",
            " [ 1.2111559 ]\n",
            " [ 0.6406199 ]\n",
            " [ 0.07762656]\n",
            " [ 0.2474978 ]\n",
            " [ 0.28465992]\n",
            " [ 0.12409011]\n",
            " [ 0.75238204]]\n",
            "31 Cost:  0.2598367 \n",
            "Prediction:\n",
            " [[-0.07873949]\n",
            " [ 1.2111624 ]\n",
            " [ 0.64062566]\n",
            " [ 0.07763155]\n",
            " [ 0.24750371]\n",
            " [ 0.2846654 ]\n",
            " [ 0.12409277]\n",
            " [ 0.7523834 ]]\n",
            "32 Cost:  0.25983366 \n",
            "Prediction:\n",
            " [[-0.07872903]\n",
            " [ 1.211169  ]\n",
            " [ 0.64063144]\n",
            " [ 0.07763652]\n",
            " [ 0.2475096 ]\n",
            " [ 0.28467086]\n",
            " [ 0.12409541]\n",
            " [ 0.7523848 ]]\n",
            "33 Cost:  0.25983065 \n",
            "Prediction:\n",
            " [[-0.07871858]\n",
            " [ 1.2111757 ]\n",
            " [ 0.64063734]\n",
            " [ 0.0776415 ]\n",
            " [ 0.24751543]\n",
            " [ 0.28467637]\n",
            " [ 0.12409808]\n",
            " [ 0.75238615]]\n",
            "34 Cost:  0.25982767 \n",
            "Prediction:\n",
            " [[-0.07870813]\n",
            " [ 1.2111821 ]\n",
            " [ 0.6406431 ]\n",
            " [ 0.07764649]\n",
            " [ 0.24752131]\n",
            " [ 0.28468186]\n",
            " [ 0.12410071]\n",
            " [ 0.7523876 ]]\n",
            "35 Cost:  0.25982463 \n",
            "Prediction:\n",
            " [[-0.07869767]\n",
            " [ 1.2111887 ]\n",
            " [ 0.64064896]\n",
            " [ 0.07765146]\n",
            " [ 0.2475272 ]\n",
            " [ 0.2846874 ]\n",
            " [ 0.12410337]\n",
            " [ 0.752389  ]]\n",
            "36 Cost:  0.25982162 \n",
            "Prediction:\n",
            " [[-0.07868722]\n",
            " [ 1.2111952 ]\n",
            " [ 0.64065486]\n",
            " [ 0.07765645]\n",
            " [ 0.2475331 ]\n",
            " [ 0.28469285]\n",
            " [ 0.12410603]\n",
            " [ 0.7523904 ]]\n",
            "37 Cost:  0.2598186 \n",
            "Prediction:\n",
            " [[-0.07867677]\n",
            " [ 1.2112018 ]\n",
            " [ 0.6406606 ]\n",
            " [ 0.07766141]\n",
            " [ 0.24753892]\n",
            " [ 0.2846983 ]\n",
            " [ 0.12410866]\n",
            " [ 0.75239176]]\n",
            "38 Cost:  0.25981563 \n",
            "Prediction:\n",
            " [[-0.07866631]\n",
            " [ 1.2112085 ]\n",
            " [ 0.6406665 ]\n",
            " [ 0.07766641]\n",
            " [ 0.24754481]\n",
            " [ 0.28470382]\n",
            " [ 0.12411134]\n",
            " [ 0.7523932 ]]\n",
            "39 Cost:  0.2598126 \n",
            "Prediction:\n",
            " [[-0.07865585]\n",
            " [ 1.2112149 ]\n",
            " [ 0.6406723 ]\n",
            " [ 0.07767136]\n",
            " [ 0.2475507 ]\n",
            " [ 0.2847093 ]\n",
            " [ 0.12411398]\n",
            " [ 0.75239456]]\n",
            "40 Cost:  0.2598096 \n",
            "Prediction:\n",
            " [[-0.07864541]\n",
            " [ 1.2112215 ]\n",
            " [ 0.6406782 ]\n",
            " [ 0.07767636]\n",
            " [ 0.24755652]\n",
            " [ 0.28471482]\n",
            " [ 0.12411663]\n",
            " [ 0.752396  ]]\n",
            "41 Cost:  0.2598066 \n",
            "Prediction:\n",
            " [[-0.07863495]\n",
            " [ 1.211228  ]\n",
            " [ 0.640684  ]\n",
            " [ 0.07768131]\n",
            " [ 0.24756244]\n",
            " [ 0.28472027]\n",
            " [ 0.12411927]\n",
            " [ 0.7523974 ]]\n",
            "42 Cost:  0.25980356 \n",
            "Prediction:\n",
            " [[-0.0786245 ]\n",
            " [ 1.2112346 ]\n",
            " [ 0.6406898 ]\n",
            " [ 0.0776863 ]\n",
            " [ 0.24756832]\n",
            " [ 0.2847258 ]\n",
            " [ 0.12412192]\n",
            " [ 0.7523987 ]]\n",
            "43 Cost:  0.25980055 \n",
            "Prediction:\n",
            " [[-0.07861405]\n",
            " [ 1.2112412 ]\n",
            " [ 0.6406957 ]\n",
            " [ 0.07769126]\n",
            " [ 0.24757412]\n",
            " [ 0.28473124]\n",
            " [ 0.12412456]\n",
            " [ 0.75240016]]\n",
            "44 Cost:  0.25979754 \n",
            "Prediction:\n",
            " [[-0.0786036 ]\n",
            " [ 1.2112478 ]\n",
            " [ 0.6407015 ]\n",
            " [ 0.07769626]\n",
            " [ 0.24758   ]\n",
            " [ 0.28473678]\n",
            " [ 0.12412723]\n",
            " [ 0.75240153]]\n",
            "45 Cost:  0.25979453 \n",
            "Prediction:\n",
            " [[-0.07859314]\n",
            " [ 1.2112542 ]\n",
            " [ 0.6407073 ]\n",
            " [ 0.07770123]\n",
            " [ 0.2475859 ]\n",
            " [ 0.28474227]\n",
            " [ 0.12412987]\n",
            " [ 0.75240296]]\n",
            "46 Cost:  0.25979152 \n",
            "Prediction:\n",
            " [[-0.07858269]\n",
            " [ 1.2112608 ]\n",
            " [ 0.64071316]\n",
            " [ 0.07770621]\n",
            " [ 0.24759176]\n",
            " [ 0.2847477 ]\n",
            " [ 0.12413252]\n",
            " [ 0.75240433]]\n",
            "47 Cost:  0.2597885 \n",
            "Prediction:\n",
            " [[-0.07857224]\n",
            " [ 1.2112674 ]\n",
            " [ 0.640719  ]\n",
            " [ 0.07771117]\n",
            " [ 0.24759765]\n",
            " [ 0.2847532 ]\n",
            " [ 0.12413516]\n",
            " [ 0.7524057 ]]\n",
            "48 Cost:  0.2597855 \n",
            "Prediction:\n",
            " [[-0.07856179]\n",
            " [ 1.2112738 ]\n",
            " [ 0.64072484]\n",
            " [ 0.07771616]\n",
            " [ 0.24760354]\n",
            " [ 0.28475872]\n",
            " [ 0.12413781]\n",
            " [ 0.75240713]]\n",
            "49 Cost:  0.2597825 \n",
            "Prediction:\n",
            " [[-0.07855134]\n",
            " [ 1.2112806 ]\n",
            " [ 0.6407307 ]\n",
            " [ 0.07772115]\n",
            " [ 0.24760936]\n",
            " [ 0.28476417]\n",
            " [ 0.12414047]\n",
            " [ 0.7524085 ]]\n",
            "50 Cost:  0.2597795 \n",
            "Prediction:\n",
            " [[-0.07854088]\n",
            " [ 1.211287  ]\n",
            " [ 0.64073646]\n",
            " [ 0.07772611]\n",
            " [ 0.24761525]\n",
            " [ 0.2847697 ]\n",
            " [ 0.12414312]\n",
            " [ 0.7524099 ]]\n",
            "51 Cost:  0.2597765 \n",
            "Prediction:\n",
            " [[-0.07853043]\n",
            " [ 1.2112937 ]\n",
            " [ 0.6407423 ]\n",
            " [ 0.07773111]\n",
            " [ 0.24762113]\n",
            " [ 0.28477514]\n",
            " [ 0.12414578]\n",
            " [ 0.7524113 ]]\n",
            "52 Cost:  0.25977346 \n",
            "Prediction:\n",
            " [[-0.07851998]\n",
            " [ 1.2113001 ]\n",
            " [ 0.64074814]\n",
            " [ 0.07773606]\n",
            " [ 0.24762702]\n",
            " [ 0.28478065]\n",
            " [ 0.1241484 ]\n",
            " [ 0.7524127 ]]\n",
            "53 Cost:  0.25977045 \n",
            "Prediction:\n",
            " [[-0.07850952]\n",
            " [ 1.2113066 ]\n",
            " [ 0.640754  ]\n",
            " [ 0.07774106]\n",
            " [ 0.24763285]\n",
            " [ 0.28478613]\n",
            " [ 0.12415105]\n",
            " [ 0.7524141 ]]\n",
            "54 Cost:  0.25976747 \n",
            "Prediction:\n",
            " [[-0.07849908]\n",
            " [ 1.2113134 ]\n",
            " [ 0.6407598 ]\n",
            " [ 0.07774601]\n",
            " [ 0.24763873]\n",
            " [ 0.28479165]\n",
            " [ 0.12415369]\n",
            " [ 0.7524155 ]]\n",
            "55 Cost:  0.25976443 \n",
            "Prediction:\n",
            " [[-0.07848863]\n",
            " [ 1.2113198 ]\n",
            " [ 0.64076567]\n",
            " [ 0.077751  ]\n",
            " [ 0.24764462]\n",
            " [ 0.28479707]\n",
            " [ 0.12415634]\n",
            " [ 0.75241685]]\n",
            "56 Cost:  0.25976145 \n",
            "Prediction:\n",
            " [[-0.07847818]\n",
            " [ 1.2113264 ]\n",
            " [ 0.64077145]\n",
            " [ 0.07775597]\n",
            " [ 0.24765044]\n",
            " [ 0.2848026 ]\n",
            " [ 0.12415899]\n",
            " [ 0.7524183 ]]\n",
            "57 Cost:  0.2597584 \n",
            "Prediction:\n",
            " [[-0.07846773]\n",
            " [ 1.2113329 ]\n",
            " [ 0.64077735]\n",
            " [ 0.07776095]\n",
            " [ 0.24765636]\n",
            " [ 0.28480807]\n",
            " [ 0.12416165]\n",
            " [ 0.75241965]]\n",
            "58 Cost:  0.2597554 \n",
            "Prediction:\n",
            " [[-0.07845728]\n",
            " [ 1.2113395 ]\n",
            " [ 0.64078313]\n",
            " [ 0.0777659 ]\n",
            " [ 0.24766225]\n",
            " [ 0.2848136 ]\n",
            " [ 0.12416428]\n",
            " [ 0.752421  ]]\n",
            "59 Cost:  0.25975242 \n",
            "Prediction:\n",
            " [[-0.07844684]\n",
            " [ 1.211346  ]\n",
            " [ 0.6407889 ]\n",
            " [ 0.0777709 ]\n",
            " [ 0.24766804]\n",
            " [ 0.28481907]\n",
            " [ 0.12416694]\n",
            " [ 0.75242245]]\n",
            "60 Cost:  0.2597494 \n",
            "Prediction:\n",
            " [[-0.07843638]\n",
            " [ 1.2113526 ]\n",
            " [ 0.6407948 ]\n",
            " [ 0.07777586]\n",
            " [ 0.24767393]\n",
            " [ 0.28482452]\n",
            " [ 0.12416957]\n",
            " [ 0.7524238 ]]\n",
            "61 Cost:  0.25974637 \n",
            "Prediction:\n",
            " [[-0.07842593]\n",
            " [ 1.2113591 ]\n",
            " [ 0.64080065]\n",
            " [ 0.07778084]\n",
            " [ 0.24767981]\n",
            " [ 0.28483   ]\n",
            " [ 0.12417224]\n",
            " [ 0.75242525]]\n",
            "62 Cost:  0.2597434 \n",
            "Prediction:\n",
            " [[-0.07841548]\n",
            " [ 1.2113657 ]\n",
            " [ 0.6408064 ]\n",
            " [ 0.0777858 ]\n",
            " [ 0.24768564]\n",
            " [ 0.28483552]\n",
            " [ 0.12417488]\n",
            " [ 0.7524266 ]]\n",
            "63 Cost:  0.25974038 \n",
            "Prediction:\n",
            " [[-0.07840504]\n",
            " [ 1.2113723 ]\n",
            " [ 0.6408123 ]\n",
            " [ 0.07779079]\n",
            " [ 0.24769156]\n",
            " [ 0.284841  ]\n",
            " [ 0.12417752]\n",
            " [ 0.752428  ]]\n",
            "64 Cost:  0.25973737 \n",
            "Prediction:\n",
            " [[-0.07839459]\n",
            " [ 1.2113787 ]\n",
            " [ 0.6408182 ]\n",
            " [ 0.07779577]\n",
            " [ 0.24769744]\n",
            " [ 0.28484654]\n",
            " [ 0.12418016]\n",
            " [ 0.7524294 ]]\n",
            "65 Cost:  0.2597344 \n",
            "Prediction:\n",
            " [[-0.07838414]\n",
            " [ 1.2113854 ]\n",
            " [ 0.64082396]\n",
            " [ 0.07780073]\n",
            " [ 0.24770327]\n",
            " [ 0.28485194]\n",
            " [ 0.12418281]\n",
            " [ 0.7524308 ]]\n",
            "66 Cost:  0.25973135 \n",
            "Prediction:\n",
            " [[-0.07837369]\n",
            " [ 1.2113919 ]\n",
            " [ 0.64082974]\n",
            " [ 0.07780572]\n",
            " [ 0.24770913]\n",
            " [ 0.28485745]\n",
            " [ 0.12418545]\n",
            " [ 0.75243217]]\n",
            "67 Cost:  0.2597284 \n",
            "Prediction:\n",
            " [[-0.07836325]\n",
            " [ 1.2113985 ]\n",
            " [ 0.64083564]\n",
            " [ 0.07781068]\n",
            " [ 0.247715  ]\n",
            " [ 0.28486297]\n",
            " [ 0.12418812]\n",
            " [ 0.7524336 ]]\n",
            "68 Cost:  0.25972533 \n",
            "Prediction:\n",
            " [[-0.07835279]\n",
            " [ 1.211405  ]\n",
            " [ 0.6408414 ]\n",
            " [ 0.07781567]\n",
            " [ 0.24772088]\n",
            " [ 0.28486845]\n",
            " [ 0.12419076]\n",
            " [ 0.75243497]]\n",
            "69 Cost:  0.25972235 \n",
            "Prediction:\n",
            " [[-0.07834235]\n",
            " [ 1.2114115 ]\n",
            " [ 0.64084727]\n",
            " [ 0.07782062]\n",
            " [ 0.24772674]\n",
            " [ 0.28487387]\n",
            " [ 0.1241934 ]\n",
            " [ 0.7524364 ]]\n",
            "70 Cost:  0.25971937 \n",
            "Prediction:\n",
            " [[-0.0783319 ]\n",
            " [ 1.2114182 ]\n",
            " [ 0.64085305]\n",
            " [ 0.07782561]\n",
            " [ 0.24773262]\n",
            " [ 0.2848794 ]\n",
            " [ 0.12419604]\n",
            " [ 0.7524378 ]]\n",
            "71 Cost:  0.25971633 \n",
            "Prediction:\n",
            " [[-0.07832146]\n",
            " [ 1.2114246 ]\n",
            " [ 0.64085895]\n",
            " [ 0.07783057]\n",
            " [ 0.24773851]\n",
            " [ 0.28488487]\n",
            " [ 0.12419869]\n",
            " [ 0.75243914]]\n",
            "72 Cost:  0.25971332 \n",
            "Prediction:\n",
            " [[-0.078311  ]\n",
            " [ 1.2114313 ]\n",
            " [ 0.6408648 ]\n",
            " [ 0.07783555]\n",
            " [ 0.24774434]\n",
            " [ 0.28489035]\n",
            " [ 0.12420133]\n",
            " [ 0.7524406 ]]\n",
            "73 Cost:  0.2597103 \n",
            "Prediction:\n",
            " [[-0.07830055]\n",
            " [ 1.2114378 ]\n",
            " [ 0.6408706 ]\n",
            " [ 0.07784051]\n",
            " [ 0.2477502 ]\n",
            " [ 0.2848959 ]\n",
            " [ 0.12420399]\n",
            " [ 0.7524419 ]]\n",
            "74 Cost:  0.25970733 \n",
            "Prediction:\n",
            " [[-0.07829011]\n",
            " [ 1.2114443 ]\n",
            " [ 0.6408764 ]\n",
            " [ 0.0778455 ]\n",
            " [ 0.24775608]\n",
            " [ 0.2849013 ]\n",
            " [ 0.12420662]\n",
            " [ 0.7524433 ]]\n",
            "75 Cost:  0.2597043 \n",
            "Prediction:\n",
            " [[-0.07827967]\n",
            " [ 1.2114508 ]\n",
            " [ 0.64088225]\n",
            " [ 0.07785045]\n",
            " [ 0.24776193]\n",
            " [ 0.2849068 ]\n",
            " [ 0.12420928]\n",
            " [ 0.75244474]]\n",
            "76 Cost:  0.2597013 \n",
            "Prediction:\n",
            " [[-0.07826921]\n",
            " [ 1.2114574 ]\n",
            " [ 0.6408881 ]\n",
            " [ 0.07785544]\n",
            " [ 0.2477678 ]\n",
            " [ 0.28491232]\n",
            " [ 0.1242119 ]\n",
            " [ 0.7524461 ]]\n",
            "77 Cost:  0.25969827 \n",
            "Prediction:\n",
            " [[-0.07825877]\n",
            " [ 1.2114639 ]\n",
            " [ 0.6408939 ]\n",
            " [ 0.07786042]\n",
            " [ 0.24777369]\n",
            " [ 0.2849178 ]\n",
            " [ 0.12421454]\n",
            " [ 0.7524475 ]]\n",
            "78 Cost:  0.2596953 \n",
            "Prediction:\n",
            " [[-0.07824832]\n",
            " [ 1.2114706 ]\n",
            " [ 0.6408997 ]\n",
            " [ 0.07786538]\n",
            " [ 0.24777952]\n",
            " [ 0.2849233 ]\n",
            " [ 0.12421718]\n",
            " [ 0.7524489 ]]\n",
            "79 Cost:  0.2596923 \n",
            "Prediction:\n",
            " [[-0.07823788]\n",
            " [ 1.2114772 ]\n",
            " [ 0.64090556]\n",
            " [ 0.07787037]\n",
            " [ 0.2477854 ]\n",
            " [ 0.28492874]\n",
            " [ 0.12421985]\n",
            " [ 0.7524503 ]]\n",
            "80 Cost:  0.25968927 \n",
            "Prediction:\n",
            " [[-0.07822744]\n",
            " [ 1.2114836 ]\n",
            " [ 0.64091134]\n",
            " [ 0.07787532]\n",
            " [ 0.24779126]\n",
            " [ 0.28493422]\n",
            " [ 0.12422248]\n",
            " [ 0.75245166]]\n",
            "81 Cost:  0.2596863 \n",
            "Prediction:\n",
            " [[-0.07821698]\n",
            " [ 1.2114903 ]\n",
            " [ 0.64091724]\n",
            " [ 0.07788031]\n",
            " [ 0.24779713]\n",
            " [ 0.2849397 ]\n",
            " [ 0.12422513]\n",
            " [ 0.752453  ]]\n",
            "82 Cost:  0.25968328 \n",
            "Prediction:\n",
            " [[-0.07820654]\n",
            " [ 1.2114967 ]\n",
            " [ 0.6409231 ]\n",
            " [ 0.07788527]\n",
            " [ 0.24780299]\n",
            " [ 0.28494525]\n",
            " [ 0.12422777]\n",
            " [ 0.75245446]]\n",
            "83 Cost:  0.25968027 \n",
            "Prediction:\n",
            " [[-0.0781961 ]\n",
            " [ 1.2115031 ]\n",
            " [ 0.64092886]\n",
            " [ 0.07789025]\n",
            " [ 0.24780887]\n",
            " [ 0.2849507 ]\n",
            " [ 0.12423041]\n",
            " [ 0.7524559 ]]\n",
            "84 Cost:  0.2596773 \n",
            "Prediction:\n",
            " [[-0.07818565]\n",
            " [ 1.21151   ]\n",
            " [ 0.6409347 ]\n",
            " [ 0.07789521]\n",
            " [ 0.24781476]\n",
            " [ 0.28495613]\n",
            " [ 0.12423307]\n",
            " [ 0.7524572 ]]\n",
            "85 Cost:  0.25967425 \n",
            "Prediction:\n",
            " [[-0.07817521]\n",
            " [ 1.2115164 ]\n",
            " [ 0.64094055]\n",
            " [ 0.07790019]\n",
            " [ 0.24782059]\n",
            " [ 0.28496164]\n",
            " [ 0.12423571]\n",
            " [ 0.75245863]]\n",
            "86 Cost:  0.25967127 \n",
            "Prediction:\n",
            " [[-0.07816476]\n",
            " [ 1.2115229 ]\n",
            " [ 0.6409463 ]\n",
            " [ 0.07790515]\n",
            " [ 0.24782646]\n",
            " [ 0.28496712]\n",
            " [ 0.12423835]\n",
            " [ 0.75246006]]\n",
            "87 Cost:  0.25966823 \n",
            "Prediction:\n",
            " [[-0.07815433]\n",
            " [ 1.2115295 ]\n",
            " [ 0.64095217]\n",
            " [ 0.07791013]\n",
            " [ 0.24783234]\n",
            " [ 0.28497267]\n",
            " [ 0.12424099]\n",
            " [ 0.7524614 ]]\n",
            "88 Cost:  0.25966525 \n",
            "Prediction:\n",
            " [[-0.07814388]\n",
            " [ 1.211536  ]\n",
            " [ 0.640958  ]\n",
            " [ 0.07791508]\n",
            " [ 0.24783814]\n",
            " [ 0.28497806]\n",
            " [ 0.12424363]\n",
            " [ 0.7524628 ]]\n",
            "89 Cost:  0.25966224 \n",
            "Prediction:\n",
            " [[-0.07813343]\n",
            " [ 1.2115425 ]\n",
            " [ 0.64096385]\n",
            " [ 0.07792006]\n",
            " [ 0.24784403]\n",
            " [ 0.28498355]\n",
            " [ 0.12424628]\n",
            " [ 0.7524642 ]]\n",
            "90 Cost:  0.25965923 \n",
            "Prediction:\n",
            " [[-0.07812299]\n",
            " [ 1.2115492 ]\n",
            " [ 0.64096963]\n",
            " [ 0.07792502]\n",
            " [ 0.24784994]\n",
            " [ 0.28498906]\n",
            " [ 0.12424893]\n",
            " [ 0.75246555]]\n",
            "91 Cost:  0.25965625 \n",
            "Prediction:\n",
            " [[-0.07811254]\n",
            " [ 1.2115557 ]\n",
            " [ 0.6409755 ]\n",
            " [ 0.07793001]\n",
            " [ 0.24785575]\n",
            " [ 0.28499454]\n",
            " [ 0.12425158]\n",
            " [ 0.752467  ]]\n",
            "92 Cost:  0.2596532 \n",
            "Prediction:\n",
            " [[-0.0781021 ]\n",
            " [ 1.2115622 ]\n",
            " [ 0.6409813 ]\n",
            " [ 0.07793499]\n",
            " [ 0.24786164]\n",
            " [ 0.28500006]\n",
            " [ 0.12425421]\n",
            " [ 0.75246835]]\n",
            "93 Cost:  0.2596502 \n",
            "Prediction:\n",
            " [[-0.07809165]\n",
            " [ 1.2115688 ]\n",
            " [ 0.6409871 ]\n",
            " [ 0.07793994]\n",
            " [ 0.24786752]\n",
            " [ 0.28500548]\n",
            " [ 0.12425686]\n",
            " [ 0.7524697 ]]\n",
            "94 Cost:  0.25964725 \n",
            "Prediction:\n",
            " [[-0.07808121]\n",
            " [ 1.2115753 ]\n",
            " [ 0.64099294]\n",
            " [ 0.07794493]\n",
            " [ 0.24787334]\n",
            " [ 0.28501096]\n",
            " [ 0.12425949]\n",
            " [ 0.75247115]]\n",
            "95 Cost:  0.2596442 \n",
            "Prediction:\n",
            " [[-0.07807077]\n",
            " [ 1.2115818 ]\n",
            " [ 0.6409988 ]\n",
            " [ 0.07794988]\n",
            " [ 0.24787922]\n",
            " [ 0.28501648]\n",
            " [ 0.12426214]\n",
            " [ 0.7524725 ]]\n",
            "96 Cost:  0.25964123 \n",
            "Prediction:\n",
            " [[-0.07806033]\n",
            " [ 1.2115885 ]\n",
            " [ 0.64100456]\n",
            " [ 0.07795487]\n",
            " [ 0.24788508]\n",
            " [ 0.285022  ]\n",
            " [ 0.12426479]\n",
            " [ 0.7524739 ]]\n",
            "97 Cost:  0.25963822 \n",
            "Prediction:\n",
            " [[-0.07804988]\n",
            " [ 1.2115949 ]\n",
            " [ 0.64101046]\n",
            " [ 0.07795982]\n",
            " [ 0.24789096]\n",
            " [ 0.28502744]\n",
            " [ 0.12426744]\n",
            " [ 0.7524753 ]]\n",
            "98 Cost:  0.2596352 \n",
            "Prediction:\n",
            " [[-0.07803944]\n",
            " [ 1.2116015 ]\n",
            " [ 0.6410163 ]\n",
            " [ 0.07796481]\n",
            " [ 0.2478968 ]\n",
            " [ 0.2850329 ]\n",
            " [ 0.12427007]\n",
            " [ 0.7524767 ]]\n",
            "99 Cost:  0.2596322 \n",
            "Prediction:\n",
            " [[-0.078029  ]\n",
            " [ 1.211608  ]\n",
            " [ 0.6410221 ]\n",
            " [ 0.07796975]\n",
            " [ 0.24790269]\n",
            " [ 0.28503835]\n",
            " [ 0.1242727 ]\n",
            " [ 0.75247806]]\n",
            "100 Cost:  0.2596292 \n",
            "Prediction:\n",
            " [[-0.07801855]\n",
            " [ 1.2116145 ]\n",
            " [ 0.64102787]\n",
            " [ 0.07797473]\n",
            " [ 0.24790858]\n",
            " [ 0.28504387]\n",
            " [ 0.12427533]\n",
            " [ 0.75247943]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SkM5_iKgxYB4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## mnist 데이터 셋으로 손글씨 데이터 분류하기\n",
        "\n",
        "\n",
        "- 입력데이터는 28x28 영상 데이터를 784차원의 1차원 백터로 사용\n",
        "- 출력데이터는 숫자 정답\n",
        "\n",
        "### 1) 손글씨 데이터 정확도 올리기? 어떻게?\n",
        "- parameters인 num_epochs와 batch_size를 변경해보자!!\n",
        "  - num_epochs = 50, batch_size = 100\n",
        "\n",
        "### 2) 손글씨 데이터 정확도 올리기? 아이디어 있으면 고고!\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "hoaDfxarN_dG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1105
        },
        "outputId": "7d4ccf34-6356-46d2-a701-37c7953b1068"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "tf.set_random_seed(777)  \n",
        "\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
        "\n",
        "\n",
        "nb_classes = 10\n",
        "\n",
        "# MNIST data image of shape 28 * 28 = 784\n",
        "X = tf.placeholder(tf.float32, [None, 784])\n",
        "\n",
        "# 0 - 9 digits recognition = 10 classes\n",
        "Y = tf.placeholder(tf.float32, [None, nb_classes])\n",
        "\n",
        "W = tf.Variable(tf.random_normal([784, nb_classes]))\n",
        "b = tf.Variable(tf.random_normal([nb_classes]))\n",
        "\n",
        "\n",
        "\n",
        "# Hypothesis (using softmax)\n",
        "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
        "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
        "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
        "\n",
        "\n",
        "\n",
        "# Test model\n",
        "is_correct = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
        "\n",
        "# parameters\n",
        "num_epochs = 15\n",
        "batch_size = 100\n",
        "num_iterations = int(mnist.train.num_examples / batch_size)\n",
        "\n",
        "\n",
        "# 배치란?\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    # Training cycle\n",
        "    for epoch in range(num_epochs):\n",
        "        avg_cost = 0\n",
        "\n",
        "        for i in range(num_iterations):\n",
        "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
        "            _, cost_val = sess.run([train, cost], feed_dict={X: batch_xs, Y: batch_ys})\n",
        "            avg_cost += cost_val / num_iterations\n",
        "\n",
        "        print(\"Epoch: {:04d}, Cost: {:.9f}\".format(epoch + 1, avg_cost))\n",
        "\n",
        "    print(\"Learning finished\")\n",
        "\n",
        "    # Test the model using test sets\n",
        "    print(\n",
        "        \"Accuracy: \",\n",
        "        accuracy.eval(\n",
        "            session=sess, feed_dict={X: mnist.test.images, Y: mnist.test.labels}\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    # Get one and predict\n",
        "    r = random.randint(0, mnist.test.num_examples - 1)\n",
        "    print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r : r + 1], 1)))\n",
        "    print(\n",
        "        \"Prediction: \",\n",
        "        sess.run(tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r : r + 1]}),\n",
        "    )\n",
        "\n",
        "    plt.imshow(\n",
        "        mnist.test.images[r : r + 1].reshape(28, 28),\n",
        "        cmap=\"Greys\",\n",
        "        interpolation=\"nearest\",\n",
        "    )\n",
        "    plt.show()\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-24-6ec791963f23>:8: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "Epoch: 0001, Cost: 2.914575397\n",
            "Epoch: 0002, Cost: 1.122490676\n",
            "Epoch: 0003, Cost: 0.878470178\n",
            "Epoch: 0004, Cost: 0.762151589\n",
            "Epoch: 0005, Cost: 0.691885865\n",
            "Epoch: 0006, Cost: 0.642864555\n",
            "Epoch: 0007, Cost: 0.606515569\n",
            "Epoch: 0008, Cost: 0.578196641\n",
            "Epoch: 0009, Cost: 0.554384501\n",
            "Epoch: 0010, Cost: 0.534664059\n",
            "Epoch: 0011, Cost: 0.517891277\n",
            "Epoch: 0012, Cost: 0.503025515\n",
            "Epoch: 0013, Cost: 0.490162713\n",
            "Epoch: 0014, Cost: 0.478790062\n",
            "Epoch: 0015, Cost: 0.467843168\n",
            "Learning finished\n",
            "Accuracy:  0.888\n",
            "Label:  [1]\n",
            "Prediction:  [1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}