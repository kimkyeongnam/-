{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3. Neural network",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "h2uVVDK4u9EH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## [실습1] Multinomial Classification 문제를 해결하기 위해 적절한  Learning Rate 를 찾아봅시다.\n",
        "### 목표 \n",
        "*  Learning Rate 설정하기\n",
        "* 데이터 정규화유무에 따른 학습 경향성 파악\n",
        "\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "### 1) 직접 코드를 이해하여 주석을 달아 제출\n",
        "- 주석을 달면서 이해가 안가는 부분 조교에게 질문\n",
        "\n",
        "### 2) Learning rate 를 찾아보세요\n",
        "-  1.5 => 1e-10 => 0.1 순으로 변경해보자. \n",
        "- 아래의 ?? 부분에 learning rate 값을 넣으면 된다.\n",
        "  - optimizer = tf.train.GradientDescentOptimizer(learning_rate=???).minimize(cost)\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "WPaiH7_Oub-w",
        "colab_type": "code",
        "outputId": "14d49ec9-454d-4f1e-9f01-d98fb631cfba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 10302
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.set_random_seed(777)  # for reproducibility\n",
        "\n",
        "# 학습용 데이터 셋\n",
        "x_data = [[1, 2, 1],\n",
        "          [1, 3, 2],\n",
        "          [1, 3, 4],\n",
        "          [1, 5, 5],\n",
        "          [1, 7, 5],\n",
        "          [1, 2, 5],\n",
        "          [1, 6, 6],\n",
        "          [1, 7, 7]]\n",
        "\n",
        "y_data = [[0, 0, 1],\n",
        "          [0, 0, 1],\n",
        "          [0, 0, 1],\n",
        "          [0, 1, 0],\n",
        "          [0, 1, 0],\n",
        "          [0, 1, 0],\n",
        "          [1, 0, 0],\n",
        "          [1, 0, 0]]\n",
        "\n",
        "\n",
        "# 평가용 데이터 셋\n",
        "x_test = [[2, 1, 1],\n",
        "          [3, 1, 2],\n",
        "          [3, 3, 4]]\n",
        "\n",
        "y_test = [[0, 0, 1],\n",
        "          [0, 0, 1],\n",
        "          [0, 0, 1]]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# XW+b 설정\n",
        "X = tf.placeholder(\"float\", [None, 3]) # 행은 얼마나 들어올지 모르지만, 열은 3개로 고정\n",
        "Y = tf.placeholder(\"float\", [None, 3]) # 행은 얼마나 들어올지 모르지만, 열은 3개로 고정\n",
        "\n",
        "W = tf.Variable(tf.random_normal([3, 3])) # XW를 하기 위해서는 X의 열 = W의 행 / 열은 3개로 고정\n",
        "b = tf.Variable(tf.random_normal([3]))\n",
        "\n",
        "\n",
        "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b) # 선형 분류. softmax를 사용하여 정규화\n",
        "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1)) # 선형 분류 cost 계산 방법을 이용하여 cost 추출\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost) # GradientDescent를 이용하여 최소cost구하기\n",
        "\n",
        "\n",
        "# a = tf.constant([[3, 10, 1],[4, 5, 6],[0, 8, 7]])\n",
        "# session.run(tf.argmax(a, 0)) -> 인덱스  [1 0 2] 가 가장 크다\n",
        "# session.run(tf.argmax(a, 1)) -> 인덱스  [1 2 1] 가 가장 크다\n",
        "prediction = tf.argmax(hypothesis, 1) # hypothesis에서 가장 큰 값 반환\n",
        "is_correct = tf.equal(prediction, tf.argmax(Y, 1)) # if(prediction == Y에서 가장 큰 값 반환값) return 1, else return 0\n",
        "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32)) # 열 단위로 평균 추출(is_correct를 float32값으로 변환)\n",
        "\n",
        "with tf.Session() as sess: # sess = tf.Session()의 또 다른 표현\n",
        "\n",
        "    sess.run(tf.global_variables_initializer()) # global_variables_initalizer()를 이용해 변수들을 초기화한 후, 결과를 세션에 전달\n",
        "    for step in range(201):\n",
        "        cost_val, W_val, _ = sess.run([cost, W, optimizer], feed_dict={X: x_data, Y: y_data})\n",
        "        print(step, cost_val, W_val)\n",
        "\n",
        "    print(\"Prediction:\", sess.run(prediction, feed_dict={X: x_test}))\n",
        "    print(\"Accuracy: \", sess.run(accuracy, feed_dict={X: x_test, Y: y_test}))\n",
        "\n",
        "\n",
        "# 결과\n",
        "# learning rate = 1.5, nan 발생\n",
        "# learning rate = 1e-10, nan은 발생하지 않았지만 예측도가 떨어짐(0.0)\n",
        "# learning rate = 0.1, nan값이 발생하지 않고 예측도가 완벽함(1.0)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 2.595216 [[ 0.14828414 -1.1842268  -0.4407778 ]\n",
            " [ 0.5941795   1.340392   -0.6584907 ]\n",
            " [-0.0230028   0.05129077  0.4284553 ]]\n",
            "1 1.8875829 [[ 0.15476401 -1.2257323  -0.40575206]\n",
            " [ 0.70692635  1.1329627  -0.5638082 ]\n",
            " [ 0.08325816 -0.13217048  0.5056556 ]]\n",
            "2 1.6825478 [[ 0.11875311 -1.2228323  -0.37264112]\n",
            " [ 0.6187256   1.1311053  -0.47375003]\n",
            " [-0.01137124 -0.10657807  0.5746926 ]]\n",
            "3 1.5328807 [[ 0.10664171 -1.2387744  -0.34458765]\n",
            " [ 0.6451185   1.0269926  -0.3960301 ]\n",
            " [ 0.01275274 -0.17755057  0.62154114]]\n",
            "4 1.4523041 [[ 0.07697174 -1.2333007  -0.32039142]\n",
            " [ 0.5705146   1.036115   -0.33054876]\n",
            " [-0.05493739 -0.13998643  0.6516671 ]]\n",
            "5 1.403357 [[ 0.06867255 -1.2431663  -0.30222663]\n",
            " [ 0.6023443   0.95863396 -0.2848973 ]\n",
            " [-0.01664836 -0.18224914  0.65564084]]\n",
            "6 1.3746909 [[ 0.0461992  -1.2359465  -0.28697312]\n",
            " [ 0.5541285   0.97510624 -0.25315383]\n",
            " [-0.05231801 -0.13657404  0.64563537]]\n",
            "7 1.3477013 [[ 0.03815019 -1.2404933  -0.27437732]\n",
            " [ 0.5817772   0.92528677 -0.23098302]\n",
            " [-0.01577448 -0.15206692  0.62458473]]\n",
            "8 1.324408 [[ 0.01978749 -1.2346113  -0.2618967 ]\n",
            " [ 0.55304396  0.93414176 -0.21110478]\n",
            " [-0.03245031 -0.11352535  0.602719  ]]\n",
            "9 1.301932 [[ 0.01004312 -1.2367584  -0.2500053 ]\n",
            " [ 0.5704661   0.8981465  -0.1925317 ]\n",
            " [-0.00594393 -0.11677165  0.57945895]]\n",
            "10 1.2809675 [[-0.00638604 -1.2324542  -0.23788027]\n",
            " [ 0.5514267   0.89872676 -0.17407256]\n",
            " [-0.01416629 -0.08653986  0.5574495 ]]\n",
            "11 1.2607358 [[-0.01725785 -1.2333893  -0.22607338]\n",
            " [ 0.5620348   0.87023294 -0.15618685]\n",
            " [ 0.00524543 -0.08363955  0.5351375 ]]\n",
            "12 1.2413859 [[-0.03246864 -1.2301097  -0.21414211]\n",
            " [ 0.54882205  0.8657595  -0.13850068]\n",
            " [ 0.00181553 -0.05886999  0.5137978 ]]\n",
            "13 1.2226868 [[-0.04395203 -1.2303257  -0.20244274]\n",
            " [ 0.5553269   0.84205115 -0.12129716]\n",
            " [ 0.01669568 -0.05239581  0.4924435 ]]\n",
            "14 1.204669 [[-0.05830548 -1.2277136  -0.19070137]\n",
            " [ 0.5459148   0.8345744  -0.10440836]\n",
            " [ 0.01618804 -0.03126943  0.47182477]]\n",
            "15 1.1872538 [[-0.07009488 -1.2274859  -0.17913961]\n",
            " [ 0.5499072   0.8141297  -0.08795607]\n",
            " [ 0.02805885 -0.02268145  0.45136595]]\n",
            "16 1.1704433 [[-0.08380731 -1.2253256  -0.16758755]\n",
            " [ 0.5430728   0.8048924  -0.0718844 ]\n",
            " [ 0.02935548 -0.00411266  0.43150052]]\n",
            "17 1.1542044 [[-0.09571589 -1.2248226  -0.15618198]\n",
            " [ 0.5455156   0.78678715 -0.05622194]\n",
            " [ 0.0391532   0.00568441  0.41190574]]\n",
            "18 1.138531 [[-0.10892279 -1.2229791  -0.14481866]\n",
            " [ 0.5404831   0.77657247 -0.04097483]\n",
            " [ 0.04155843  0.02236601  0.3928189 ]]\n",
            "19 1.1234045 [[-0.12083219 -1.2223073  -0.13358095]\n",
            " [ 0.54197556  0.7602264  -0.02612126]\n",
            " [ 0.04988259  0.03279056  0.37407023]]\n",
            "20 1.108816 [[-0.13362058 -1.2206938  -0.12240607]\n",
            " [ 0.5382373   0.749542   -0.01169853]\n",
            " [ 0.05295619  0.04801001  0.35577717]]\n",
            "21 1.0947505 [[-0.14545418 -1.2199227  -0.11134364]\n",
            " [ 0.53915757  0.734583    0.00234019]\n",
            " [ 0.06020281  0.0586798   0.33786076]]\n",
            "22 1.0811973 [[-0.15788117 -1.2184825  -0.10035687]\n",
            " [ 0.5363719   0.723764    0.01594489]\n",
            " [ 0.06366416  0.07271237  0.32036683]]\n",
            "23 1.0681422 [[-0.16958883 -1.2176574  -0.08947424]\n",
            " [ 0.5369595   0.70994854  0.02917272]\n",
            " [ 0.07010201  0.08337267  0.30326867]]\n",
            "24 1.0555729 [[-0.18169282 -1.2163527  -0.07867502]\n",
            " [ 0.53489035  0.699219    0.04197145]\n",
            " [ 0.07377203  0.09640072  0.2865706 ]]\n",
            "25 1.0434754 [[-0.19324164 -1.2155044  -0.06797452]\n",
            " [ 0.5352966   0.6863839   0.05440028]\n",
            " [ 0.07958845  0.10688043  0.2702745 ]]\n",
            "26 1.0318357 [[-0.20504934 -1.2143093  -0.05736186]\n",
            " [ 0.53377634  0.675894    0.06641043]\n",
            " [ 0.08335342  0.11902822  0.25436175]]\n",
            "27 1.0206394 [[-0.216418   -1.2134583  -0.04684425]\n",
            " [ 0.5340952   0.66392684  0.07805868]\n",
            " [ 0.08868212  0.1292128   0.23884846]]\n",
            "28 1.0098721 [[-0.22794873 -1.2123551  -0.03641665]\n",
            " [ 0.53300154  0.6537769   0.08930233]\n",
            " [ 0.09247068  0.14056817  0.22370455]]\n",
            "29 0.9995189 [[-0.23912385 -1.2115153  -0.02608139]\n",
            " [ 0.5332901   0.64259696  0.10019367]\n",
            " [ 0.09740949  0.15038209  0.2089518 ]]\n",
            "30 0.989565 [[-0.25039232 -1.2104914  -0.01583681]\n",
            " [ 0.53253156  0.6328522   0.11069698]\n",
            " [ 0.10117789  0.16101006  0.19455543]]\n",
            "31 0.9799955 [[-0.26136613 -1.209672   -0.00568242]\n",
            " [ 0.5328226   0.6223989   0.12085919]\n",
            " [ 0.10579986  0.1704054   0.1805381 ]]\n",
            "32 0.9707957 [[-0.2723842  -1.208718    0.00438165]\n",
            " [ 0.53232926  0.61309993  0.13065155]\n",
            " [ 0.10952222  0.18035688  0.16686426]]\n",
            "33 0.9619504 [[-0.28315285 -1.2079251   0.01435746]\n",
            " [ 0.5326403   0.60332495  0.14011556]\n",
            " [ 0.11388309  0.18930538  0.15355492]]\n",
            "34 0.95344555 [[-0.2939306  -1.2070339   0.02424398]\n",
            " [ 0.5323574   0.5944946   0.1492288 ]\n",
            " [ 0.11754504  0.19862248  0.14057586]]\n",
            "35 0.9452665 [[-0.30449316 -1.2062714   0.03404405]\n",
            " [ 0.53269595  0.5853572   0.15802765]\n",
            " [ 0.121688    0.2071102   0.12794517]]\n",
            "36 0.9373994 [[-0.31503958 -1.2054373   0.04375631]\n",
            " [ 0.5325802   0.5770053   0.16649523]\n",
            " [ 0.12528253  0.21582969  0.11563116]]\n",
            "37 0.9298303 [[-0.32539722 -1.2047073   0.05338395]\n",
            " [ 0.53294796  0.56846946  0.17466329]\n",
            " [ 0.1292416   0.22385311  0.10364864]]\n",
            "38 0.92254615 [[-0.3357207  -1.2039255   0.06292558]\n",
            " [ 0.5329646   0.56059647  0.18251964]\n",
            " [ 0.13276617  0.23200835  0.09196883]]\n",
            "39 0.9155338 [[-0.3458761  -1.203229    0.07238442]\n",
            " [ 0.5333596   0.5526292   0.19009191]\n",
            " [ 0.1365684   0.23957165  0.08060329]]\n",
            "40 0.9087805 [[-0.3559845  -1.2024953   0.08175924]\n",
            " [ 0.5334806   0.5452284   0.1973717 ]\n",
            " [ 0.14002343  0.24719372  0.06952621]]\n",
            "41 0.9022745 [[-0.36594146 -1.2018323   0.09105317]\n",
            " [ 0.53389907  0.5377985   0.20438312]\n",
            " [ 0.14369059  0.2543065   0.05874628]]\n",
            "42 0.8960038 [[-0.37584254 -1.2011433   0.10026518]\n",
            " [ 0.53410155  0.53085834  0.21112084]\n",
            " [ 0.14707789  0.26142502  0.04824049]]\n",
            "43 0.8899574 [[-0.3856057  -1.2005131   0.10939825]\n",
            " [ 0.5345388   0.5239361   0.21760584]\n",
            " [ 0.15062754  0.26810086  0.03801499]]\n",
            "44 0.8841244 [[-0.3953069  -1.1998652   0.11845157]\n",
            " [ 0.5348043   0.5174413   0.22383523]\n",
            " [ 0.15394984  0.27474418  0.02804937]]\n",
            "45 0.87849456 [[-0.40488142 -1.199267    0.1274279 ]\n",
            " [ 0.5352555   0.51099795  0.22982734]\n",
            " [ 0.15739638  0.28099936  0.01834767]]\n",
            "46 0.8730579 [[-0.41439003 -1.1986573   0.13632673]\n",
            " [ 0.53556883  0.5049308   0.23558114]\n",
            " [ 0.16065677  0.28719482  0.00889185]]\n",
            "47 0.86780524 [[-4.2378148e-01 -1.1980896e+00  1.4515054e-01]\n",
            " [ 5.3602946e-01  4.9893865e-01  2.4111266e-01]\n",
            " [ 1.6401197e-01  2.9304734e-01 -3.1584050e-04]]\n",
            "48 0.8627274 [[-0.43310466 -1.1975149   0.15389906]\n",
            " [ 0.53637844  0.49328002  0.24642232]\n",
            " [ 0.16721354  0.29882124 -0.00929133]]\n",
            "49 0.8578159 [[-0.44231886 -1.1969762   0.16257457]\n",
            " [ 0.5368443   0.48771226  0.2515242 ]\n",
            " [ 0.17048706  0.30429012 -0.01803373]]\n",
            "50 0.8530626 [[-0.4514636  -1.1964339   0.17117698]\n",
            " [ 0.53721905  0.48244202  0.25641972]\n",
            " [ 0.17363286  0.3096678  -0.02655719]]\n",
            "51 0.84845996 [[-0.46050647 -1.1959225   0.17970839]\n",
            " [ 0.5376865   0.4772729   0.26112133]\n",
            " [ 0.17683278  0.31477258 -0.0348619 ]]\n",
            "52 0.84400046 [[-0.4694796  -1.1954099   0.18816887]\n",
            " [ 0.53807896  0.4723706   0.26563117]\n",
            " [ 0.17992562  0.31977823 -0.0429604 ]]\n",
            "53 0.839677 [[-0.47835705 -1.1949239   0.19656034]\n",
            " [ 0.53854513  0.46757537  0.26996025]\n",
            " [ 0.18305863  0.32453856 -0.05085372]]\n",
            "54 0.8354834 [[-0.48716515 -1.1944385   0.204883  ]\n",
            " [ 0.53894895  0.46302053  0.2741113 ]\n",
            " [ 0.18610111  0.32919535 -0.05855299]]\n",
            "55 0.8314132 [[-0.49588314 -1.193976    0.21313861]\n",
            " [ 0.5394111   0.4585756   0.27809402]\n",
            " [ 0.18917273  0.33363065 -0.06605988]]\n",
            "56 0.8274604 [[-0.5045326  -1.1935154   0.2213275 ]\n",
            " [ 0.5398213   0.45434803  0.28191146]\n",
            " [ 0.19216733  0.33796054 -0.07338434]]\n",
            "57 0.8236195 [[-0.5130969  -1.1930748   0.22945128]\n",
            " [ 0.5402776   0.4502309   0.2855723 ]\n",
            " [ 0.19518225  0.34208974 -0.08052848]]\n",
            "58 0.8198853 [[-0.521594   -1.1926368   0.23751035]\n",
            " [ 0.54069006  0.446311    0.28907976]\n",
            " [ 0.198131    0.34611368 -0.08750118]]\n",
            "59 0.81625277 [[-0.5300104  -1.1922163   0.24550621]\n",
            " [ 0.54113895  0.4425003   0.2924416 ]\n",
            " [ 0.20109326  0.3499551  -0.09430487]]\n",
            "60 0.81271696 [[-0.538361   -1.1917988   0.25343934]\n",
            " [ 0.54155064  0.43886906  0.29566115]\n",
            " [ 0.20399815  0.35369286 -0.10094751]]\n",
            "61 0.8092737 [[-0.5466351  -1.1913965   0.2613111 ]\n",
            " [ 0.5419909   0.43534455  0.2987454 ]\n",
            " [ 0.20691104  0.3572642  -0.10743173]]\n",
            "62 0.8059186 [[-0.554845   -1.1909976   0.26912206]\n",
            " [ 0.5423994   0.4319838   0.3016976 ]\n",
            " [ 0.20977385  0.36073437 -0.11376473]]\n",
            "63 0.80264765 [[-0.5629821  -1.1906118   0.27687347]\n",
            " [ 0.5428302   0.42872646  0.3045241 ]\n",
            " [ 0.21264027  0.36405244 -0.11994923]]\n",
            "64 0.7994571 [[-0.5710567  -1.1902295   0.28456587]\n",
            " [ 0.5432339   0.4256188   0.3072281 ]\n",
            " [ 0.2154626   0.36727262 -0.12599173]]\n",
            "65 0.7963434 [[-0.5790621  -1.1898588   0.2922005 ]\n",
            " [ 0.54365474  0.4226107   0.30981532]\n",
            " [ 0.218285    0.3703535  -0.131895  ]]\n",
            "66 0.79330313 [[-0.5870068  -1.1894915   0.29977793]\n",
            " [ 0.5440522   0.41973966  0.31228894]\n",
            " [ 0.22106826  0.37334013 -0.13766493]]\n",
            "67 0.79033315 [[-0.59488547 -1.1891342   0.3072993 ]\n",
            " [ 0.5444628   0.4169639   0.31465408]\n",
            " [ 0.22384883  0.37619895 -0.1433043 ]]\n",
            "68 0.7874304 [[-0.6027053  -1.1887803   0.31476516]\n",
            " [ 0.54485303  0.4143139   0.3169138 ]\n",
            " [ 0.22659436  0.3789676  -0.14881848]]\n",
            "69 0.7845921 [[-0.610462   -1.1884351   0.3221766 ]\n",
            " [ 0.5452533   0.41175467  0.3190728 ]\n",
            " [ 0.22933505  0.38161865 -0.15421021]]\n",
            "70 0.7818153 [[-0.61816186 -1.188093    0.32953423]\n",
            " [ 0.5456357   0.40931103  0.321134  ]\n",
            " [ 0.23204403  0.38418382 -0.15948436]]\n",
            "71 0.7790977 [[-0.6258013  -1.1877583   0.33683905]\n",
            " [ 0.54602563  0.40695345  0.3231016 ]\n",
            " [ 0.2347465   0.38664064 -0.16464363]]\n",
            "72 0.7764368 [[-0.6333857  -1.1874266   0.34409165]\n",
            " [ 0.54639995  0.40470228  0.32497847]\n",
            " [ 0.23742011  0.38901585 -0.16969246]]\n",
            "73 0.7738303 [[-0.6409123  -1.1871012   0.35129297]\n",
            " [ 0.54677963  0.4025326   0.32676846]\n",
            " [ 0.24008581  0.39129117 -0.17463346]]\n",
            "74 0.771276 [[-0.64838564 -1.1867785   0.35844362]\n",
            " [ 0.5471456   0.40046087  0.32847422]\n",
            " [ 0.2427251   0.3934891  -0.17947069]]\n",
            "75 0.76877207 [[-0.6558036  -1.1864614   0.3655445 ]\n",
            " [ 0.54751533  0.3984661   0.33009925]\n",
            " [ 0.24535541  0.3955948  -0.18420672]]\n",
            "76 0.7663161 [[-0.6631701  -1.1861466   0.37259617]\n",
            " [ 0.54787296  0.39656165  0.3316461 ]\n",
            " [ 0.24796137  0.39762738 -0.18884525]]\n",
            "77 0.76390666 [[-0.67048347 -1.1858366   0.3795995 ]\n",
            " [ 0.548233    0.3947297   0.33311802]\n",
            " [ 0.2505575   0.3995747  -0.19338869]]\n",
            "78 0.76154184 [[-0.67774713 -1.1855285   0.3865551 ]\n",
            " [ 0.54858226  0.39298108  0.33451733]\n",
            " [ 0.2531311   0.40145293 -0.19784048]]\n",
            "79 0.7592199 [[-0.6849598  -1.1852245   0.39346373]\n",
            " [ 0.54893297  0.39130068  0.33584702]\n",
            " [ 0.2556942   0.4032523  -0.20220296]]\n",
            "80 0.7569394 [[-0.6921244  -1.1849222   0.40032598]\n",
            " [ 0.54927415  0.38969722  0.33710933]\n",
            " [ 0.2582363   0.40498656 -0.20647927]]\n",
            "81 0.75469875 [[-0.69924    -1.1846232   0.40714264]\n",
            " [ 0.54961586  0.38815784  0.33830696]\n",
            " [ 0.26076743  0.40664777 -0.21067162]]\n",
            "82 0.7524967 [[-0.7063092  -1.1843257   0.41391423]\n",
            " [ 0.54994905  0.38668957  0.339442  ]\n",
            " [ 0.26327884  0.40824768 -0.21478292]]\n",
            "83 0.7503317 [[-0.7133313  -1.1840309   0.42064148]\n",
            " [ 0.55028224  0.38528138  0.340517  ]\n",
            " [ 0.26577902  0.40977988 -0.21881528]]\n",
            "84 0.74820244 [[-0.7203085  -1.1837372   0.42732498]\n",
            " [ 0.55060774  0.383939    0.3415339 ]\n",
            " [ 0.26826057  0.41125444 -0.22277139]]\n",
            "85 0.7461079 [[-0.7272404  -1.1834457   0.43396538]\n",
            " [ 0.5509328   0.38265288  0.34249496]\n",
            " [ 0.2707307   0.41266617 -0.22665326]]\n",
            "86 0.7440467 [[-0.73412895 -1.183155    0.44056323]\n",
            " [ 0.55125093  0.38142765  0.3434021 ]\n",
            " [ 0.27318317  0.4140238  -0.23046336]]\n",
            "87 0.7420179 [[-0.74097383 -1.182866    0.44711918]\n",
            " [ 0.5515682   0.38025507  0.34425735]\n",
            " [ 0.2756241   0.41532308 -0.23420358]]\n",
            "88 0.7400204 [[-0.7477768  -1.1825776   0.45363376]\n",
            " [ 0.5518793   0.37913886  0.34506252]\n",
            " [ 0.27804828  0.41657153 -0.23787622]]\n",
            "89 0.73805314 [[-0.7545377  -1.1822906   0.46010756]\n",
            " [ 0.55218923  0.37807193  0.34581953]\n",
            " [ 0.28046083  0.41776586 -0.24148309]]\n",
            "90 0.7361152 [[-0.76125807 -1.1820037   0.46654117]\n",
            " [ 0.5524935   0.3770572   0.34653005]\n",
            " [ 0.28285742  0.4189125  -0.2450263 ]]\n",
            "91 0.73420566 [[-0.7679379  -1.1817179   0.4729351 ]\n",
            " [ 0.5527965   0.37608847  0.3471958 ]\n",
            " [ 0.28524244  0.42000878 -0.2485076 ]]\n",
            "92 0.73232377 [[-0.7745785  -1.181432    0.47928992]\n",
            " [ 0.55309427  0.37516814  0.34781837]\n",
            " [ 0.28761217  0.42106038 -0.25192893]]\n",
            "93 0.7304684 [[-0.78117996 -1.1811467   0.48560613]\n",
            " [ 0.5533906   0.37429076  0.3483994 ]\n",
            " [ 0.2899704   0.42206514 -0.2552919 ]]\n",
            "94 0.72863907 [[-0.7877435  -1.1808612   0.49188426]\n",
            " [ 0.55368227  0.37345824  0.34894028]\n",
            " [ 0.29231396  0.42302805 -0.2585984 ]]\n",
            "95 0.7268348 [[-0.79426926 -1.1805761   0.49812484]\n",
            " [ 0.55397236  0.37266582  0.3494426 ]\n",
            " [ 0.2946461   0.42394736 -0.26184985]]\n",
            "96 0.7250549 [[-0.80075836 -1.1802905   0.5043283 ]\n",
            " [ 0.5542581   0.37191498  0.3499077 ]\n",
            " [ 0.29696417  0.4248275  -0.26504803]]\n",
            "97 0.72329885 [[-0.8072109  -1.1800048   0.51049525]\n",
            " [ 0.5545423   0.37120152  0.35033694]\n",
            " [ 0.299271    0.42566702 -0.26819438]]\n",
            "98 0.7215657 [[-0.81362796 -1.1797186   0.51662606]\n",
            " [ 0.5548225   0.37052664  0.3507316 ]\n",
            " [ 0.30156425  0.4264699  -0.27129048]]\n",
            "99 0.7198551 [[-0.82000965 -1.179432    0.52272123]\n",
            " [ 0.5551011   0.3698866   0.35109308]\n",
            " [ 0.30384636  0.42723498 -0.27433765]]\n",
            "100 0.71816635 [[-0.82635695 -1.1791447   0.52878124]\n",
            " [ 0.555376    0.36928234  0.35142246]\n",
            " [ 0.30611542  0.42796567 -0.2773374 ]]\n",
            "101 0.71649873 [[-0.8326701  -1.1788568   0.53480655]\n",
            " [ 0.5556493   0.36871055  0.351721  ]\n",
            " [ 0.3083735   0.42866117 -0.28029102]]\n",
            "102 0.71485174 [[-0.8389499  -1.178568    0.5407976 ]\n",
            " [ 0.5559191   0.36817196  0.35198975]\n",
            " [ 0.310619    0.42932454 -0.28319988]]\n",
            "103 0.713225 [[-0.84519666 -1.1782784   0.5467548 ]\n",
            " [ 0.55618733  0.3676636   0.3522299 ]\n",
            " [ 0.31285375  0.4299551  -0.2860652 ]]\n",
            "104 0.7116177 [[-0.85141116 -1.1779878   0.5526786 ]\n",
            " [ 0.5564524   0.367186    0.3524424 ]\n",
            " [ 0.3150763   0.4305556  -0.28888825]]\n",
            "105 0.7100296 [[-0.8575936  -1.1776962   0.55856943]\n",
            " [ 0.5567159   0.36673656  0.35262838]\n",
            " [ 0.3172883   0.43112552 -0.29167017]]\n",
            "106 0.7084601 [[-0.86374474 -1.1774033   0.56442773]\n",
            " [ 0.5569764   0.3663157   0.35278878]\n",
            " [ 0.31948847  0.43166736 -0.29441217]]\n",
            "107 0.7069089 [[-0.8698648  -1.1771094   0.57025385]\n",
            " [ 0.5572353   0.36592096  0.35292456]\n",
            " [ 0.32167837  0.4321806  -0.29711533]]\n",
            "108 0.7053753 [[-0.87595457 -1.176814    0.5760482 ]\n",
            " [ 0.5574914   0.36555278  0.3530366 ]\n",
            " [ 0.3238568   0.43266764 -0.2997808 ]]\n",
            "109 0.703859 [[-0.88201416 -1.1765174   0.5818112 ]\n",
            " [ 0.557746    0.36520892  0.3531259 ]\n",
            " [ 0.32602513  0.43312806 -0.30240953]]\n",
            "110 0.70235974 [[-0.8880443  -1.1762192   0.5875432 ]\n",
            " [ 0.55799794  0.36488968  0.3531932 ]\n",
            " [ 0.3281824   0.43356392 -0.30500263]]\n",
            "111 0.7008768 [[-0.89404523 -1.1759198   0.5932446 ]\n",
            " [ 0.5582484   0.36459303  0.3532394 ]\n",
            " [ 0.33032978  0.43397492 -0.307561  ]]\n",
            "112 0.69941014 [[-0.90001756 -1.1756186   0.5989158 ]\n",
            " [ 0.5584963   0.36431924  0.35326526]\n",
            " [ 0.33246636  0.434363   -0.31008568]]\n",
            "113 0.6979592 [[-0.9059615  -1.175316    0.60455716]\n",
            " [ 0.5587428   0.36406642  0.35327157]\n",
            " [ 0.3345934   0.43472788 -0.31257755]]\n",
            "114 0.6965234 [[-0.91187775 -1.1750116   0.610169  ]\n",
            " [ 0.5589869   0.36383483  0.3532591 ]\n",
            " [ 0.3367099   0.43507135 -0.31503752]]\n",
            "115 0.69510317 [[-0.91776645 -1.1747056   0.6157517 ]\n",
            " [ 0.5592296   0.3636227   0.3532285 ]\n",
            " [ 0.33881703  0.43539315 -0.31746647]]\n",
            "116 0.6936972 [[-0.92362815 -1.1743978   0.6213056 ]\n",
            " [ 0.55947006  0.36343023  0.35318053]\n",
            " [ 0.34091404  0.43569493 -0.31986523]]\n",
            "117 0.692306 [[-0.9294631  -1.1740884   0.626831  ]\n",
            " [ 0.5597091   0.3632559   0.35311586]\n",
            " [ 0.3430018   0.43597654 -0.3222346 ]]\n",
            "118 0.69092876 [[-0.9352718  -1.173777    0.63232833]\n",
            " [ 0.559946    0.3630997   0.35303512]\n",
            " [ 0.3450798   0.43623936 -0.32457542]]\n",
            "119 0.68956536 [[-0.9410545  -1.1734638   0.63779783]\n",
            " [ 0.56018144  0.36296043  0.35293895]\n",
            " [ 0.34714875  0.43648344 -0.32688844]]\n",
            "120 0.6882155 [[-0.94681174 -1.1731486   0.64323986]\n",
            " [ 0.56041497  0.3628379   0.35282794]\n",
            " [ 0.34920824  0.4367099  -0.3291744 ]]\n",
            "121 0.6868789 [[-0.9525437  -1.1728315   0.64865476]\n",
            " [ 0.5606471   0.36273104  0.3527027 ]\n",
            " [ 0.3512589   0.43691885 -0.331434  ]]\n",
            "122 0.6855552 [[-0.95825076 -1.1725124   0.6540428 ]\n",
            " [ 0.56087726  0.36263978  0.3525638 ]\n",
            " [ 0.35330036  0.4371114  -0.33366802]]\n",
            "123 0.68424433 [[-0.9639333  -1.1721914   0.6594043 ]\n",
            " [ 0.561106    0.362563    0.3524118 ]\n",
            " [ 0.35533318  0.4372876  -0.33587703]]\n",
            "124 0.6829459 [[-0.9695917  -1.1718683   0.66473955]\n",
            " [ 0.561333    0.36250067  0.35224718]\n",
            " [ 0.3573571   0.43744844 -0.3380618 ]]\n",
            "125 0.68165976 [[-0.9752261  -1.1715432   0.6700489 ]\n",
            " [ 0.56155854  0.36245176  0.35207048]\n",
            " [ 0.35937262  0.43759403 -0.3402229 ]]\n",
            "126 0.68038553 [[-0.980837   -1.171216    0.6753326 ]\n",
            " [ 0.5617824   0.3624162   0.3518822 ]\n",
            " [ 0.36137947  0.43772528 -0.342361  ]]\n",
            "127 0.6791231 [[-0.9864246  -1.1708868   0.6805909 ]\n",
            " [ 0.56200486  0.36239314  0.3516828 ]\n",
            " [ 0.36337808  0.4378423  -0.34447667]]\n",
            "128 0.67787224 [[-0.9919893  -1.1705554   0.6858242 ]\n",
            " [ 0.5622257   0.36238235  0.3514728 ]\n",
            " [ 0.36536834  0.43794587 -0.3465705 ]]\n",
            "129 0.6766327 [[-0.99753135 -1.1702218   0.6910327 ]\n",
            " [ 0.56244504  0.36238322  0.35125256]\n",
            " [ 0.36735052  0.43803623 -0.34864303]]\n",
            "130 0.67540425 [[-1.003051   -1.1698861   0.6962167 ]\n",
            " [ 0.5626629   0.36239532  0.35102257]\n",
            " [ 0.36932465  0.43811393 -0.35069486]]\n",
            "131 0.6741867 [[-1.0085486  -1.1695483   0.70137644]\n",
            " [ 0.5628793   0.36241826  0.3507832 ]\n",
            " [ 0.37129086  0.43817937 -0.35272652]]\n",
            "132 0.6729799 [[-1.0140244  -1.1692082   0.70651215]\n",
            " [ 0.5630942   0.36245164  0.35053492]\n",
            " [ 0.3732492   0.43823302 -0.3547385 ]]\n",
            "133 0.6717837 [[-1.0194787  -1.1688659   0.71162415]\n",
            " [ 0.5633077   0.36249498  0.35027808]\n",
            " [ 0.3751999   0.43827516 -0.35673133]]\n",
            "134 0.6705977 [[-1.0249116  -1.1685214   0.7167127 ]\n",
            " [ 0.5635198   0.36254793  0.35001305]\n",
            " [ 0.37714297  0.43830627 -0.3587055 ]]\n",
            "135 0.66942185 [[-1.0303236  -1.1681747   0.72177804]\n",
            " [ 0.5637304   0.36261016  0.3497402 ]\n",
            " [ 0.3790785   0.4383267  -0.36066145]]\n",
            "136 0.66825604 [[-1.0357149  -1.1678258   0.7268204 ]\n",
            " [ 0.56393963  0.36268127  0.34945992]\n",
            " [ 0.38100663  0.4383368  -0.36259964]]\n",
            "137 0.6671001 [[-1.0410857  -1.1674746   0.7318401 ]\n",
            " [ 0.5641475   0.36276084  0.3491725 ]\n",
            " [ 0.38292748  0.43833688 -0.36452058]]\n",
            "138 0.66595376 [[-1.0464363  -1.1671213   0.73683727]\n",
            " [ 0.5643539   0.36284867  0.3488783 ]\n",
            " [ 0.38484105  0.43832737 -0.36642465]]\n",
            "139 0.6648168 [[-1.0517669  -1.1667657   0.7418122 ]\n",
            " [ 0.564559    0.36294428  0.3485776 ]\n",
            " [ 0.3867476   0.43830848 -0.3683123 ]]\n",
            "140 0.66368926 [[-1.0570778  -1.1664078   0.7467652 ]\n",
            " [ 0.5647626   0.3630475   0.34827074]\n",
            " [ 0.38864702  0.43828064 -0.3701839 ]]\n",
            "141 0.66257083 [[-1.0623691  -1.1660477   0.7516964 ]\n",
            " [ 0.56496495  0.3631579   0.347958  ]\n",
            " [ 0.39053962  0.43824404 -0.3720399 ]]\n",
            "142 0.6614614 [[-1.0676411  -1.1656853   0.75660604]\n",
            " [ 0.5651659   0.36327532  0.34763965]\n",
            " [ 0.3924253   0.4381991  -0.37388068]]\n",
            "143 0.6603608 [[-1.0728941  -1.1653206   0.7614944 ]\n",
            " [ 0.56536555  0.3633993   0.347316  ]\n",
            " [ 0.39430434  0.43814597 -0.37570658]]\n",
            "144 0.65926903 [[-1.0781282  -1.1649537   0.76636165]\n",
            " [ 0.56556374  0.3635298   0.34698734]\n",
            " [ 0.39617658  0.4380851  -0.37751797]]\n",
            "145 0.65818584 [[-1.0833437  -1.1645845   0.77120805]\n",
            " [ 0.56576073  0.36366627  0.34665388]\n",
            " [ 0.3980424   0.4380165  -0.3793152 ]]\n",
            "146 0.65711105 [[-1.0885409  -1.1642131   0.77603376]\n",
            " [ 0.56595623  0.36380875  0.3463159 ]\n",
            " [ 0.39990163  0.43794072 -0.38109863]]\n",
            "147 0.6560446 [[-1.0937198  -1.1638393   0.780839  ]\n",
            " [ 0.56615055  0.36395675  0.34597358]\n",
            " [ 0.4017546   0.43785775 -0.38286862]]\n",
            "148 0.65498626 [[-1.0988808  -1.1634634   0.785624  ]\n",
            " [ 0.5663434   0.3641102   0.34562725]\n",
            " [ 0.40360114  0.437768   -0.38462543]]\n",
            "149 0.653936 [[-1.1040239  -1.1630851   0.790389  ]\n",
            " [ 0.56653506  0.36426875  0.34527707]\n",
            " [ 0.40544152  0.4376716  -0.38636944]]\n",
            "150 0.6528938 [[-1.1091495  -1.1627046   0.7951341 ]\n",
            " [ 0.5667253   0.3644323   0.34492326]\n",
            " [ 0.4072757   0.43756887 -0.3881009 ]]\n",
            "151 0.6518593 [[-1.1142576  -1.1623219   0.7998596 ]\n",
            " [ 0.5669143   0.3646005   0.34456605]\n",
            " [ 0.4091039   0.43745992 -0.38982013]]\n",
            "152 0.65083253 [[-1.1193485  -1.161937    0.8045656 ]\n",
            " [ 0.56710196  0.36477327  0.34420562]\n",
            " [ 0.41092604  0.4373451  -0.39152744]]\n",
            "153 0.6498134 [[-1.1244224  -1.1615498   0.8092524 ]\n",
            " [ 0.56728834  0.36495033  0.34384218]\n",
            " [ 0.41274235  0.43722445 -0.3932231 ]]\n",
            "154 0.64880157 [[-1.1294795  -1.1611603   0.8139201 ]\n",
            " [ 0.5674734   0.3651315   0.34347594]\n",
            " [ 0.41455278  0.43709826 -0.39490736]]\n",
            "155 0.6477972 [[-1.13452    -1.1607687   0.81856894]\n",
            " [ 0.5676572   0.36531663  0.34310704]\n",
            " [ 0.41635743  0.43696675 -0.3965805 ]]\n",
            "156 0.64680004 [[-1.139544   -1.1603749   0.8231991 ]\n",
            " [ 0.5678397   0.36550546  0.3427357 ]\n",
            " [ 0.41815647  0.43682998 -0.39824277]]\n",
            "157 0.64580995 [[-1.1445518  -1.1599789   0.82781076]\n",
            " [ 0.5680208   0.36569795  0.34236208]\n",
            " [ 0.41994983  0.4366883  -0.39989445]]\n",
            "158 0.6448271 [[-1.1495433  -1.1595806   0.8324041 ]\n",
            " [ 0.5682007   0.3658938   0.34198633]\n",
            " [ 0.4217377   0.43654174 -0.40153575]]\n",
            "159 0.64385116 [[-1.1545188  -1.1591802   0.83697927]\n",
            " [ 0.56837934  0.36609295  0.34160858]\n",
            " [ 0.4235201   0.43639052 -0.40316692]]\n",
            "160 0.642882 [[-1.1594787  -1.1587776   0.8415365 ]\n",
            " [ 0.5685566   0.3662952   0.34122905]\n",
            " [ 0.42529705  0.4362348  -0.40478817]]\n",
            "161 0.64191943 [[-1.1644229  -1.1583729   0.84607595]\n",
            " [ 0.5687327   0.36650035  0.34084782]\n",
            " [ 0.42706874  0.43607467 -0.40639976]]\n",
            "162 0.64096373 [[-1.1693516  -1.1579659   0.85059774]\n",
            " [ 0.5689074   0.36670843  0.34046507]\n",
            " [ 0.4288351   0.43591043 -0.40800187]]\n",
            "163 0.6400145 [[-1.174265   -1.1575568   0.85510206]\n",
            " [ 0.5690809   0.36691904  0.34008095]\n",
            " [ 0.43059635  0.43574202 -0.4095947 ]]\n",
            "164 0.63907176 [[-1.1791633  -1.1571455   0.8595891 ]\n",
            " [ 0.569253    0.36713225  0.33969554]\n",
            " [ 0.43235242  0.43556976 -0.41117853]]\n",
            "165 0.6381353 [[-1.1840466  -1.1567322   0.86405903]\n",
            " [ 0.569424    0.36734784  0.33930904]\n",
            " [ 0.43410346  0.4353937  -0.4127535 ]]\n",
            "166 0.63720524 [[-1.188915   -1.1563168   0.868512  ]\n",
            " [ 0.5695936   0.36756575  0.33892152]\n",
            " [ 0.43584946  0.435214   -0.41431978]]\n",
            "167 0.6362813 [[-1.1937687  -1.1558992   0.8729481 ]\n",
            " [ 0.56976193  0.3677858   0.33853313]\n",
            " [ 0.4375905   0.4350308  -0.4158776 ]]\n",
            "168 0.63536364 [[-1.1986079  -1.1554796   0.8773676 ]\n",
            " [ 0.56992906  0.3680079   0.33814394]\n",
            " [ 0.4393267   0.43484414 -0.41742718]]\n",
            "169 0.63445187 [[-1.2034327  -1.1550578   0.8817706 ]\n",
            " [ 0.5700949   0.36823195  0.3377541 ]\n",
            " [ 0.44105807  0.43465424 -0.41896865]]\n",
            "170 0.633546 [[-1.2082431  -1.154634    0.8861573 ]\n",
            " [ 0.57025945  0.3684578   0.33736372]\n",
            " [ 0.44278467  0.43446115 -0.42050216]]\n",
            "171 0.63264614 [[-1.2130395  -1.1542082   0.8905277 ]\n",
            " [ 0.5704227   0.3686854   0.33697286]\n",
            " [ 0.44450656  0.43426505 -0.42202792]]\n",
            "172 0.631752 [[-1.2178218  -1.1537802   0.89488214]\n",
            " [ 0.5705848   0.36891454  0.33658168]\n",
            " [ 0.44622383  0.4340659  -0.42354608]]\n",
            "173 0.63086355 [[-1.2225903  -1.1533502   0.89922065]\n",
            " [ 0.57074547  0.36914527  0.33619022]\n",
            " [ 0.44793645  0.433864   -0.4250568 ]]\n",
            "174 0.62998086 [[-1.2273451  -1.1529183   0.9035434 ]\n",
            " [ 0.57090497  0.36937737  0.3357986 ]\n",
            " [ 0.44964457  0.4336593  -0.42656022]]\n",
            "175 0.6291038 [[-1.2320863  -1.1524844   0.90785056]\n",
            " [ 0.5710632   0.36961088  0.33540687]\n",
            " [ 0.4513482   0.43345198 -0.42805654]]\n",
            "176 0.62823206 [[-1.2368139  -1.1520485   0.9121423 ]\n",
            " [ 0.5712202   0.36984557  0.33501518]\n",
            " [ 0.4530474   0.43324205 -0.42954585]]\n",
            "177 0.6273658 [[-1.2415283  -1.1516105   0.9164187 ]\n",
            " [ 0.5713759   0.3700815   0.33462358]\n",
            " [ 0.45474216  0.43302977 -0.4310283 ]]\n",
            "178 0.626505 [[-1.2462294  -1.1511706   0.92067987]\n",
            " [ 0.5715304   0.37031844  0.33423215]\n",
            " [ 0.4564327   0.432815   -0.43250406]]\n",
            "179 0.62564945 [[-1.2509174  -1.1507287   0.92492604]\n",
            " [ 0.5716836   0.37055647  0.33384094]\n",
            " [ 0.45811886  0.432598   -0.43397325]]\n",
            "180 0.62479913 [[-1.2555925  -1.1502849   0.92915726]\n",
            " [ 0.5718356   0.37079534  0.33345005]\n",
            " [ 0.4598009   0.4323787  -0.435436  ]]\n",
            "181 0.62395394 [[-1.2602546  -1.1498392   0.9333737 ]\n",
            " [ 0.57198626  0.37103516  0.33305955]\n",
            " [ 0.46147868  0.43215737 -0.43689245]]\n",
            "182 0.6231139 [[-1.264904   -1.1493915   0.93757546]\n",
            " [ 0.57213575  0.37127572  0.3326695 ]\n",
            " [ 0.46315235  0.43193394 -0.43834272]]\n",
            "183 0.6222789 [[-1.2695408  -1.148942    0.94176275]\n",
            " [ 0.5722839   0.37151706  0.33227998]\n",
            " [ 0.4648219   0.43170857 -0.4397869 ]]\n",
            "184 0.6214489 [[-1.274165   -1.1484907   0.9459356 ]\n",
            " [ 0.5724309   0.371759    0.33189103]\n",
            " [ 0.46648747  0.4314812  -0.44122514]]\n",
            "185 0.62062377 [[-1.2787769  -1.1480374   0.9500942 ]\n",
            " [ 0.57257664  0.37200162  0.3315027 ]\n",
            " [ 0.468149    0.43125212 -0.44265756]]\n",
            "186 0.6198036 [[-1.2833765  -1.1475823   0.9542387 ]\n",
            " [ 0.5727212   0.37224472  0.3311151 ]\n",
            " [ 0.4698066   0.43102118 -0.44408423]]\n",
            "187 0.6189881 [[-1.2879639  -1.1471254   0.95836914]\n",
            " [ 0.5728645   0.37248835  0.33072823]\n",
            " [ 0.47146028  0.43078858 -0.4455053 ]]\n",
            "188 0.61817735 [[-1.2925391  -1.1466666   0.9624857 ]\n",
            " [ 0.5730066   0.37273234  0.33034217]\n",
            " [ 0.47311014  0.43055427 -0.44692087]]\n",
            "189 0.6173714 [[-1.2971025  -1.1462061   0.96658844]\n",
            " [ 0.5731473   0.37297678  0.32995698]\n",
            " [ 0.4747561   0.43031847 -0.44833103]]\n",
            "190 0.61656994 [[-1.3016539  -1.1457438   0.97067755]\n",
            " [ 0.57328695  0.37322146  0.32957265]\n",
            " [ 0.47639838  0.43008104 -0.44973588]]\n",
            "191 0.61577314 [[-1.3061936  -1.1452798   0.97475314]\n",
            " [ 0.57342523  0.37346652  0.32918927]\n",
            " [ 0.4780368   0.4298423  -0.45113555]]\n",
            "192 0.6149809 [[-1.3107216  -1.1448139   0.97881526]\n",
            " [ 0.57356244  0.37371168  0.3288069 ]\n",
            " [ 0.47967166  0.42960197 -0.4525301 ]]\n",
            "193 0.61419296 [[-1.3152381  -1.1443462   0.9828641 ]\n",
            " [ 0.5736983   0.37395716  0.32842556]\n",
            " [ 0.4813027   0.42936042 -0.45391962]]\n",
            "194 0.6134096 [[-1.319743   -1.1438769   0.98689973]\n",
            " [ 0.5738331   0.37420258  0.3280453 ]\n",
            " [ 0.48293033  0.4291174  -0.4553042 ]]\n",
            "195 0.6126306 [[-1.3242366  -1.1434058   0.9909223 ]\n",
            " [ 0.5739665   0.37444836  0.32766613]\n",
            " [ 0.48455414  0.4288734  -0.45668396]]\n",
            "196 0.61185586 [[-1.3287189  -1.142933    0.9949318 ]\n",
            " [ 0.5740989   0.37469393  0.32728815]\n",
            " [ 0.4861746   0.42862788 -0.45805895]]\n",
            "197 0.61108536 [[-1.3331901  -1.1424586   0.9989285 ]\n",
            " [ 0.5742299   0.37493977  0.32691133]\n",
            " [ 0.48779136  0.42838147 -0.4594293 ]]\n",
            "198 0.61031914 [[-1.3376501  -1.1419824   1.0029124 ]\n",
            " [ 0.57435995  0.3751853   0.32653576]\n",
            " [ 0.4894049   0.4281337  -0.46079502]]\n",
            "199 0.60955715 [[-1.3420992  -1.1415046   1.0068836 ]\n",
            " [ 0.5744885   0.3754311   0.3261614 ]\n",
            " [ 0.4910147   0.42788512 -0.46215624]]\n",
            "200 0.60879916 [[-1.3465374  -1.1410253   1.0108423 ]\n",
            " [ 0.5746162   0.37567648  0.32578838]\n",
            " [ 0.49262136  0.4276352  -0.463513  ]]\n",
            "Prediction: [2 2 2]\n",
            "Accuracy:  1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ddxGZ_2vwYIv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## [실습2] 입력 데이터의 정규화의 유무에 따른 모델 학습 성공 유무 확인하기\n",
        "\n",
        "### 1) 코드에 주석을 달아 제출하세요\n",
        "\n",
        "### 2) 데이터 정규화 유무에 따른 모델 학습의 결과를 서술하여 제출하세요.\n",
        "\n",
        "- 입력데이터의 feature 별 값의 스케일 차가 크면, 학습이 이루어지지 않고 발산하기 쉽다. <br> [??] 부분에 xy = min_max_scaler(xy)  코드 넣어, 안정적으로 모델이 학습되는 것을 확인한다."
      ]
    },
    {
      "metadata": {
        "id": "PJRlDNlowYzX",
        "colab_type": "code",
        "outputId": "856ea47c-9b25-4420-ecf1-877bdfb40818",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17493
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "tf.set_random_seed(777)  # 랜덤 시드설정\n",
        "\n",
        "xy = np.array([[828.659973, 833.450012, 908100, 828.349976, 831.659973],\n",
        "               [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n",
        "               [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n",
        "               [816, 820.958984, 1008100, 815.48999, 819.23999],\n",
        "               [819.359985, 823, 1188100, 818.469971, 818.97998],\n",
        "               [819, 823, 1198100, 816, 820.450012],\n",
        "               [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n",
        "               [809.51001, 816.659973, 1398100, 804.539978, 809.559998]])\n",
        "\n",
        "\n",
        "# very important. It does not work without it.\n",
        "# 이 코드를 그대로 돌리면 발산하여 nan값이 나온다.\n",
        "# 따라서 우리는 데이터를 정규화할 필요가 있다.(모든 특성들이 0 과 1 사이에 위치하도록 데이터를 비례적으로 조정)\n",
        "def min_max_scaler(data):\n",
        "    numerator = data - np.min(data, 0) # 분자\n",
        "    denominator = np.max(data, 0) - np.min(data, 0) # 분모\n",
        "    return numerator / (denominator + 1e-7)\n",
        "  \n",
        "xy = min_max_scaler(xy) # xy값을 정규화\n",
        "\n",
        "x_data = xy[:, 0:-1] # 모든 행, 열 중 맨 앞의 열과 마지막 한열을 제외\n",
        "y_data = xy[:, [-1]] # 모든 행, 열 중 마지막 한열만을 가지고 배열 만들기\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=[None, 4]) # 열이 4인 placeholder 생성\n",
        "Y = tf.placeholder(tf.float32, shape=[None, 1]) # 열이 1인 placeholder 생성\n",
        "\n",
        "W = tf.Variable(tf.random_normal([4, 1]), name='weight') # XW + b이므로 행은 무조건 4\n",
        "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
        "\n",
        "hypothesis = tf.matmul(X, W) + b # 행렬의 곱셈\n",
        "cost = tf.reduce_mean(tf.square(hypothesis - Y)) # 열 단위로 평균 추출\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)\n",
        "train = optimizer.minimize(cost) # cost 최소값 구함\n",
        "\n",
        "sess = tf.Session() # 세션 설정\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "for step in range(101): # 100까지 step 돌리기\n",
        "    cost_val, hy_val, _ = sess.run(\n",
        "        [cost, hypothesis, train], feed_dict={X: x_data, Y: y_data})\n",
        "    # if step % 10 == 0:    #101개 출력하면 길어져서 10개만 대표확인\n",
        "    print(step, \"Cost: \", cost_val, \"\\nPrediction:\\n\", hy_val)\n",
        "\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.99999999 0.99999999 0.         1.         1.        ]\n",
            " [0.70548491 0.70439552 1.         0.71881782 0.83755791]\n",
            " [0.54412549 0.50274824 0.57608696 0.606468   0.6606331 ]\n",
            " [0.33890353 0.31368023 0.10869565 0.45989134 0.43800918]\n",
            " [0.51436    0.42582389 0.30434783 0.58504805 0.42624401]\n",
            " [0.49556179 0.42582389 0.31521739 0.48131134 0.49276137]\n",
            " [0.11436064 0.         0.20652174 0.22007776 0.18597238]\n",
            " [0.         0.07747099 0.5326087  0.         0.        ]]\n",
            "!\n",
            "[[1.        ]\n",
            " [0.83755791]\n",
            " [0.6606331 ]\n",
            " [0.43800918]\n",
            " [0.42624401]\n",
            " [0.49276137]\n",
            " [0.18597238]\n",
            " [0.        ]]\n",
            "!\n",
            "0 Cost:  3.9463515 \n",
            "Prediction:\n",
            " [[-2.1793876 ]\n",
            " [-1.7115263 ]\n",
            " [-1.4586613 ]\n",
            " [-1.1753334 ]\n",
            " [-1.3885761 ]\n",
            " [-1.3231955 ]\n",
            " [-0.75447935]\n",
            " [-0.62807035]]\n",
            "1 Cost:  3.9460573 \n",
            "Prediction:\n",
            " [[-2.1792862 ]\n",
            " [-1.7114296 ]\n",
            " [-1.4585807 ]\n",
            " [-1.175271  ]\n",
            " [-1.3885021 ]\n",
            " [-1.3231239 ]\n",
            " [-0.75443226]\n",
            " [-0.6280247 ]]\n",
            "2 Cost:  3.9457626 \n",
            "Prediction:\n",
            " [[-2.1791847 ]\n",
            " [-1.7113329 ]\n",
            " [-1.4585001 ]\n",
            " [-1.1752086 ]\n",
            " [-1.388428  ]\n",
            " [-1.3230526 ]\n",
            " [-0.75438523]\n",
            " [-0.627979  ]]\n",
            "3 Cost:  3.9454684 \n",
            "Prediction:\n",
            " [[-2.179083  ]\n",
            " [-1.7112362 ]\n",
            " [-1.4584196 ]\n",
            " [-1.1751461 ]\n",
            " [-1.3883538 ]\n",
            " [-1.3229811 ]\n",
            " [-0.75433815]\n",
            " [-0.6279333 ]]\n",
            "4 Cost:  3.945174 \n",
            "Prediction:\n",
            " [[-2.1789818 ]\n",
            " [-1.7111397 ]\n",
            " [-1.458339  ]\n",
            " [-1.1750836 ]\n",
            " [-1.3882798 ]\n",
            " [-1.3229097 ]\n",
            " [-0.75429106]\n",
            " [-0.6278876 ]]\n",
            "5 Cost:  3.9448798 \n",
            "Prediction:\n",
            " [[-2.1788802 ]\n",
            " [-1.7110429 ]\n",
            " [-1.4582584 ]\n",
            " [-1.1750214 ]\n",
            " [-1.3882058 ]\n",
            " [-1.3228383 ]\n",
            " [-0.754244  ]\n",
            " [-0.62784195]]\n",
            "6 Cost:  3.9445853 \n",
            "Prediction:\n",
            " [[-2.1787786 ]\n",
            " [-1.7109463 ]\n",
            " [-1.4581779 ]\n",
            " [-1.174959  ]\n",
            " [-1.3881316 ]\n",
            " [-1.3227668 ]\n",
            " [-0.75419694]\n",
            " [-0.62779623]]\n",
            "7 Cost:  3.9442906 \n",
            "Prediction:\n",
            " [[-2.1786773]\n",
            " [-1.7108495]\n",
            " [-1.4580972]\n",
            " [-1.1748965]\n",
            " [-1.3880575]\n",
            " [-1.3226954]\n",
            " [-0.7541499]\n",
            " [-0.6277506]]\n",
            "8 Cost:  3.943997 \n",
            "Prediction:\n",
            " [[-2.178576  ]\n",
            " [-1.710753  ]\n",
            " [-1.4580168 ]\n",
            " [-1.174834  ]\n",
            " [-1.3879836 ]\n",
            " [-1.322624  ]\n",
            " [-0.7541028 ]\n",
            " [-0.62770486]]\n",
            "9 Cost:  3.9437025 \n",
            "Prediction:\n",
            " [[-2.1784744 ]\n",
            " [-1.7106564 ]\n",
            " [-1.457936  ]\n",
            " [-1.1747718 ]\n",
            " [-1.3879094 ]\n",
            " [-1.3225524 ]\n",
            " [-0.75405574]\n",
            " [-0.6276592 ]]\n",
            "10 Cost:  3.943408 \n",
            "Prediction:\n",
            " [[-2.1783729]\n",
            " [-1.7105596]\n",
            " [-1.4578556]\n",
            " [-1.1747093]\n",
            " [-1.3878353]\n",
            " [-1.322481 ]\n",
            " [-0.7540087]\n",
            " [-0.6276135]]\n",
            "11 Cost:  3.9431136 \n",
            "Prediction:\n",
            " [[-2.1782713]\n",
            " [-1.7104628]\n",
            " [-1.4577749]\n",
            " [-1.1746469]\n",
            " [-1.3877611]\n",
            " [-1.3224096]\n",
            " [-0.7539617]\n",
            " [-0.6275678]]\n",
            "12 Cost:  3.9428194 \n",
            "Prediction:\n",
            " [[-2.1781697]\n",
            " [-1.7103664]\n",
            " [-1.4576944]\n",
            " [-1.1745844]\n",
            " [-1.3876872]\n",
            " [-1.3223381]\n",
            " [-0.7539146]\n",
            " [-0.6275221]]\n",
            "13 Cost:  3.9425251 \n",
            "Prediction:\n",
            " [[-2.1780684 ]\n",
            " [-1.7102697 ]\n",
            " [-1.457614  ]\n",
            " [-1.174522  ]\n",
            " [-1.387613  ]\n",
            " [-1.3222667 ]\n",
            " [-0.7538675 ]\n",
            " [-0.62747645]]\n",
            "14 Cost:  3.9422307 \n",
            "Prediction:\n",
            " [[-2.1779668]\n",
            " [-1.710173 ]\n",
            " [-1.4575334]\n",
            " [-1.1744596]\n",
            " [-1.3875389]\n",
            " [-1.3221952]\n",
            " [-0.7538205]\n",
            " [-0.6274308]]\n",
            "15 Cost:  3.9419365 \n",
            "Prediction:\n",
            " [[-2.1778655]\n",
            " [-1.7100763]\n",
            " [-1.4574528]\n",
            " [-1.1743972]\n",
            " [-1.387465 ]\n",
            " [-1.3221238]\n",
            " [-0.7537734]\n",
            " [-0.6273851]]\n",
            "16 Cost:  3.9416423 \n",
            "Prediction:\n",
            " [[-2.177764  ]\n",
            " [-1.7099797 ]\n",
            " [-1.4573721 ]\n",
            " [-1.1743348 ]\n",
            " [-1.3873909 ]\n",
            " [-1.3220524 ]\n",
            " [-0.75372636]\n",
            " [-0.6273394 ]]\n",
            "17 Cost:  3.9413483 \n",
            "Prediction:\n",
            " [[-2.1776626]\n",
            " [-1.709883 ]\n",
            " [-1.4572916]\n",
            " [-1.1742724]\n",
            " [-1.3873167]\n",
            " [-1.321981 ]\n",
            " [-0.7536793]\n",
            " [-0.6272937]]\n",
            "18 Cost:  3.941054 \n",
            "Prediction:\n",
            " [[-2.177561 ]\n",
            " [-1.7097864]\n",
            " [-1.457211 ]\n",
            " [-1.1742101]\n",
            " [-1.3872428]\n",
            " [-1.3219094]\n",
            " [-0.7536323]\n",
            " [-0.6272481]]\n",
            "19 Cost:  3.9407601 \n",
            "Prediction:\n",
            " [[-2.1774597]\n",
            " [-1.7096899]\n",
            " [-1.4571304]\n",
            " [-1.1741477]\n",
            " [-1.3871686]\n",
            " [-1.3218381]\n",
            " [-0.7535853]\n",
            " [-0.6272025]]\n",
            "20 Cost:  3.9404664 \n",
            "Prediction:\n",
            " [[-2.1773582 ]\n",
            " [-1.7095933 ]\n",
            " [-1.4570501 ]\n",
            " [-1.1740854 ]\n",
            " [-1.3870947 ]\n",
            " [-1.3217669 ]\n",
            " [-0.7535383 ]\n",
            " [-0.62715685]]\n",
            "21 Cost:  3.9401724 \n",
            "Prediction:\n",
            " [[-2.1772568 ]\n",
            " [-1.7094967 ]\n",
            " [-1.4569695 ]\n",
            " [-1.1740229 ]\n",
            " [-1.3870207 ]\n",
            " [-1.3216953 ]\n",
            " [-0.7534913 ]\n",
            " [-0.62711126]]\n",
            "22 Cost:  3.9398785 \n",
            "Prediction:\n",
            " [[-2.1771555]\n",
            " [-1.7094002]\n",
            " [-1.4568889]\n",
            " [-1.1739607]\n",
            " [-1.3869467]\n",
            " [-1.321624 ]\n",
            " [-0.7534443]\n",
            " [-0.6270656]]\n",
            "23 Cost:  3.9395852 \n",
            "Prediction:\n",
            " [[-2.1770542]\n",
            " [-1.7093036]\n",
            " [-1.4568086]\n",
            " [-1.1738985]\n",
            " [-1.3868728]\n",
            " [-1.3215528]\n",
            " [-0.7533973]\n",
            " [-0.62702  ]]\n",
            "24 Cost:  3.9392905 \n",
            "Prediction:\n",
            " [[-2.1769526]\n",
            " [-1.7092068]\n",
            " [-1.456728 ]\n",
            " [-1.173836 ]\n",
            " [-1.3867986]\n",
            " [-1.3214812]\n",
            " [-0.7533503]\n",
            " [-0.6269744]]\n",
            "25 Cost:  3.9389973 \n",
            "Prediction:\n",
            " [[-2.1768513 ]\n",
            " [-1.7091105 ]\n",
            " [-1.4566476 ]\n",
            " [-1.1737738 ]\n",
            " [-1.3867247 ]\n",
            " [-1.32141   ]\n",
            " [-0.7533033 ]\n",
            " [-0.62692875]]\n",
            "26 Cost:  3.9387035 \n",
            "Prediction:\n",
            " [[-2.17675   ]\n",
            " [-1.7090137 ]\n",
            " [-1.456567  ]\n",
            " [-1.1737113 ]\n",
            " [-1.3866508 ]\n",
            " [-1.3213387 ]\n",
            " [-0.7532563 ]\n",
            " [-0.62688315]]\n",
            "27 Cost:  3.9384098 \n",
            "Prediction:\n",
            " [[-2.1766486 ]\n",
            " [-1.7089174 ]\n",
            " [-1.4564865 ]\n",
            " [-1.1736491 ]\n",
            " [-1.3865767 ]\n",
            " [-1.3212671 ]\n",
            " [-0.75320935]\n",
            " [-0.62683755]]\n",
            "28 Cost:  3.9381156 \n",
            "Prediction:\n",
            " [[-2.176547 ]\n",
            " [-1.7088206]\n",
            " [-1.456406 ]\n",
            " [-1.1735866]\n",
            " [-1.3865027]\n",
            " [-1.3211958]\n",
            " [-0.7531623]\n",
            " [-0.6267919]]\n",
            "29 Cost:  3.9378216 \n",
            "Prediction:\n",
            " [[-2.1764457]\n",
            " [-1.708724 ]\n",
            " [-1.4563255]\n",
            " [-1.1735244]\n",
            " [-1.3864286]\n",
            " [-1.3211244]\n",
            " [-0.7531153]\n",
            " [-0.6267463]]\n",
            "30 Cost:  3.9375277 \n",
            "Prediction:\n",
            " [[-2.1763444]\n",
            " [-1.7086275]\n",
            " [-1.4562451]\n",
            " [-1.1734619]\n",
            " [-1.3863547]\n",
            " [-1.321053 ]\n",
            " [-0.7530683]\n",
            " [-0.6267007]]\n",
            "31 Cost:  3.9372342 \n",
            "Prediction:\n",
            " [[-2.176243  ]\n",
            " [-1.7085309 ]\n",
            " [-1.4561646 ]\n",
            " [-1.1733997 ]\n",
            " [-1.3862808 ]\n",
            " [-1.3209817 ]\n",
            " [-0.75302136]\n",
            " [-0.62665504]]\n",
            "32 Cost:  3.9369407 \n",
            "Prediction:\n",
            " [[-2.1761415 ]\n",
            " [-1.7084343 ]\n",
            " [-1.456084  ]\n",
            " [-1.1733372 ]\n",
            " [-1.3862066 ]\n",
            " [-1.3209103 ]\n",
            " [-0.75297433]\n",
            " [-0.62660944]]\n",
            "33 Cost:  3.9366467 \n",
            "Prediction:\n",
            " [[-2.1760402 ]\n",
            " [-1.7083378 ]\n",
            " [-1.4560037 ]\n",
            " [-1.173275  ]\n",
            " [-1.3861327 ]\n",
            " [-1.3208389 ]\n",
            " [-0.75292736]\n",
            " [-0.62656385]]\n",
            "34 Cost:  3.9363532 \n",
            "Prediction:\n",
            " [[-2.1759388 ]\n",
            " [-1.7082412 ]\n",
            " [-1.4559231 ]\n",
            " [-1.1732125 ]\n",
            " [-1.3860587 ]\n",
            " [-1.3207676 ]\n",
            " [-0.75288033]\n",
            " [-0.6265182 ]]\n",
            "35 Cost:  3.9360595 \n",
            "Prediction:\n",
            " [[-2.1758375 ]\n",
            " [-1.7081447 ]\n",
            " [-1.4558425 ]\n",
            " [-1.1731503 ]\n",
            " [-1.3859847 ]\n",
            " [-1.3206962 ]\n",
            " [-0.75283337]\n",
            " [-0.6264726 ]]\n",
            "36 Cost:  3.9357655 \n",
            "Prediction:\n",
            " [[-2.175736 ]\n",
            " [-1.7080481]\n",
            " [-1.455762 ]\n",
            " [-1.1730881]\n",
            " [-1.3859107]\n",
            " [-1.3206248]\n",
            " [-0.7527864]\n",
            " [-0.626427 ]]\n",
            "37 Cost:  3.9354718 \n",
            "Prediction:\n",
            " [[-2.1756346 ]\n",
            " [-1.7079515 ]\n",
            " [-1.4556816 ]\n",
            " [-1.1730256 ]\n",
            " [-1.3858367 ]\n",
            " [-1.3205535 ]\n",
            " [-0.75273937]\n",
            " [-0.62638134]]\n",
            "38 Cost:  3.9351783 \n",
            "Prediction:\n",
            " [[-2.1755333 ]\n",
            " [-1.707855  ]\n",
            " [-1.455601  ]\n",
            " [-1.1729633 ]\n",
            " [-1.3857627 ]\n",
            " [-1.3204823 ]\n",
            " [-0.75269234]\n",
            " [-0.62633574]]\n",
            "39 Cost:  3.9348845 \n",
            "Prediction:\n",
            " [[-2.175432  ]\n",
            " [-1.7077584 ]\n",
            " [-1.4555206 ]\n",
            " [-1.1729009 ]\n",
            " [-1.3856888 ]\n",
            " [-1.3204107 ]\n",
            " [-0.7526454 ]\n",
            " [-0.62629014]]\n",
            "40 Cost:  3.934591 \n",
            "Prediction:\n",
            " [[-2.1753304]\n",
            " [-1.7076619]\n",
            " [-1.45544  ]\n",
            " [-1.1728387]\n",
            " [-1.3856146]\n",
            " [-1.3203394]\n",
            " [-0.7525984]\n",
            " [-0.6262445]]\n",
            "41 Cost:  3.934297 \n",
            "Prediction:\n",
            " [[-2.175229 ]\n",
            " [-1.7075653]\n",
            " [-1.4553597]\n",
            " [-1.1727762]\n",
            " [-1.3855407]\n",
            " [-1.320268 ]\n",
            " [-0.7525514]\n",
            " [-0.6261989]]\n",
            "42 Cost:  3.9340038 \n",
            "Prediction:\n",
            " [[-2.1751277]\n",
            " [-1.7074687]\n",
            " [-1.4552791]\n",
            " [-1.172714 ]\n",
            " [-1.3854668]\n",
            " [-1.3201966]\n",
            " [-0.7525044]\n",
            " [-0.6261533]]\n",
            "43 Cost:  3.93371 \n",
            "Prediction:\n",
            " [[-2.1750264]\n",
            " [-1.7073722]\n",
            " [-1.4551988]\n",
            " [-1.1726515]\n",
            " [-1.3853927]\n",
            " [-1.3201253]\n",
            " [-0.7524574]\n",
            " [-0.6261077]]\n",
            "44 Cost:  3.9334164 \n",
            "Prediction:\n",
            " [[-2.1749249 ]\n",
            " [-1.7072756 ]\n",
            " [-1.4551182 ]\n",
            " [-1.1725893 ]\n",
            " [-1.3853188 ]\n",
            " [-1.320054  ]\n",
            " [-0.7524104 ]\n",
            " [-0.62606204]]\n",
            "45 Cost:  3.9331229 \n",
            "Prediction:\n",
            " [[-2.1748238 ]\n",
            " [-1.7071791 ]\n",
            " [-1.4550378 ]\n",
            " [-1.1725268 ]\n",
            " [-1.3852448 ]\n",
            " [-1.3199825 ]\n",
            " [-0.75236344]\n",
            " [-0.62601644]]\n",
            "46 Cost:  3.9328296 \n",
            "Prediction:\n",
            " [[-2.1747222 ]\n",
            " [-1.7070825 ]\n",
            " [-1.4549572 ]\n",
            " [-1.1724646 ]\n",
            " [-1.3851708 ]\n",
            " [-1.3199112 ]\n",
            " [-0.7523165 ]\n",
            " [-0.62597084]]\n",
            "47 Cost:  3.9325361 \n",
            "Prediction:\n",
            " [[-2.174621 ]\n",
            " [-1.706986 ]\n",
            " [-1.4548769]\n",
            " [-1.1724024]\n",
            " [-1.3850968]\n",
            " [-1.31984  ]\n",
            " [-0.7522695]\n",
            " [-0.6259252]]\n",
            "48 Cost:  3.9322429 \n",
            "Prediction:\n",
            " [[-2.1745195]\n",
            " [-1.7068896]\n",
            " [-1.4547963]\n",
            " [-1.1723399]\n",
            " [-1.3850229]\n",
            " [-1.3197687]\n",
            " [-0.7522225]\n",
            " [-0.6258796]]\n",
            "49 Cost:  3.9319496 \n",
            "Prediction:\n",
            " [[-2.1744184]\n",
            " [-1.7067931]\n",
            " [-1.454716 ]\n",
            " [-1.1722777]\n",
            " [-1.384949 ]\n",
            " [-1.3196974]\n",
            " [-0.7521755]\n",
            " [-0.625834 ]]\n",
            "50 Cost:  3.9316554 \n",
            "Prediction:\n",
            " [[-2.174317 ]\n",
            " [-1.7066965]\n",
            " [-1.4546354]\n",
            " [-1.1722153]\n",
            " [-1.3848748]\n",
            " [-1.3196259]\n",
            " [-0.7521285]\n",
            " [-0.6257884]]\n",
            "51 Cost:  3.9313626 \n",
            "Prediction:\n",
            " [[-2.1742158]\n",
            " [-1.7066   ]\n",
            " [-1.454555 ]\n",
            " [-1.172153 ]\n",
            " [-1.3848009]\n",
            " [-1.3195546]\n",
            " [-0.7520815]\n",
            " [-0.6257428]]\n",
            "52 Cost:  3.931069 \n",
            "Prediction:\n",
            " [[-2.1741142 ]\n",
            " [-1.7065034 ]\n",
            " [-1.4544744 ]\n",
            " [-1.1720908 ]\n",
            " [-1.384727  ]\n",
            " [-1.3194833 ]\n",
            " [-0.75203454]\n",
            " [-0.62569714]]\n",
            "53 Cost:  3.930776 \n",
            "Prediction:\n",
            " [[-2.1740131 ]\n",
            " [-1.7064071 ]\n",
            " [-1.4543941 ]\n",
            " [-1.1720285 ]\n",
            " [-1.3846531 ]\n",
            " [-1.319412  ]\n",
            " [-0.7519876 ]\n",
            " [-0.62565154]]\n",
            "54 Cost:  3.9304824 \n",
            "Prediction:\n",
            " [[-2.1739116 ]\n",
            " [-1.7063105 ]\n",
            " [-1.4543138 ]\n",
            " [-1.1719661 ]\n",
            " [-1.3845792 ]\n",
            " [-1.3193407 ]\n",
            " [-0.7519406 ]\n",
            " [-0.62560594]]\n",
            "55 Cost:  3.930189 \n",
            "Prediction:\n",
            " [[-2.1738105 ]\n",
            " [-1.706214  ]\n",
            " [-1.4542332 ]\n",
            " [-1.1719038 ]\n",
            " [-1.3845052 ]\n",
            " [-1.3192692 ]\n",
            " [-0.75189364]\n",
            " [-0.62556034]]\n",
            "56 Cost:  3.9298956 \n",
            "Prediction:\n",
            " [[-2.173709 ]\n",
            " [-1.7061174]\n",
            " [-1.4541528]\n",
            " [-1.1718416]\n",
            " [-1.3844311]\n",
            " [-1.3191979]\n",
            " [-0.7518467]\n",
            " [-0.6255147]]\n",
            "57 Cost:  3.9296026 \n",
            "Prediction:\n",
            " [[-2.1736078]\n",
            " [-1.7060211]\n",
            " [-1.4540722]\n",
            " [-1.1717792]\n",
            " [-1.3843572]\n",
            " [-1.3191266]\n",
            " [-0.7517997]\n",
            " [-0.6254691]]\n",
            "58 Cost:  3.929309 \n",
            "Prediction:\n",
            " [[-2.1735063]\n",
            " [-1.7059245]\n",
            " [-1.4539919]\n",
            " [-1.1717169]\n",
            " [-1.3842833]\n",
            " [-1.3190553]\n",
            " [-0.7517527]\n",
            " [-0.6254235]]\n",
            "59 Cost:  3.9290164 \n",
            "Prediction:\n",
            " [[-2.1734052 ]\n",
            " [-1.705828  ]\n",
            " [-1.4539115 ]\n",
            " [-1.1716547 ]\n",
            " [-1.3842094 ]\n",
            " [-1.318984  ]\n",
            " [-0.75170577]\n",
            " [-0.62537795]]\n",
            "60 Cost:  3.9287236 \n",
            "Prediction:\n",
            " [[-2.173304  ]\n",
            " [-1.7057316 ]\n",
            " [-1.4538311 ]\n",
            " [-1.1715925 ]\n",
            " [-1.3841355 ]\n",
            " [-1.3189127 ]\n",
            " [-0.75165886]\n",
            " [-0.6253324 ]]\n",
            "61 Cost:  3.92843 \n",
            "Prediction:\n",
            " [[-2.1732025 ]\n",
            " [-1.7056352 ]\n",
            " [-1.4537507 ]\n",
            " [-1.1715302 ]\n",
            " [-1.3840616 ]\n",
            " [-1.3188415 ]\n",
            " [-0.75161195]\n",
            " [-0.6252868 ]]\n",
            "62 Cost:  3.9281373 \n",
            "Prediction:\n",
            " [[-2.1731014 ]\n",
            " [-1.7055387 ]\n",
            " [-1.4536703 ]\n",
            " [-1.171468  ]\n",
            " [-1.3839877 ]\n",
            " [-1.3187702 ]\n",
            " [-0.75156504]\n",
            " [-0.6252413 ]]\n",
            "63 Cost:  3.9278438 \n",
            "Prediction:\n",
            " [[-2.173     ]\n",
            " [-1.7054422 ]\n",
            " [-1.45359   ]\n",
            " [-1.1714058 ]\n",
            " [-1.3839138 ]\n",
            " [-1.318699  ]\n",
            " [-0.75151813]\n",
            " [-0.62519574]]\n",
            "64 Cost:  3.9275517 \n",
            "Prediction:\n",
            " [[-2.172899  ]\n",
            " [-1.7053459 ]\n",
            " [-1.4535096 ]\n",
            " [-1.1713436 ]\n",
            " [-1.3838401 ]\n",
            " [-1.3186278 ]\n",
            " [-0.75147116]\n",
            " [-0.6251502 ]]\n",
            "65 Cost:  3.927258 \n",
            "Prediction:\n",
            " [[-2.1727977 ]\n",
            " [-1.7052494 ]\n",
            " [-1.4534292 ]\n",
            " [-1.1712812 ]\n",
            " [-1.383766  ]\n",
            " [-1.3185565 ]\n",
            " [-0.7514243 ]\n",
            " [-0.62510467]]\n",
            "66 Cost:  3.9269652 \n",
            "Prediction:\n",
            " [[-2.1726964 ]\n",
            " [-1.705153  ]\n",
            " [-1.4533489 ]\n",
            " [-1.171219  ]\n",
            " [-1.3836923 ]\n",
            " [-1.3184853 ]\n",
            " [-0.75137734]\n",
            " [-0.6250591 ]]\n",
            "67 Cost:  3.9266725 \n",
            "Prediction:\n",
            " [[-2.1725953 ]\n",
            " [-1.7050565 ]\n",
            " [-1.4532685 ]\n",
            " [-1.1711568 ]\n",
            " [-1.3836184 ]\n",
            " [-1.318414  ]\n",
            " [-0.75133044]\n",
            " [-0.6250136 ]]\n",
            "68 Cost:  3.9263797 \n",
            "Prediction:\n",
            " [[-2.1724942]\n",
            " [-1.7049601]\n",
            " [-1.4531882]\n",
            " [-1.1710947]\n",
            " [-1.3835444]\n",
            " [-1.3183427]\n",
            " [-0.7512835]\n",
            " [-0.624968 ]]\n",
            "69 Cost:  3.926087 \n",
            "Prediction:\n",
            " [[-2.1723928 ]\n",
            " [-1.7048637 ]\n",
            " [-1.4531078 ]\n",
            " [-1.1710323 ]\n",
            " [-1.3834705 ]\n",
            " [-1.3182714 ]\n",
            " [-0.7512366 ]\n",
            " [-0.62492245]]\n",
            "70 Cost:  3.9257936 \n",
            "Prediction:\n",
            " [[-2.1722915]\n",
            " [-1.7047672]\n",
            " [-1.4530274]\n",
            " [-1.1709701]\n",
            " [-1.3833966]\n",
            " [-1.3182001]\n",
            " [-0.7511897]\n",
            " [-0.6248769]]\n",
            "71 Cost:  3.925501 \n",
            "Prediction:\n",
            " [[-2.1721902]\n",
            " [-1.7046709]\n",
            " [-1.452947 ]\n",
            " [-1.1709077]\n",
            " [-1.3833228]\n",
            " [-1.318129 ]\n",
            " [-0.7511428]\n",
            " [-0.6248314]]\n",
            "72 Cost:  3.925208 \n",
            "Prediction:\n",
            " [[-2.172089  ]\n",
            " [-1.7045746 ]\n",
            " [-1.4528666 ]\n",
            " [-1.1708455 ]\n",
            " [-1.3832488 ]\n",
            " [-1.3180577 ]\n",
            " [-0.7510959 ]\n",
            " [-0.62478584]]\n",
            "73 Cost:  3.9249158 \n",
            "Prediction:\n",
            " [[-2.171988 ]\n",
            " [-1.704478 ]\n",
            " [-1.4527863]\n",
            " [-1.1707833]\n",
            " [-1.383175 ]\n",
            " [-1.3179865]\n",
            " [-0.751049 ]\n",
            " [-0.6247403]]\n",
            "74 Cost:  3.9246223 \n",
            "Prediction:\n",
            " [[-2.1718864]\n",
            " [-1.7043815]\n",
            " [-1.4527059]\n",
            " [-1.170721 ]\n",
            " [-1.3831012]\n",
            " [-1.3179152]\n",
            " [-0.7510021]\n",
            " [-0.6246947]]\n",
            "75 Cost:  3.9243298 \n",
            "Prediction:\n",
            " [[-2.1717854 ]\n",
            " [-1.7042851 ]\n",
            " [-1.4526255 ]\n",
            " [-1.1706588 ]\n",
            " [-1.3830273 ]\n",
            " [-1.3178439 ]\n",
            " [-0.7509551 ]\n",
            " [-0.62464917]]\n",
            "76 Cost:  3.924037 \n",
            "Prediction:\n",
            " [[-2.1716843 ]\n",
            " [-1.7041888 ]\n",
            " [-1.4525452 ]\n",
            " [-1.1705966 ]\n",
            " [-1.3829534 ]\n",
            " [-1.3177726 ]\n",
            " [-0.75090826]\n",
            " [-0.6246036 ]]\n",
            "77 Cost:  3.923744 \n",
            "Prediction:\n",
            " [[-2.1715827]\n",
            " [-1.7040924]\n",
            " [-1.4524648]\n",
            " [-1.1705344]\n",
            " [-1.3828795]\n",
            " [-1.3177013]\n",
            " [-0.7508613]\n",
            " [-0.6245581]]\n",
            "78 Cost:  3.9234514 \n",
            "Prediction:\n",
            " [[-2.1714816 ]\n",
            " [-1.703996  ]\n",
            " [-1.4523845 ]\n",
            " [-1.1704721 ]\n",
            " [-1.3828056 ]\n",
            " [-1.3176302 ]\n",
            " [-0.7508144 ]\n",
            " [-0.62451255]]\n",
            "79 Cost:  3.9231584 \n",
            "Prediction:\n",
            " [[-2.1713803 ]\n",
            " [-1.7038995 ]\n",
            " [-1.4523041 ]\n",
            " [-1.1704099 ]\n",
            " [-1.3827317 ]\n",
            " [-1.3175589 ]\n",
            " [-0.75076747]\n",
            " [-0.624467  ]]\n",
            "80 Cost:  3.9228656 \n",
            "Prediction:\n",
            " [[-2.1712792 ]\n",
            " [-1.7038031 ]\n",
            " [-1.4522238 ]\n",
            " [-1.1703477 ]\n",
            " [-1.3826578 ]\n",
            " [-1.3174877 ]\n",
            " [-0.75072056]\n",
            " [-0.6244215 ]]\n",
            "81 Cost:  3.922573 \n",
            "Prediction:\n",
            " [[-2.1711779 ]\n",
            " [-1.7037067 ]\n",
            " [-1.4521434 ]\n",
            " [-1.1702855 ]\n",
            " [-1.382584  ]\n",
            " [-1.3174164 ]\n",
            " [-0.75067365]\n",
            " [-0.62437594]]\n",
            "82 Cost:  3.9222806 \n",
            "Prediction:\n",
            " [[-2.1710768 ]\n",
            " [-1.7036104 ]\n",
            " [-1.4520631 ]\n",
            " [-1.1702232 ]\n",
            " [-1.3825102 ]\n",
            " [-1.3173451 ]\n",
            " [-0.75062674]\n",
            " [-0.6243304 ]]\n",
            "83 Cost:  3.921988 \n",
            "Prediction:\n",
            " [[-2.1709757 ]\n",
            " [-1.703514  ]\n",
            " [-1.4519827 ]\n",
            " [-1.170161  ]\n",
            " [-1.3824363 ]\n",
            " [-1.317274  ]\n",
            " [-0.75057983]\n",
            " [-0.62428486]]\n",
            "84 Cost:  3.9216952 \n",
            "Prediction:\n",
            " [[-2.1708746]\n",
            " [-1.7034175]\n",
            " [-1.4519024]\n",
            " [-1.1700988]\n",
            " [-1.3823624]\n",
            " [-1.3172028]\n",
            " [-0.7505329]\n",
            " [-0.6242393]]\n",
            "85 Cost:  3.9214032 \n",
            "Prediction:\n",
            " [[-2.1707735]\n",
            " [-1.7033212]\n",
            " [-1.451822 ]\n",
            " [-1.1700366]\n",
            " [-1.3822886]\n",
            " [-1.3171315]\n",
            " [-0.750486 ]\n",
            " [-0.6241938]]\n",
            "86 Cost:  3.9211102 \n",
            "Prediction:\n",
            " [[-2.1706722 ]\n",
            " [-1.7032249 ]\n",
            " [-1.4517417 ]\n",
            " [-1.1699743 ]\n",
            " [-1.3822148 ]\n",
            " [-1.3170602 ]\n",
            " [-0.7504391 ]\n",
            " [-0.62414825]]\n",
            "87 Cost:  3.9208179 \n",
            "Prediction:\n",
            " [[-2.1705709]\n",
            " [-1.7031286]\n",
            " [-1.4516613]\n",
            " [-1.1699122]\n",
            " [-1.3821409]\n",
            " [-1.3169891]\n",
            " [-0.7503922]\n",
            " [-0.6241027]]\n",
            "88 Cost:  3.920525 \n",
            "Prediction:\n",
            " [[-2.1704698 ]\n",
            " [-1.7030323 ]\n",
            " [-1.451581  ]\n",
            " [-1.1698499 ]\n",
            " [-1.382067  ]\n",
            " [-1.3169179 ]\n",
            " [-0.75034523]\n",
            " [-0.6240572 ]]\n",
            "89 Cost:  3.9202328 \n",
            "Prediction:\n",
            " [[-2.1703687 ]\n",
            " [-1.7029358 ]\n",
            " [-1.4515008 ]\n",
            " [-1.1697878 ]\n",
            " [-1.3819932 ]\n",
            " [-1.3168466 ]\n",
            " [-0.7502984 ]\n",
            " [-0.62401164]]\n",
            "90 Cost:  3.91994 \n",
            "Prediction:\n",
            " [[-2.1702673]\n",
            " [-1.7028394]\n",
            " [-1.4514203]\n",
            " [-1.1697254]\n",
            " [-1.3819194]\n",
            " [-1.3167753]\n",
            " [-0.7502514]\n",
            " [-0.6239661]]\n",
            "91 Cost:  3.9196475 \n",
            "Prediction:\n",
            " [[-2.1701663 ]\n",
            " [-1.702743  ]\n",
            " [-1.4513401 ]\n",
            " [-1.1696632 ]\n",
            " [-1.3818455 ]\n",
            " [-1.3167042 ]\n",
            " [-0.75020456]\n",
            " [-0.62392056]]\n",
            "92 Cost:  3.9193552 \n",
            "Prediction:\n",
            " [[-2.1700652]\n",
            " [-1.7026467]\n",
            " [-1.4512597]\n",
            " [-1.169601 ]\n",
            " [-1.3817716]\n",
            " [-1.316633 ]\n",
            " [-0.7501576]\n",
            " [-0.623875 ]]\n",
            "93 Cost:  3.9190629 \n",
            "Prediction:\n",
            " [[-2.1699638]\n",
            " [-1.7025503]\n",
            " [-1.4511794]\n",
            " [-1.1695389]\n",
            " [-1.3816978]\n",
            " [-1.3165617]\n",
            " [-0.7501107]\n",
            " [-0.6238295]]\n",
            "94 Cost:  3.9187703 \n",
            "Prediction:\n",
            " [[-2.1698627 ]\n",
            " [-1.7024539 ]\n",
            " [-1.4510992 ]\n",
            " [-1.1694767 ]\n",
            " [-1.3816239 ]\n",
            " [-1.3164904 ]\n",
            " [-0.7500638 ]\n",
            " [-0.62378395]]\n",
            "95 Cost:  3.9184778 \n",
            "Prediction:\n",
            " [[-2.1697614 ]\n",
            " [-1.7023575 ]\n",
            " [-1.4510187 ]\n",
            " [-1.1694144 ]\n",
            " [-1.3815501 ]\n",
            " [-1.3164192 ]\n",
            " [-0.75001687]\n",
            " [-0.6237384 ]]\n",
            "96 Cost:  3.9181855 \n",
            "Prediction:\n",
            " [[-2.1696603 ]\n",
            " [-1.7022612 ]\n",
            " [-1.4509385 ]\n",
            " [-1.1693522 ]\n",
            " [-1.3814762 ]\n",
            " [-1.3163481 ]\n",
            " [-0.74996996]\n",
            " [-0.62369287]]\n",
            "97 Cost:  3.9178927 \n",
            "Prediction:\n",
            " [[-2.169559  ]\n",
            " [-1.7021649 ]\n",
            " [-1.4508581 ]\n",
            " [-1.16929   ]\n",
            " [-1.3814024 ]\n",
            " [-1.3162768 ]\n",
            " [-0.74992305]\n",
            " [-0.6236474 ]]\n",
            "98 Cost:  3.9176006 \n",
            "Prediction:\n",
            " [[-2.169458  ]\n",
            " [-1.7020686 ]\n",
            " [-1.4507778 ]\n",
            " [-1.1692278 ]\n",
            " [-1.3813286 ]\n",
            " [-1.3162055 ]\n",
            " [-0.74987614]\n",
            " [-0.62360185]]\n",
            "99 Cost:  3.9173083 \n",
            "Prediction:\n",
            " [[-2.1693568 ]\n",
            " [-1.7019722 ]\n",
            " [-1.4506975 ]\n",
            " [-1.1691656 ]\n",
            " [-1.3812547 ]\n",
            " [-1.3161343 ]\n",
            " [-0.74982923]\n",
            " [-0.6235563 ]]\n",
            "100 Cost:  3.917016 \n",
            "Prediction:\n",
            " [[-2.1692557]\n",
            " [-1.7018759]\n",
            " [-1.4506171]\n",
            " [-1.1691034]\n",
            " [-1.3811808]\n",
            " [-1.3160632]\n",
            " [-0.7497823]\n",
            " [-0.6235108]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SkM5_iKgxYB4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## [실습3] mnist 데이터 셋으로 손글씨 데이터 분류하기\n",
        "\n",
        "\n",
        "- 입력데이터는 28x28 영상 데이터를 784차원의 1차원 백터로 사용\n",
        "- 출력데이터는 숫자 정답\n",
        "\n",
        "### 1) 손글씨 데이터 정확도 올리기? 어떻게?\n",
        "- parameters인 num_epochs와 batch_size를 변경해보자!!\n",
        "  - num_epochs = 50, batch_size = 100\n",
        "\n",
        "### 2) 손글씨 데이터 정확도 올리기? 아이디어 있으면 고고!\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "0KZqjBvcxYcp",
        "colab_type": "code",
        "outputId": "af93635c-460c-4b4b-de6a-99ab3a34677b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1765
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from tensorflow.examples.tutorials.mnist import input_data # tensorflow에 있는 예제문제\n",
        "\n",
        "tf.set_random_seed(777)  # 랜덤 시드 설정\n",
        "\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True) # 데이터 가져오기\n",
        "\n",
        "nb_classes = 10 # 10개의 숫자. 0~9\n",
        "\n",
        "\n",
        "# MNIST data image of shape 28 * 28 = 784\n",
        "X = tf.placeholder(tf.float32, [None, 784])\n",
        "\n",
        "# 0 - 9 digits recognition = 10 classes\n",
        "Y = tf.placeholder(tf.float32, [None, nb_classes])\n",
        "\n",
        "# Wx+b 설정\n",
        "W = tf.Variable(tf.random_normal([784, nb_classes]))\n",
        "b = tf.Variable(tf.random_normal([nb_classes]))\n",
        "\n",
        "# Hypothesis (using softmax)\n",
        "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)\n",
        "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
        "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
        "\n",
        "# Test model\n",
        "is_correct = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
        "\n",
        "# parameters\n",
        "num_epochs = 80 # epoch: 학습용 사진 전체를 딱 한번 사용했을 때 한 세대(이폭, epoch)이 지나갔다고 한다\n",
        "batch_size = 100 # batch: 한 번에 처리하는 사진의 장수\n",
        "num_iterations = int(mnist.train.num_examples / batch_size)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    # Training cycle\n",
        "    for epoch in range(num_epochs):\n",
        "        avg_cost = 0\n",
        "\n",
        "        for i in range(num_iterations):\n",
        "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
        "            _, cost_val = sess.run([train, cost], feed_dict={X: batch_xs, Y: batch_ys})\n",
        "            avg_cost += cost_val / num_iterations\n",
        "\n",
        "        print(\"Epoch: {:04d}, Cost: {:.9f}\".format(epoch + 1, avg_cost))\n",
        "\n",
        "    print(\"Learning finished\")\n",
        "\n",
        "    # Test the model using test sets\n",
        "    print(\n",
        "        \"Accuracy: \",\n",
        "        accuracy.eval(\n",
        "            session=sess, feed_dict={X: mnist.test.images, Y: mnist.test.labels}\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    # Get one and predict\n",
        "    r = random.randint(0, mnist.test.num_examples - 1)\n",
        "    print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r : r + 1], 1)))\n",
        "    print(\n",
        "        \"Prediction: \",\n",
        "        sess.run(tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r : r + 1]}),\n",
        "    )\n",
        "\n",
        "    plt.imshow(\n",
        "        mnist.test.images[r : r + 1].reshape(28, 28),\n",
        "        cmap=\"Greys\",\n",
        "        interpolation=\"nearest\",\n",
        "    )\n",
        "    plt.show()\n",
        "\n",
        "# num_epochs = 15, batch_size = 100\n",
        "# num_epochs = 50, batch_size = 100\n",
        "# >>실험결과: num_epochs가 높을 수록 정확도는 올라간다"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "Epoch: 0001, Cost: 2.927468839\n",
            "Epoch: 0002, Cost: 1.122083631\n",
            "Epoch: 0003, Cost: 0.893103996\n",
            "Epoch: 0004, Cost: 0.780897876\n",
            "Epoch: 0005, Cost: 0.711102810\n",
            "Epoch: 0006, Cost: 0.660522637\n",
            "Epoch: 0007, Cost: 0.622352033\n",
            "Epoch: 0008, Cost: 0.591736882\n",
            "Epoch: 0009, Cost: 0.566317065\n",
            "Epoch: 0010, Cost: 0.545093061\n",
            "Epoch: 0011, Cost: 0.526647926\n",
            "Epoch: 0012, Cost: 0.510631604\n",
            "Epoch: 0013, Cost: 0.496749826\n",
            "Epoch: 0014, Cost: 0.484006910\n",
            "Epoch: 0015, Cost: 0.472562138\n",
            "Epoch: 0016, Cost: 0.462391974\n",
            "Epoch: 0017, Cost: 0.453646580\n",
            "Epoch: 0018, Cost: 0.444665392\n",
            "Epoch: 0019, Cost: 0.437160311\n",
            "Epoch: 0020, Cost: 0.429748484\n",
            "Epoch: 0021, Cost: 0.423083899\n",
            "Epoch: 0022, Cost: 0.417294601\n",
            "Epoch: 0023, Cost: 0.411390953\n",
            "Epoch: 0024, Cost: 0.405904846\n",
            "Epoch: 0025, Cost: 0.400774483\n",
            "Epoch: 0026, Cost: 0.396120978\n",
            "Epoch: 0027, Cost: 0.391605761\n",
            "Epoch: 0028, Cost: 0.387206277\n",
            "Epoch: 0029, Cost: 0.383117354\n",
            "Epoch: 0030, Cost: 0.379783827\n",
            "Epoch: 0031, Cost: 0.375862452\n",
            "Epoch: 0032, Cost: 0.372373273\n",
            "Epoch: 0033, Cost: 0.369082885\n",
            "Epoch: 0034, Cost: 0.365531861\n",
            "Epoch: 0035, Cost: 0.363143556\n",
            "Epoch: 0036, Cost: 0.360129066\n",
            "Epoch: 0037, Cost: 0.357296105\n",
            "Epoch: 0038, Cost: 0.354739512\n",
            "Epoch: 0039, Cost: 0.351753634\n",
            "Epoch: 0040, Cost: 0.349621216\n",
            "Epoch: 0041, Cost: 0.347213079\n",
            "Epoch: 0042, Cost: 0.345035978\n",
            "Epoch: 0043, Cost: 0.342724876\n",
            "Epoch: 0044, Cost: 0.340829707\n",
            "Epoch: 0045, Cost: 0.338618853\n",
            "Epoch: 0046, Cost: 0.336626317\n",
            "Epoch: 0047, Cost: 0.334628939\n",
            "Epoch: 0048, Cost: 0.332921416\n",
            "Epoch: 0049, Cost: 0.331155760\n",
            "Epoch: 0050, Cost: 0.329503008\n",
            "Epoch: 0051, Cost: 0.327914488\n",
            "Epoch: 0052, Cost: 0.325971197\n",
            "Epoch: 0053, Cost: 0.324311186\n",
            "Epoch: 0054, Cost: 0.322866622\n",
            "Epoch: 0055, Cost: 0.321271146\n",
            "Epoch: 0056, Cost: 0.319668281\n",
            "Epoch: 0057, Cost: 0.318643125\n",
            "Epoch: 0058, Cost: 0.317177045\n",
            "Epoch: 0059, Cost: 0.315833734\n",
            "Epoch: 0060, Cost: 0.314453152\n",
            "Epoch: 0061, Cost: 0.313108726\n",
            "Epoch: 0062, Cost: 0.311712116\n",
            "Epoch: 0063, Cost: 0.310795338\n",
            "Epoch: 0064, Cost: 0.309485606\n",
            "Epoch: 0065, Cost: 0.308279440\n",
            "Epoch: 0066, Cost: 0.307231795\n",
            "Epoch: 0067, Cost: 0.306313841\n",
            "Epoch: 0068, Cost: 0.305094228\n",
            "Epoch: 0069, Cost: 0.303985905\n",
            "Epoch: 0070, Cost: 0.302941898\n",
            "Epoch: 0071, Cost: 0.302375470\n",
            "Epoch: 0072, Cost: 0.301014499\n",
            "Epoch: 0073, Cost: 0.300250772\n",
            "Epoch: 0074, Cost: 0.299404501\n",
            "Epoch: 0075, Cost: 0.298307053\n",
            "Epoch: 0076, Cost: 0.297500045\n",
            "Epoch: 0077, Cost: 0.296477520\n",
            "Epoch: 0078, Cost: 0.295729251\n",
            "Epoch: 0079, Cost: 0.294917860\n",
            "Epoch: 0080, Cost: 0.293978372\n",
            "Learning finished\n",
            "Accuracy:  0.9142\n",
            "Label:  [7]\n",
            "Prediction:  [7]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}
